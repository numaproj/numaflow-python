{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Python SDK for Numaflow","text":"<p>This is the Python SDK for Numaflow.</p> <p>This SDK provides the interface for writing different functionalities of Numaflow like UDFs, UDSinks, UDSources and SideInput in Python.</p>"},{"location":"#installation","title":"Installation","text":"<p>Install the package using pip. <pre><code>pip install pynumaflow\n</code></pre></p>"},{"location":"#build-locally","title":"Build locally","text":"<p>This project uses Poetry for dependency management and packaging. To build the package locally, run the following command from the root of the project.</p> <pre><code>make setup\n</code></pre> <p>To run unit tests: <pre><code>make test\n</code></pre></p> <p>To format code style using black and ruff: <pre><code>make lint\n</code></pre></p> <p>Setup pre-commit hooks: <pre><code>pre-commit install\n</code></pre></p>"},{"location":"#implementing-different-functionalities","title":"Implementing different functionalities","text":"<ul> <li>Implement User Defined Sources</li> <li>Implement User Defined Source Transformers</li> <li>Implement User Defined Functions<ul> <li>Map</li> <li>Reduce</li> <li>Map Stream</li> <li>Batch Map</li> </ul> </li> <li>Implement User Defined Sinks</li> <li>Implement User Defined SideInputs</li> </ul>"},{"location":"#server-types","title":"Server Types","text":"<p>There are different types of gRPC server mechanisms which can be used to serve the UDFs, UDSinks and UDSource. These have different functionalities and are used for different use cases.</p> <p>Currently we support the following server types:</p> <ul> <li>Sync Server</li> <li>Asyncronous Server</li> <li>MultiProcessing Server</li> </ul> <p>Not all of the above are supported for all UDFs, UDSource and UDSinks.</p> <p>For each of the UDFs, UDSource and UDSinks, there are seperate classes for each of the server types. This helps in keeping the interface simple and easy to use, and the user can start the specific server type based on the use case.</p>"},{"location":"#syncserver","title":"SyncServer","text":"<p>Syncronous Server is the simplest server type. It is a multithreaded threaded server which can be used for simple UDFs and UDSinks. Here the server will invoke the handler function for each message. The messaging is synchronous and the server will wait for the handler to return before processing the next message.</p> <pre><code>grpc_server = MapServer(handler)\n</code></pre>"},{"location":"#asyncserver","title":"AsyncServer","text":"<p>Asyncronous Server is a multi threaded server which can be used for UDFs which are asyncronous. Here we utilize the asyncronous capabilities of Python to process multiple messages in parallel. The server will invoke the handler function for each message. The messaging is asyncronous and the server will not wait for the handler to return before processing the next message. Thus this server type is useful for UDFs which are asyncronous. The handler function for such a server should be an async function.</p> <pre><code>grpc_server = MapAsyncServer(handler)\n</code></pre>"},{"location":"#multiprocessserver","title":"MultiProcessServer","text":"<p>MultiProcess Server is a multi process server which can be used for UDFs which are CPU intensive. Here we utilize the multi process capabilities of Python to process multiple messages in parallel by forking multiple servers in different processes. The server will invoke the handler function for each message. Individually at the server level the messaging is synchronous and the server will wait for the handler to return before processing the next message. But since we have multiple servers running in parallel, the overall messaging also executes in parallel.</p> <p>This could be an alternative to creating multiple replicas of the same UDF container as here we are using the multi processing capabilities of the system to process multiple messages in parallel but within the same container.</p> <p>Thus this server type is useful for UDFs which are CPU intensive. <pre><code>grpc_server = MapMultiProcServer(mapper_instance=handler, server_count=2)\n</code></pre></p>"},{"location":"#currently-supported-server-types-for-each-functionality","title":"Currently Supported Server Types for each functionality","text":"<p>These are the class names for the server types supported by each of the functionalities.</p> <ul> <li>UDFs<ul> <li>Map<ul> <li>MapServer</li> <li>MapAsyncServer</li> <li>MapMultiProcServer</li> </ul> </li> <li>Reduce<ul> <li>ReduceAsyncServer</li> </ul> </li> <li>MapStream<ul> <li>MapStreamAsyncServer</li> </ul> </li> <li>BatchMap</li> <li>BatchMapAsyncServer</li> <li>Source Transform<ul> <li>SourceTransformServer</li> <li>SourceTransformMultiProcServer</li> </ul> </li> </ul> </li> <li>UDSource<ul> <li>SourceServer</li> <li>SourceAsyncServer</li> </ul> </li> <li>UDSink<ul> <li>SinkServer</li> <li>SinkAsyncServer</li> </ul> </li> <li>SideInput<ul> <li>SideInputServer</li> </ul> </li> </ul>"},{"location":"#handler-function-and-classes","title":"Handler Function and Classes","text":"<p>All the server types take a instance of a handler class or a handler function as an argument. The handler function or class is the function or class which implements the functionality of the UDF, UDSource or UDSink. For ease of use the user can pass either of the two to the server and the server will handle the rest.</p> <p>The handler for each of the servers has a specific signature which is defined by the server type and the implentation of the handlers should follow the same signature.</p> <p>For using the class based handlers the user can inherit from the base handler class for each of the functionalities and implement the handler function. The base handler class for each of the functionalities has the same signature as the handler function for the respective server type. The list of base handler classes for each of the functionalities is given below:</p> <ul> <li>UDFs<ul> <li>Map<ul> <li>Mapper</li> </ul> </li> <li>Reduce<ul> <li>Reducer</li> </ul> </li> <li>MapStream<ul> <li>MapStreamer</li> </ul> </li> <li>Source Transform<ul> <li>SourceTransformer</li> </ul> </li> <li>Batch Map</li> <li>BatchMapper</li> </ul> </li> <li>UDSource<ul> <li>Sourcer</li> </ul> </li> <li>UDSink<ul> <li>Sinker</li> </ul> </li> <li>SideInput<ul> <li>SideInput</li> </ul> </li> </ul> <p>More details about the signature of the handler function for each of the server types is given in the documentation of the respective server type.</p>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to pynumaflow will be documented in this page.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"changelog/#0110-latest","title":"[0.11.0] - Latest","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>Accumulator functionality for stateful data accumulation</li> <li>ReduceStream for streaming reduce results</li> <li>Improved type hints throughout the codebase</li> </ul>"},{"location":"changelog/#changed","title":"Changed","text":"<ul> <li>Updated dependencies to latest versions</li> <li>Enhanced error handling in gRPC servers</li> </ul>"},{"location":"changelog/#fixed","title":"Fixed","text":"<ul> <li>Various bug fixes and performance improvements</li> </ul>"},{"location":"changelog/#0100","title":"[0.10.0]","text":""},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li>BatchMap support for processing messages in batches</li> <li>MultiProcess server support for Map and SourceTransform</li> <li>Improved async server implementations</li> </ul>"},{"location":"changelog/#changed_1","title":"Changed","text":"<ul> <li>Refactored server architecture for better performance</li> <li>Updated protobuf definitions</li> </ul>"},{"location":"changelog/#090","title":"[0.9.0]","text":""},{"location":"changelog/#added_2","title":"Added","text":"<ul> <li>MapStream functionality</li> <li>Side Input support</li> <li>Enhanced metadata in Datum objects</li> </ul>"},{"location":"changelog/#changed_2","title":"Changed","text":"<ul> <li>Improved connection handling</li> <li>Better error messages</li> </ul>"},{"location":"changelog/#080","title":"[0.8.0]","text":""},{"location":"changelog/#added_3","title":"Added","text":"<ul> <li>User Defined Source support</li> <li>Source Transform functionality</li> <li>Headers support in messages</li> </ul>"},{"location":"changelog/#changed_3","title":"Changed","text":"<ul> <li>Updated gRPC communication protocol</li> <li>Enhanced logging</li> </ul>"},{"location":"changelog/#070","title":"[0.7.0]","text":""},{"location":"changelog/#added_4","title":"Added","text":"<ul> <li>Async server support for Map and Reduce</li> <li>User Defined Sink functionality</li> <li>Tagging support for message routing</li> </ul>"},{"location":"changelog/#changed_4","title":"Changed","text":"<ul> <li>Improved memory management</li> <li>Better type annotations</li> </ul>"},{"location":"changelog/#060","title":"[0.6.0]","text":""},{"location":"changelog/#added_5","title":"Added","text":"<ul> <li>Basic Map and Reduce UDF support</li> <li>gRPC server implementation</li> <li>Initial documentation</li> </ul>"},{"location":"changelog/#upgrade-guide","title":"Upgrade Guide","text":""},{"location":"changelog/#upgrading-to-0110","title":"Upgrading to 0.11.0","text":"<p>No breaking changes. New features are additive.</p>"},{"location":"changelog/#upgrading-to-0100","title":"Upgrading to 0.10.0","text":"<p>If using custom server configurations, review the new server options.</p>"},{"location":"changelog/#release-notes","title":"Release Notes","text":"<p>For detailed release notes, see the GitHub Releases page.</p>"},{"location":"contributing/","title":"Contributing","text":"<p>Thank you for your interest in contributing to pynumaflow! This guide will help you get started.</p>"},{"location":"contributing/#development-setup","title":"Development Setup","text":""},{"location":"contributing/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.9 or higher</li> <li>Poetry for dependency management</li> <li>Git</li> </ul>"},{"location":"contributing/#clone-and-install","title":"Clone and Install","text":"<pre><code># Clone the repository\ngit clone https://github.com/numaproj/numaflow-python.git\ncd numaflow-python/packages/pynumaflow\n\n# Install dependencies\nmake setup\n</code></pre> <p>This will install all development dependencies including testing and linting tools.</p>"},{"location":"contributing/#verify-installation","title":"Verify Installation","text":"<pre><code># Run tests\nmake test\n\n# Run linting\nmake lint\n</code></pre>"},{"location":"contributing/#code-style","title":"Code Style","text":"<p>We use Black for code formatting and Ruff for linting.</p>"},{"location":"contributing/#format-code","title":"Format Code","text":"<pre><code>make format\n</code></pre>"},{"location":"contributing/#lint-code","title":"Lint Code","text":"<pre><code>make lint\n</code></pre>"},{"location":"contributing/#pre-commit-hooks","title":"Pre-commit Hooks","text":"<p>We recommend setting up pre-commit hooks to automatically format and lint code before commits:</p> <pre><code>pre-commit install\n</code></pre>"},{"location":"contributing/#testing","title":"Testing","text":""},{"location":"contributing/#run-all-tests","title":"Run All Tests","text":"<pre><code>make test\n</code></pre>"},{"location":"contributing/#run-specific-tests","title":"Run Specific Tests","text":"<pre><code>poetry run pytest tests/test_mapper.py -v\n</code></pre>"},{"location":"contributing/#test-coverage","title":"Test Coverage","text":"<pre><code>poetry run pytest tests/ --cov=pynumaflow --cov-report=html\n</code></pre>"},{"location":"contributing/#project-structure","title":"Project Structure","text":"<pre><code>packages/pynumaflow/\n\u251c\u2500\u2500 pynumaflow/           # Main package\n\u2502   \u251c\u2500\u2500 mapper/           # Map UDF implementation\n\u2502   \u251c\u2500\u2500 reducer/          # Reduce UDF implementation\n\u2502   \u251c\u2500\u2500 mapstreamer/      # MapStream implementation\n\u2502   \u251c\u2500\u2500 batchmapper/      # BatchMap implementation\n\u2502   \u251c\u2500\u2500 sourcer/          # Source implementation\n\u2502   \u251c\u2500\u2500 sinker/           # Sink implementation\n\u2502   \u251c\u2500\u2500 sourcetransformer/# SourceTransform implementation\n\u2502   \u251c\u2500\u2500 sideinput/        # SideInput implementation\n\u2502   \u251c\u2500\u2500 reducestreamer/   # ReduceStream implementation\n\u2502   \u251c\u2500\u2500 accumulator/      # Accumulator implementation\n\u2502   \u251c\u2500\u2500 proto/            # Protocol buffer definitions\n\u2502   \u251c\u2500\u2500 shared/           # Shared utilities\n\u2502   \u2514\u2500\u2500 types.py          # Common type definitions\n\u251c\u2500\u2500 tests/                # Test files\n\u251c\u2500\u2500 examples/             # Example implementations\n\u251c\u2500\u2500 docs/                 # Documentation source\n\u251c\u2500\u2500 pyproject.toml        # Project configuration\n\u2514\u2500\u2500 Makefile              # Development commands\n</code></pre>"},{"location":"contributing/#making-changes","title":"Making Changes","text":""},{"location":"contributing/#1-create-a-branch","title":"1. Create a Branch","text":"<pre><code>git checkout -b feature/your-feature-name\n</code></pre>"},{"location":"contributing/#2-make-your-changes","title":"2. Make Your Changes","text":"<ul> <li>Write clear, documented code</li> <li>Follow the existing code style</li> <li>Add tests for new functionality</li> <li>Update documentation as needed</li> </ul>"},{"location":"contributing/#3-test-your-changes","title":"3. Test Your Changes","text":"<pre><code>make test\nmake lint\n</code></pre>"},{"location":"contributing/#4-commit-your-changes","title":"4. Commit Your Changes","text":"<p>Write clear commit messages:</p> <pre><code>git commit -m \"Add feature: description of your change\"\n</code></pre>"},{"location":"contributing/#5-push-and-create-pr","title":"5. Push and Create PR","text":"<pre><code>git push origin feature/your-feature-name\n</code></pre> <p>Then create a Pull Request on GitHub.</p>"},{"location":"contributing/#protocol-buffers","title":"Protocol Buffers","text":"<p>If you need to modify protocol buffer definitions:</p> <ol> <li>Edit the <code>.proto</code> files in <code>pynumaflow/proto/</code></li> <li>Regenerate Python files:</li> </ol> <pre><code>make proto\n</code></pre>"},{"location":"contributing/#documentation","title":"Documentation","text":""},{"location":"contributing/#local-preview","title":"Local Preview","text":"<pre><code># Install docs dependencies\npoetry install --with docs\n\n# Serve locally\nmake docs-serve\n</code></pre> <p>Visit <code>http://localhost:8000</code> to preview.</p>"},{"location":"contributing/#writing-documentation","title":"Writing Documentation","text":"<ul> <li>Documentation source files are in <code>docs/</code></li> <li>Use Markdown with MkDocs extensions</li> <li>Include code examples</li> <li>Keep explanations clear and concise</li> </ul>"},{"location":"contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":""},{"location":"contributing/#before-submitting","title":"Before Submitting","text":"<ul> <li>[ ] Tests pass (<code>make test</code>)</li> <li>[ ] Code is formatted (<code>make lint</code>)</li> <li>[ ] Documentation is updated (if applicable)</li> <li>[ ] Commit messages are clear</li> </ul>"},{"location":"contributing/#pr-description","title":"PR Description","text":"<p>Include:</p> <ul> <li>What the change does</li> <li>Why the change is needed</li> <li>How to test the change</li> <li>Any breaking changes</li> </ul>"},{"location":"contributing/#review-process","title":"Review Process","text":"<ol> <li>A maintainer will review your PR</li> <li>Address any feedback</li> <li>Once approved, your PR will be merged</li> </ol>"},{"location":"contributing/#reporting-issues","title":"Reporting Issues","text":""},{"location":"contributing/#bug-reports","title":"Bug Reports","text":"<p>Include:</p> <ul> <li>Python version</li> <li>pynumaflow version</li> <li>Steps to reproduce</li> <li>Expected behavior</li> <li>Actual behavior</li> <li>Error messages (if any)</li> </ul>"},{"location":"contributing/#feature-requests","title":"Feature Requests","text":"<p>Include:</p> <ul> <li>Use case description</li> <li>Proposed solution</li> <li>Alternatives considered</li> </ul>"},{"location":"contributing/#code-of-conduct","title":"Code of Conduct","text":"<p>Please be respectful and constructive in all interactions. We're all here to build great software together.</p>"},{"location":"contributing/#getting-help","title":"Getting Help","text":"<ul> <li>GitHub Issues</li> <li>Numaflow Slack</li> <li>Numaflow Documentation</li> </ul>"},{"location":"contributing/#license","title":"License","text":"<p>By contributing, you agree that your contributions will be licensed under the Apache 2.0 License.</p>"},{"location":"api/","title":"API Reference","text":"<p>This section provides detailed API documentation for all pynumaflow modules.</p>"},{"location":"api/#modules","title":"Modules","text":"Module Description Sourcer User Defined Source for custom data sources Source Transformer Transform data at ingestion Mapper Map UDF for transforming messages one at a time Map Streamer MapStream UDF for streaming results as they're produced Batch Mapper BatchMap UDF for processing messages in batches Sinker User Defined Sink for custom data destinations Reducer Reduce UDF for aggregating messages by key and time window Reduce Streamer Stream reduce results incrementally Accumulator Accumulate and process data with state Side Input Inject external data into UDFs"},{"location":"api/accumulator/","title":"Accumulator","text":"<p>This module offers tools for accumulating and processing data while managing state. With it, you can:</p> <ul> <li>Accumulate data over time</li> <li>Maintain state across messages</li> <li>Process accumulated data</li> </ul>"},{"location":"api/accumulator/#classes","title":"Classes","text":""},{"location":"api/accumulator/#pynumaflow.accumulator.Message","title":"Message  <code>dataclass</code>","text":"<pre><code>Message(\n    value: bytes,\n    keys: list[str] = None,\n    tags: list[str] = None,\n    watermark: datetime = None,\n    event_time: datetime = None,\n    headers: dict[str, str] = None,\n    id: str = None,\n)\n</code></pre> <p>Basic datatype for data passing to the next vertex/vertices.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>bytes</code> <p>data in bytes</p> required <code>keys</code> <code>list[str]</code> <p>[]string keys for vertex (optional)</p> <code>None</code> <code>tags</code> <code>list[str]</code> <p>[]string tags for conditional forwarding (optional)</p> <code>None</code> <code>watermark</code> <code>datetime</code> <p>watermark for this message (optional)</p> <code>None</code> <code>event_time</code> <code>datetime</code> <p>event time for this message (optional)</p> <code>None</code> <code>headers</code> <code>dict[str, str]</code> <p>headers for this message (optional)</p> <code>None</code> <code>id</code> <code>str</code> <p>message id (optional)</p> <code>None</code> Source code in <code>pynumaflow/accumulator/_dtypes.py</code> <pre><code>def __init__(\n    self,\n    value: bytes,\n    keys: list[str] = None,\n    tags: list[str] = None,\n    watermark: datetime = None,\n    event_time: datetime = None,\n    headers: dict[str, str] = None,\n    id: str = None,\n):\n    \"\"\"\n    Creates a Message object to send value to a vertex.\n    \"\"\"\n    self._keys = keys or []\n    self._tags = tags or []\n    self._value = value or b\"\"\n    self._watermark = watermark\n    self._event_time = event_time\n    self._headers = headers or {}\n    self._id = id or \"\"\n</code></pre>"},{"location":"api/accumulator/#pynumaflow.accumulator.Message.value","title":"value  <code>property</code>","text":"<pre><code>value: bytes\n</code></pre> <p>Returns the message payload value.</p> <p>Returns:</p> Type Description <code>bytes</code> <p>The message payload data as bytes.</p>"},{"location":"api/accumulator/#pynumaflow.accumulator.Message.keys","title":"keys  <code>property</code>","text":"<pre><code>keys: list[str]\n</code></pre> <p>Returns the message keys.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of string keys associated with this message.</p>"},{"location":"api/accumulator/#pynumaflow.accumulator.Message.tags","title":"tags  <code>property</code>","text":"<pre><code>tags: list[str]\n</code></pre> <p>Returns the message tags for conditional forwarding.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of string tags used for conditional forwarding.</p>"},{"location":"api/accumulator/#pynumaflow.accumulator.Message.watermark","title":"watermark  <code>property</code>","text":"<pre><code>watermark: datetime\n</code></pre> <p>Returns the watermark timestamp for this message.</p> <p>Returns:</p> Type Description <code>datetime</code> <p>The watermark timestamp, or None if not set.</p>"},{"location":"api/accumulator/#pynumaflow.accumulator.Message.event_time","title":"event_time  <code>property</code>","text":"<pre><code>event_time: datetime\n</code></pre> <p>Returns the event time for this message.</p> <p>Returns:</p> Type Description <code>datetime</code> <p>The event time timestamp, or None if not set.</p>"},{"location":"api/accumulator/#pynumaflow.accumulator.Message.headers","title":"headers  <code>property</code>","text":"<pre><code>headers: dict[str, str]\n</code></pre> <p>Returns the message headers.</p> <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>A dictionary containing header key-value pairs for this message.</p>"},{"location":"api/accumulator/#pynumaflow.accumulator.Message.id","title":"id  <code>property</code>","text":"<pre><code>id: str\n</code></pre> <p>Returns the message ID.</p> <p>Returns:</p> Type Description <code>str</code> <p>The unique identifier for this message.</p>"},{"location":"api/accumulator/#pynumaflow.accumulator.Message.to_drop","title":"to_drop  <code>classmethod</code>","text":"<pre><code>to_drop() -&gt; M\n</code></pre> <p>Creates a Message instance that indicates the message should be dropped.</p> <p>Returns:</p> Type Description <code>M</code> <p>A Message instance with empty value and DROP tag indicating the message should be dropped.</p> Source code in <code>pynumaflow/accumulator/_dtypes.py</code> <pre><code>@classmethod\ndef to_drop(cls: type[M]) -&gt; M:\n    \"\"\"Creates a Message instance that indicates the message should be dropped.\n\n    Returns:\n        M: A Message instance with empty value and DROP tag indicating\n           the message should be dropped.\n    \"\"\"\n    return cls(b\"\", None, [DROP])\n</code></pre>"},{"location":"api/accumulator/#pynumaflow.accumulator.Message.from_datum","title":"from_datum  <code>classmethod</code>","text":"<pre><code>from_datum(datum: Datum)\n</code></pre> <p>Create a Message instance from a Datum object.</p> <p>Parameters:</p> Name Type Description Default <code>datum</code> <code>Datum</code> <p>The Datum object to convert</p> required <p>Returns:</p> Type Description <code>Message</code> <p>A new Message instance with data from the datum</p> Source code in <code>pynumaflow/accumulator/_dtypes.py</code> <pre><code>@classmethod\ndef from_datum(cls, datum: Datum):\n    \"\"\"Create a Message instance from a Datum object.\n\n    Args:\n        datum: The Datum object to convert\n\n    Returns:\n        Message: A new Message instance with data from the datum\n    \"\"\"\n    return cls(\n        value=datum.value,\n        keys=datum.keys,\n        watermark=datum.watermark,\n        event_time=datum.event_time,\n        headers=datum.headers,\n        id=datum.id,\n    )\n</code></pre>"},{"location":"api/accumulator/#pynumaflow.accumulator.Datum","title":"Datum  <code>dataclass</code>","text":"<pre><code>Datum(\n    keys: list[str],\n    value: bytes,\n    event_time: datetime,\n    watermark: datetime,\n    id_: str,\n    headers: Optional[dict[str, str]] = None,\n)\n</code></pre> <p>Class to define the important information for the event.</p> <p>Parameters:</p> Name Type Description Default <code>keys</code> <code>list[str]</code> <p>the keys of the event.</p> required <code>value</code> <code>bytes</code> <p>the payload of the event.</p> required <code>event_time</code> <code>datetime</code> <p>the event time of the event.</p> required <code>watermark</code> <code>datetime</code> <p>the watermark of the event.</p> required <p>Example usage <pre><code>from pynumaflow.accumulator import Datum\nfrom datetime import datetime, timezone\n\nd = Datum(\n      keys=[\"test_key\"],\n      value=b\"test_mock_message\",\n      event_time=datetime.fromtimestamp(1662998400, timezone.utc),\n      watermark=datetime.fromtimestamp(1662998460, timezone.utc),\n      headers={\"key1\": \"value1\", \"key2\": \"value2\"},\n   )\n</code></pre></p> Source code in <code>pynumaflow/accumulator/_dtypes.py</code> <pre><code>def __init__(\n    self,\n    keys: list[str],\n    value: bytes,\n    event_time: datetime,\n    watermark: datetime,\n    id_: str,\n    headers: Optional[dict[str, str]] = None,\n):\n    self._keys = keys or list()\n    self._value = value or b\"\"\n    if not isinstance(event_time, datetime):\n        raise TypeError(f\"Wrong data type: {type(event_time)} for Datum.event_time\")\n    self._event_time = event_time\n    if not isinstance(watermark, datetime):\n        raise TypeError(f\"Wrong data type: {type(watermark)} for Datum.watermark\")\n    self._watermark = watermark\n    self._headers = headers or {}\n    self._id = id_\n</code></pre>"},{"location":"api/accumulator/#pynumaflow.accumulator.Datum.keys","title":"keys  <code>property</code>","text":"<pre><code>keys: list[str]\n</code></pre> <p>Returns the keys of the event.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of string keys associated with this event.</p>"},{"location":"api/accumulator/#pynumaflow.accumulator.Datum.value","title":"value  <code>property</code>","text":"<pre><code>value: bytes\n</code></pre> <p>Returns the value of the event.</p> <p>Returns:</p> Type Description <code>bytes</code> <p>The payload data of the event as bytes.</p>"},{"location":"api/accumulator/#pynumaflow.accumulator.Datum.event_time","title":"event_time  <code>property</code>","text":"<pre><code>event_time: datetime\n</code></pre> <p>Returns the event time of the event.</p> <p>Returns:</p> Type Description <code>datetime</code> <p>The timestamp when the event occurred.</p>"},{"location":"api/accumulator/#pynumaflow.accumulator.Datum.watermark","title":"watermark  <code>property</code>","text":"<pre><code>watermark: datetime\n</code></pre> <p>Returns the watermark of the event.</p> <p>Returns:</p> Type Description <code>datetime</code> <p>The watermark timestamp indicating the progress of event time.</p>"},{"location":"api/accumulator/#pynumaflow.accumulator.Datum.headers","title":"headers  <code>property</code>","text":"<pre><code>headers: dict[str, str]\n</code></pre> <p>Returns the headers of the event.</p> <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>A dictionary containing header key-value pairs for this event.</p>"},{"location":"api/accumulator/#pynumaflow.accumulator.Datum.id","title":"id  <code>property</code>","text":"<pre><code>id: str\n</code></pre> <p>Returns the id of the event.</p> <p>Returns:</p> Type Description <code>str</code> <p>The unique identifier for this event.</p>"},{"location":"api/accumulator/#pynumaflow.accumulator.IntervalWindow","title":"IntervalWindow  <code>dataclass</code>","text":"<pre><code>IntervalWindow(start: datetime, end: datetime)\n</code></pre> <p>Defines the start and end of the interval window for the event.</p> Source code in <code>pynumaflow/accumulator/_dtypes.py</code> <pre><code>def __init__(self, start: datetime, end: datetime):\n    self._start = start\n    self._end = end\n</code></pre>"},{"location":"api/accumulator/#pynumaflow.accumulator.IntervalWindow.start","title":"start  <code>property</code>","text":"<pre><code>start: datetime\n</code></pre> <p>Returns the start point of the interval window.</p> <p>Returns:</p> Type Description <code>datetime</code> <p>The start timestamp of the interval window.</p>"},{"location":"api/accumulator/#pynumaflow.accumulator.IntervalWindow.end","title":"end  <code>property</code>","text":"<pre><code>end: datetime\n</code></pre> <p>Returns the end point of the interval window.</p> <p>Returns:</p> Type Description <code>datetime</code> <p>The end timestamp of the interval window.</p>"},{"location":"api/accumulator/#pynumaflow.accumulator.KeyedWindow","title":"KeyedWindow  <code>dataclass</code>","text":"<pre><code>KeyedWindow(\n    start: datetime,\n    end: datetime,\n    slot: str = \"\",\n    keys: list[str] = [],\n)\n</code></pre> <p>Defines the window for a accumulator operation which includes the interval window along with the slot.</p> Source code in <code>pynumaflow/accumulator/_dtypes.py</code> <pre><code>def __init__(self, start: datetime, end: datetime, slot: str = \"\", keys: list[str] = []):\n    self._window = IntervalWindow(start=start, end=end)\n    self._slot = slot\n    self._keys = keys\n</code></pre>"},{"location":"api/accumulator/#pynumaflow.accumulator.KeyedWindow.start","title":"start  <code>property</code>","text":"<pre><code>start: datetime\n</code></pre> <p>Returns the start point of the interval window.</p> <p>Returns:</p> Type Description <code>datetime</code> <p>The start timestamp of the interval window.</p>"},{"location":"api/accumulator/#pynumaflow.accumulator.KeyedWindow.end","title":"end  <code>property</code>","text":"<pre><code>end: datetime\n</code></pre> <p>Returns the end point of the interval window.</p> <p>Returns:</p> Type Description <code>datetime</code> <p>The end timestamp of the interval window.</p>"},{"location":"api/accumulator/#pynumaflow.accumulator.KeyedWindow.slot","title":"slot  <code>property</code>","text":"<pre><code>slot: str\n</code></pre> <p>Returns the slot from the window.</p> <p>Returns:</p> Type Description <code>str</code> <p>The slot identifier for this window.</p>"},{"location":"api/accumulator/#pynumaflow.accumulator.KeyedWindow.window","title":"window  <code>property</code>","text":"<pre><code>window: IntervalWindow\n</code></pre> <p>Returns the interval window.</p> <p>Returns:</p> Type Description <code>IntervalWindow</code> <p>The underlying interval window object.</p>"},{"location":"api/accumulator/#pynumaflow.accumulator.KeyedWindow.keys","title":"keys  <code>property</code>","text":"<pre><code>keys: list[str]\n</code></pre> <p>Returns the keys for window.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of keys associated with this window.</p>"},{"location":"api/accumulator/#pynumaflow.accumulator.Accumulator","title":"Accumulator","text":"<p>Accumulate can read unordered from the input stream and emit the ordered data to the output stream. Once the watermark (WM) of the output stream progresses, the data in WAL until that WM will be garbage collected. NOTE: A message can be silently dropped if need be, and it will be cleared from the WAL when the WM progresses.</p>"},{"location":"api/accumulator/#pynumaflow.accumulator.Accumulator.handler","title":"handler  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>handler(\n    datums: AsyncIterable[Datum],\n    output: NonBlockingIterator,\n)\n</code></pre> <p>Implement this handler function which implements the AccumulatorStreamCallable interface.</p> Source code in <code>pynumaflow/accumulator/_dtypes.py</code> <pre><code>@abstractmethod\nasync def handler(\n    self,\n    datums: AsyncIterable[Datum],\n    output: NonBlockingIterator,\n):\n    \"\"\"\n    Implement this handler function which implements the AccumulatorStreamCallable interface.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/accumulator/#pynumaflow.accumulator.AccumulatorAsyncServer","title":"AccumulatorAsyncServer","text":"<pre><code>AccumulatorAsyncServer(\n    accumulator_instance: AccumulatorStreamCallable,\n    init_args: tuple = (),\n    init_kwargs: Optional[dict] = None,\n    sock_path=ACCUMULATOR_SOCK_PATH,\n    max_message_size=MAX_MESSAGE_SIZE,\n    max_threads=NUM_THREADS_DEFAULT,\n    server_info_file=ACCUMULATOR_SERVER_INFO_FILE_PATH,\n)\n</code></pre> <p>               Bases: <code>NumaflowServer</code></p> <p>Class for a new Accumulator Server instance. A new servicer instance is created and attached to the server. The server instance is returned.</p> <p>Parameters:</p> Name Type Description Default <code>accumulator_instance</code> <code>AccumulatorStreamCallable</code> <p>The accumulator instance to be used for     Accumulator UDF</p> required <code>init_args</code> <code>tuple</code> <p>The arguments to be passed to the accumulator_handler</p> <code>()</code> <code>init_kwargs</code> <code>Optional[dict]</code> <p>The keyword arguments to be passed to the accumulator_handler</p> <code>None</code> <code>sock_path</code> <p>The UNIX socket path to be used for the server</p> <code>ACCUMULATOR_SOCK_PATH</code> <code>max_message_size</code> <p>The max message size in bytes the server can receive and send</p> <code>MAX_MESSAGE_SIZE</code> <code>max_threads</code> <p>The max number of threads to be spawned;</p> <code>NUM_THREADS_DEFAULT</code> <code>server_info_file</code> <p>The path to the server info file</p> <code>ACCUMULATOR_SERVER_INFO_FILE_PATH</code> <p>Example invocation: <pre><code>import os\nfrom collections.abc import AsyncIterable\nfrom datetime import datetime\n\nfrom pynumaflow.accumulator import Accumulator, AccumulatorAsyncServer\nfrom pynumaflow.accumulator import Message, Datum\nfrom pynumaflow.shared.asynciter import NonBlockingIterator\n\nclass StreamSorter(Accumulator):\n    def __init__(self, counter):\n        self.latest_wm = datetime.fromtimestamp(-1)\n        self.sorted_buffer: list[Datum] = []\n\n    async def handler(\n        self,\n        datums: AsyncIterable[Datum],\n        output: NonBlockingIterator,\n    ):\n        async for _ in datums:\n            # Process the datums and send output\n            if datum.watermark and datum.watermark &gt; self.latest_wm:\n                self.latest_wm = datum.watermark\n                await self.flush_buffer(output)\n\n            self.insert_sorted(datum)\n\n    def insert_sorted(self, datum: Datum):\n        # Binary insert to keep sorted buffer in order\n        left, right = 0, len(self.sorted_buffer)\n        while left &lt; right:\n            mid = (left + right) // 2\n            if self.sorted_buffer[mid].event_time &gt; datum.event_time:\n                right = mid\n            else:\n                left = mid + 1\n        self.sorted_buffer.insert(left, datum)\n\n    async def flush_buffer(self, output: NonBlockingIterator):\n        i = 0\n        for datum in self.sorted_buffer:\n            if datum.event_time &gt; self.latest_wm:\n                break\n            await output.put(Message.from_datum(datum))\n            i += 1\n        # Remove flushed items\n        self.sorted_buffer = self.sorted_buffer[i:]\n\n\nif __name__ == \"__main__\":\n    grpc_server = AccumulatorAsyncServer(StreamSorter)\n    grpc_server.start()\n</code></pre></p> Source code in <code>pynumaflow/accumulator/async_server.py</code> <pre><code>def __init__(\n    self,\n    accumulator_instance: AccumulatorStreamCallable,\n    init_args: tuple = (),\n    init_kwargs: Optional[dict] = None,\n    sock_path=ACCUMULATOR_SOCK_PATH,\n    max_message_size=MAX_MESSAGE_SIZE,\n    max_threads=NUM_THREADS_DEFAULT,\n    server_info_file=ACCUMULATOR_SERVER_INFO_FILE_PATH,\n):\n    init_kwargs = init_kwargs or {}\n    self.accumulator_handler = get_handler(accumulator_instance, init_args, init_kwargs)\n    self.sock_path = f\"unix://{sock_path}\"\n    self.max_message_size = max_message_size\n    self.max_threads = min(max_threads, MAX_NUM_THREADS)\n    self.server_info_file = server_info_file\n\n    self._server_options = [\n        (\"grpc.max_send_message_length\", self.max_message_size),\n        (\"grpc.max_receive_message_length\", self.max_message_size),\n    ]\n    # Get the servicer instance for the async server\n    self.servicer = AsyncAccumulatorServicer(self.accumulator_handler)\n</code></pre>"},{"location":"api/accumulator/#pynumaflow.accumulator.AccumulatorAsyncServer.start","title":"start","text":"<pre><code>start()\n</code></pre> <p>Starter function for the Async server class, need a separate caller so that all the async coroutines can be started from a single context</p> Source code in <code>pynumaflow/accumulator/async_server.py</code> <pre><code>def start(self):\n    \"\"\"\n    Starter function for the Async server class, need a separate caller\n    so that all the async coroutines can be started from a single context\n    \"\"\"\n    _LOGGER.info(\n        \"Starting Async Accumulator Server\",\n    )\n    aiorun.run(self.aexec(), use_uvloop=True)\n</code></pre>"},{"location":"api/accumulator/#pynumaflow.accumulator.AccumulatorAsyncServer.aexec","title":"aexec  <code>async</code>","text":"<pre><code>aexec()\n</code></pre> <p>Starts the Async gRPC server on the given UNIX socket with given max threads.</p> Source code in <code>pynumaflow/accumulator/async_server.py</code> <pre><code>async def aexec(self):\n    \"\"\"\n    Starts the Async gRPC server on the given UNIX socket with\n    given max threads.\n    \"\"\"\n    # As the server is async, we need to create a new server instance in the\n    # same thread as the event loop so that all the async calls are made in the\n    # same context\n    # Create a new async server instance and add the servicer to it\n    server = grpc.aio.server(options=self._server_options)\n    server.add_insecure_port(self.sock_path)\n    accumulator_pb2_grpc.add_AccumulatorServicer_to_server(self.servicer, server)\n\n    serv_info = ServerInfo.get_default_server_info()\n    serv_info.minimum_numaflow_version = MINIMUM_NUMAFLOW_VERSION[ContainerType.Accumulator]\n    await start_async_server(\n        server_async=server,\n        sock_path=self.sock_path,\n        max_threads=self.max_threads,\n        cleanup_coroutines=list(),\n        server_info_file=self.server_info_file,\n        server_info=serv_info,\n    )\n</code></pre>"},{"location":"api/batchmapper/","title":"Batch Mapper","text":"<p>The Batch Mapper module offers tools for building BatchMap UDFs, allowing you to process multiple messages simultaneously. This enables more efficient handling of workloads such as bulk API requests or batch database operations by grouping messages and processing them together in a single operation.</p>"},{"location":"api/batchmapper/#classes","title":"Classes","text":""},{"location":"api/batchmapper/#pynumaflow.batchmapper.Message","title":"Message  <code>dataclass</code>","text":"<pre><code>Message(\n    value: bytes,\n    keys: Optional[list[str]] = None,\n    tags: Optional[list[str]] = None,\n)\n</code></pre> <p>Basic datatype for data passing to the next vertex/vertices.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>bytes</code> <p>data in bytes</p> required <code>keys</code> <code>Optional[list[str]]</code> <p>list of keys for vertex (optional)</p> <code>None</code> <code>tags</code> <code>Optional[list[str]]</code> <p>list of tags for conditional forwarding (optional)</p> <code>None</code> Source code in <code>pynumaflow/batchmapper/_dtypes.py</code> <pre><code>def __init__(\n    self, value: bytes, keys: Optional[list[str]] = None, tags: Optional[list[str]] = None\n):\n    \"\"\"\n    Creates a Message object to send value to a vertex.\n    \"\"\"\n    self._keys = keys or []\n    self._tags = tags or []\n    self._value = value or b\"\"\n</code></pre>"},{"location":"api/batchmapper/#pynumaflow.batchmapper.Datum","title":"Datum  <code>dataclass</code>","text":"<pre><code>Datum(\n    id: str,\n    keys: list[str],\n    value: bytes,\n    event_time: datetime,\n    watermark: datetime,\n    headers: Optional[dict[str, str]] = None,\n)\n</code></pre> <p>Class to define the important information for the event.</p> <p>Parameters:</p> Name Type Description Default <code>keys</code> <code>list[str]</code> <p>the keys of the event.</p> required <code>value</code> <code>bytes</code> <p>the payload of the event.</p> required <code>event_time</code> <code>datetime</code> <p>the event time of the event.</p> required <code>watermark</code> <code>datetime</code> <p>the watermark of the event.</p> required <code>headers</code> <code>Optional[dict[str, str]]</code> <p>the headers of the event.</p> <code>None</code> <code>id</code> <code>str</code> <p>the unique ID for this request</p> required Source code in <code>pynumaflow/batchmapper/_dtypes.py</code> <pre><code>def __init__(\n    self,\n    id: str,\n    keys: list[str],\n    value: bytes,\n    event_time: datetime,\n    watermark: datetime,\n    headers: Optional[dict[str, str]] = None,\n):\n    self._id = id\n    self._keys = keys or list()\n    self._value = value or b\"\"\n    if not isinstance(event_time, datetime):\n        raise TypeError(f\"Wrong data type: {type(event_time)} for Datum.event_time\")\n    self._event_time = event_time\n    if not isinstance(watermark, datetime):\n        raise TypeError(f\"Wrong data type: {type(watermark)} for Datum.watermark\")\n    self._watermark = watermark\n    self._headers = headers or {}\n</code></pre>"},{"location":"api/batchmapper/#pynumaflow.batchmapper.Datum.keys","title":"keys  <code>property</code>","text":"<pre><code>keys: list[str]\n</code></pre> <p>Returns the keys of the event</p>"},{"location":"api/batchmapper/#pynumaflow.batchmapper.Datum.value","title":"value  <code>property</code>","text":"<pre><code>value: bytes\n</code></pre> <p>Returns the value of the event.</p>"},{"location":"api/batchmapper/#pynumaflow.batchmapper.Datum.event_time","title":"event_time  <code>property</code>","text":"<pre><code>event_time: datetime\n</code></pre> <p>Returns the event time of the event.</p>"},{"location":"api/batchmapper/#pynumaflow.batchmapper.Datum.watermark","title":"watermark  <code>property</code>","text":"<pre><code>watermark: datetime\n</code></pre> <p>Returns the watermark of the event.</p>"},{"location":"api/batchmapper/#pynumaflow.batchmapper.Datum.headers","title":"headers  <code>property</code>","text":"<pre><code>headers: dict[str, str]\n</code></pre> <p>Returns the headers of the event.</p>"},{"location":"api/batchmapper/#pynumaflow.batchmapper.Datum.id","title":"id  <code>property</code>","text":"<pre><code>id: str\n</code></pre> <p>Returns the id of the event.</p>"},{"location":"api/batchmapper/#pynumaflow.batchmapper.BatchMapper","title":"BatchMapper","text":"<p>Provides an interface to write a Batch Mapper which will be exposed over a gRPC server.</p> <p>Args:</p>"},{"location":"api/batchmapper/#pynumaflow.batchmapper.BatchMapper.handler","title":"handler  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>handler(datums: AsyncIterable[Datum]) -&gt; BatchResponses\n</code></pre> <p>Implement this handler function which implements the BatchMapAsyncCallable interface.</p> Source code in <code>pynumaflow/batchmapper/_dtypes.py</code> <pre><code>@abstractmethod\nasync def handler(self, datums: AsyncIterable[Datum]) -&gt; BatchResponses:\n    \"\"\"\n    Implement this handler function which implements the BatchMapAsyncCallable interface.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/batchmapper/#pynumaflow.batchmapper.BatchResponses","title":"BatchResponses","text":"<pre><code>BatchResponses(*responses: B)\n</code></pre> <p>               Bases: <code>Sequence[B]</code></p> <p>Class to define a list of Batch Response objects.</p> <p>Parameters:</p> Name Type Description Default <code>responses</code> <code>B</code> <p>list of Batch Response objects.</p> <code>()</code> Source code in <code>pynumaflow/batchmapper/_dtypes.py</code> <pre><code>def __init__(self, *responses: B):\n    self._responses = list(responses) or []\n</code></pre>"},{"location":"api/batchmapper/#pynumaflow.batchmapper.BatchResponse","title":"BatchResponse  <code>dataclass</code>","text":"<pre><code>BatchResponse(_id: str, messages: list[M])\n</code></pre> <p>Basic datatype for Batch map response.</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <p>the id of the request.</p> required <code>messages</code> <code>list[M]</code> <p>list of responses for corresponding to the request id</p> required"},{"location":"api/batchmapper/#pynumaflow.batchmapper.BatchMapAsyncServer","title":"BatchMapAsyncServer","text":"<pre><code>BatchMapAsyncServer(\n    batch_mapper_instance: BatchMapCallable,\n    sock_path=BATCH_MAP_SOCK_PATH,\n    max_message_size=MAX_MESSAGE_SIZE,\n    max_threads=NUM_THREADS_DEFAULT,\n    server_info_file=MAP_SERVER_INFO_FILE_PATH,\n)\n</code></pre> <p>               Bases: <code>NumaflowServer</code></p> <p>Class for a new Batch Map Async Server instance.</p> <p>The server instance is returned.</p> <p>Parameters:</p> Name Type Description Default <code>batch_mapper_instance</code> <code>BatchMapCallable</code> <p>The batch map stream instance to be used for Batch Map UDF</p> required <code>sock_path</code> <p>The UNIX socket path to be used for the server</p> <code>BATCH_MAP_SOCK_PATH</code> <code>max_message_size</code> <p>The max message size in bytes the server can receive and send</p> <code>MAX_MESSAGE_SIZE</code> <code>max_threads</code> <p>The max number of threads to be spawned;             defaults to 4 and max capped at 16</p> <code>NUM_THREADS_DEFAULT</code> <p>Example invocation: <pre><code>class Flatmap(BatchMapper):\n   async def handler(\n       self,\n       datums: AsyncIterable[Datum],\n   ) -&gt; BatchResponses:\n       batch_responses = BatchResponses()\n       async for datum in datums:\n           val = datum.value\n           _ = datum.event_time\n           _ = datum.watermark\n           strs = val.decode(\"utf-8\").split(\",\")\n           batch_response = BatchResponse.from_id(datum.id)\n           if len(strs) == 0:\n               batch_response.append(Message.to_drop())\n           else:\n               for s in strs:\n                   batch_response.append(Message(str.encode(s)))\n           batch_responses.append(batch_response)\n\n       return batch_responses\nif __name__ == \"__main__\":\n    grpc_server = BatchMapAsyncServer(Flatmap())\n    grpc_server.start()\n</code></pre></p> Source code in <code>pynumaflow/batchmapper/async_server.py</code> <pre><code>def __init__(\n    self,\n    batch_mapper_instance: BatchMapCallable,\n    sock_path=BATCH_MAP_SOCK_PATH,\n    max_message_size=MAX_MESSAGE_SIZE,\n    max_threads=NUM_THREADS_DEFAULT,\n    server_info_file=MAP_SERVER_INFO_FILE_PATH,\n):\n    \"\"\"\n    Create a new grpc Async Batch Map Server instance.\n    A new servicer instance is created and attached to the server.\n    The server instance is returned.\n\n    Args:\n        batch_mapper_instance: The batch map stream instance to be used for Batch Map UDF\n        sock_path: The UNIX socket path to be used for the server\n        max_message_size: The max message size in bytes the server can receive and send\n        max_threads: The max number of threads to be spawned;\n                        defaults to 4 and max capped at 16\n\n    Example invocation:\n    ```py\n    class Flatmap(BatchMapper):\n       async def handler(\n           self,\n           datums: AsyncIterable[Datum],\n       ) -&gt; BatchResponses:\n           batch_responses = BatchResponses()\n           async for datum in datums:\n               val = datum.value\n               _ = datum.event_time\n               _ = datum.watermark\n               strs = val.decode(\"utf-8\").split(\",\")\n               batch_response = BatchResponse.from_id(datum.id)\n               if len(strs) == 0:\n                   batch_response.append(Message.to_drop())\n               else:\n                   for s in strs:\n                       batch_response.append(Message(str.encode(s)))\n               batch_responses.append(batch_response)\n\n           return batch_responses\n    if __name__ == \"__main__\":\n        grpc_server = BatchMapAsyncServer(Flatmap())\n        grpc_server.start()\n    ```\n    \"\"\"\n    self.batch_mapper_instance: BatchMapCallable = batch_mapper_instance\n    self.sock_path = f\"unix://{sock_path}\"\n    self.max_threads = min(max_threads, MAX_NUM_THREADS)\n    self.max_message_size = max_message_size\n    self.server_info_file = server_info_file\n\n    self._server_options = [\n        (\"grpc.max_send_message_length\", self.max_message_size),\n        (\"grpc.max_receive_message_length\", self.max_message_size),\n    ]\n\n    self.servicer = AsyncBatchMapServicer(handler=self.batch_mapper_instance)\n</code></pre>"},{"location":"api/batchmapper/#pynumaflow.batchmapper.BatchMapAsyncServer.start","title":"start","text":"<pre><code>start()\n</code></pre> <p>Starter function for the Async Batch Map server, we need a separate caller to the aexec so that all the async coroutines can be started from a single context</p> Source code in <code>pynumaflow/batchmapper/async_server.py</code> <pre><code>def start(self):\n    \"\"\"\n    Starter function for the Async Batch Map server, we need a separate caller\n    to the aexec so that all the async coroutines can be started from a single context\n    \"\"\"\n    aiorun.run(self.aexec(), use_uvloop=True)\n</code></pre>"},{"location":"api/batchmapper/#pynumaflow.batchmapper.BatchMapAsyncServer.aexec","title":"aexec  <code>async</code>","text":"<pre><code>aexec()\n</code></pre> <p>Starts the Async gRPC server on the given UNIX socket with given max threads.</p> Source code in <code>pynumaflow/batchmapper/async_server.py</code> <pre><code>async def aexec(self):\n    \"\"\"\n    Starts the Async gRPC server on the given UNIX socket with\n    given max threads.\n    \"\"\"\n    # As the server is async, we need to create a new server instance in the\n    # same thread as the event loop so that all the async calls are made in the\n    # same context\n    # Create a new async server instance and add the servicer to it\n    server = grpc.aio.server(options=self._server_options)\n    server.add_insecure_port(self.sock_path)\n    map_pb2_grpc.add_MapServicer_to_server(\n        self.servicer,\n        server,\n    )\n    _LOGGER.info(\"Starting Batch Map Server\")\n    serv_info = ServerInfo.get_default_server_info()\n    serv_info.minimum_numaflow_version = MINIMUM_NUMAFLOW_VERSION[ContainerType.Mapper]\n    # Add the MAP_MODE metadata to the server info for the correct map mode\n    serv_info.metadata[MAP_MODE_KEY] = MapMode.BatchMap\n\n    # Start the async server\n    await start_async_server(\n        server_async=server,\n        sock_path=self.sock_path,\n        max_threads=self.max_threads,\n        cleanup_coroutines=list(),\n        server_info_file=self.server_info_file,\n        server_info=serv_info,\n    )\n</code></pre>"},{"location":"api/mapper/","title":"Mapper","text":"<p>The Mapper module provides classes and functions for implementing Map UDFs that transform messages one at a time. Map is the most common UDF type. It receives one message at a time and can return:</p> <ul> <li>One message (1:1 transformation)</li> <li>Multiple messages (fan-out)</li> <li>No messages (filter/drop)</li> </ul>"},{"location":"api/mapper/#classes","title":"Classes","text":""},{"location":"api/mapper/#pynumaflow.mapper.MapAsyncServer","title":"MapAsyncServer","text":"<pre><code>MapAsyncServer(\n    mapper_instance: MapAsyncCallable,\n    sock_path=MAP_SOCK_PATH,\n    max_message_size=MAX_MESSAGE_SIZE,\n    max_threads=NUM_THREADS_DEFAULT,\n    server_info_file=MAP_SERVER_INFO_FILE_PATH,\n)\n</code></pre> <p>               Bases: <code>NumaflowServer</code></p> <p>Create a new grpc Map Server instance. Args:     mapper_instance: The mapper instance to be used for Map UDF     sock_path: The UNIX socket path to be used for the server     max_message_size: The max message size in bytes the server can receive and send     max_threads: The max number of threads to be spawned;                     defaults to 4 and max capped at 16</p> <p>Example invocation: <pre><code>from pynumaflow.mapper import Messages, Message, Datum, MapAsyncServer\n\nasync def async_map_handler(keys: list[str], datum: Datum) -&gt; Messages:\n    val = datum.value\n    msg = (\n        f\"payload:{val.decode('utf-8')} \"\n        f\"event_time:{datum.event_time} \"\n        f\"watermark:{datum.watermark}\"\n    )\n    return Messages(Message(value=msg.encode('utf-8'), keys=keys))\n\nif __name__ == \"__main__\":\n    grpc_server = MapAsyncServer(async_map_handler)\n    grpc_server.start()\n</code></pre></p> <p>The server instance is returned.</p> <p>Parameters:</p> Name Type Description Default <code>mapper_instance</code> <code>MapAsyncCallable</code> <p>The mapper instance to be used for Map UDF</p> required <code>sock_path</code> <p>The UNIX socket path to be used for the server</p> <code>MAP_SOCK_PATH</code> <code>max_message_size</code> <p>The max message size in bytes the server can receive and send</p> <code>MAX_MESSAGE_SIZE</code> <code>max_threads</code> <p>The max number of threads to be spawned;         defaults to 4 and max capped at 16</p> <code>NUM_THREADS_DEFAULT</code> Source code in <code>pynumaflow/mapper/async_server.py</code> <pre><code>def __init__(\n    self,\n    mapper_instance: MapAsyncCallable,\n    sock_path=MAP_SOCK_PATH,\n    max_message_size=MAX_MESSAGE_SIZE,\n    max_threads=NUM_THREADS_DEFAULT,\n    server_info_file=MAP_SERVER_INFO_FILE_PATH,\n):\n    \"\"\"\n    Create a new grpc Asynchronous Map Server instance.\n    A new servicer instance is created and attached to the server.\n    The server instance is returned.\n\n    Args:\n        mapper_instance: The mapper instance to be used for Map UDF\n        sock_path: The UNIX socket path to be used for the server\n        max_message_size: The max message size in bytes the server can receive and send\n        max_threads: The max number of threads to be spawned;\n                    defaults to 4 and max capped at 16\n    \"\"\"\n    self.sock_path = f\"unix://{sock_path}\"\n    self.max_threads = min(max_threads, MAX_NUM_THREADS)\n    self.max_message_size = max_message_size\n    self.server_info_file = server_info_file\n\n    self.mapper_instance = mapper_instance\n\n    self._server_options = [\n        (\"grpc.max_send_message_length\", self.max_message_size),\n        (\"grpc.max_receive_message_length\", self.max_message_size),\n    ]\n    # Get the servicer instance for the async server\n    self.servicer = AsyncMapServicer(handler=mapper_instance)\n</code></pre>"},{"location":"api/mapper/#pynumaflow.mapper.MapAsyncServer.start","title":"start","text":"<pre><code>start() -&gt; None\n</code></pre> <p>Starter function for the Async server class, need a separate caller so that all the async coroutines can be started from a single context</p> Source code in <code>pynumaflow/mapper/async_server.py</code> <pre><code>def start(self) -&gt; None:\n    \"\"\"\n    Starter function for the Async server class, need a separate caller\n    so that all the async coroutines can be started from a single context\n    \"\"\"\n    aiorun.run(self.aexec(), use_uvloop=True)\n</code></pre>"},{"location":"api/mapper/#pynumaflow.mapper.MapAsyncServer.aexec","title":"aexec  <code>async</code>","text":"<pre><code>aexec() -&gt; None\n</code></pre> <p>Starts the Async gRPC server on the given UNIX socket with given max threads.</p> Source code in <code>pynumaflow/mapper/async_server.py</code> <pre><code>async def aexec(self) -&gt; None:\n    \"\"\"\n    Starts the Async gRPC server on the given UNIX socket with\n    given max threads.\n    \"\"\"\n\n    # As the server is async, we need to create a new server instance in the\n    # same thread as the event loop so that all the async calls are made in the\n    # same context\n\n    server_new = grpc.aio.server(options=self._server_options)\n    server_new.add_insecure_port(self.sock_path)\n    map_pb2_grpc.add_MapServicer_to_server(self.servicer, server_new)\n\n    serv_info = ServerInfo.get_default_server_info()\n    serv_info.minimum_numaflow_version = MINIMUM_NUMAFLOW_VERSION[ContainerType.Mapper]\n    # Add the MAP_MODE metadata to the server info for the correct map mode\n    serv_info.metadata[MAP_MODE_KEY] = MapMode.UnaryMap\n\n    # Start the async server\n    await start_async_server(\n        server_async=server_new,\n        sock_path=self.sock_path,\n        max_threads=self.max_threads,\n        cleanup_coroutines=list(),\n        server_info_file=self.server_info_file,\n        server_info=serv_info,\n    )\n</code></pre>"},{"location":"api/mapper/#pynumaflow.mapper.MapMultiprocServer","title":"MapMultiprocServer","text":"<pre><code>MapMultiprocServer(\n    mapper_instance: MapSyncCallable,\n    server_count: int = _PROCESS_COUNT,\n    sock_path=MAP_SOCK_PATH,\n    max_message_size=MAX_MESSAGE_SIZE,\n    max_threads=NUM_THREADS_DEFAULT,\n    server_info_file=MAP_SERVER_INFO_FILE_PATH,\n)\n</code></pre> <p>               Bases: <code>NumaflowServer</code></p> <p>Create a new grpc Multiproc Map Server instance.</p> <p>The server instance is returned.</p> <p>Parameters:</p> Name Type Description Default <code>mapper_instance</code> <code>MapSyncCallable</code> <p>The mapper instance to be used for Map UDF</p> required <code>server_count</code> <code>int</code> <p>The number of grpc server instances to be forked for multiproc</p> <code>_PROCESS_COUNT</code> <code>sock_path</code> <p>The UNIX socket path to be used for the server</p> <code>MAP_SOCK_PATH</code> <code>max_message_size</code> <p>The max message size in bytes the server can receive and send</p> <code>MAX_MESSAGE_SIZE</code> <code>max_threads</code> <p>The max number of threads to be spawned;         defaults to 4 and max capped at 16</p> <code>NUM_THREADS_DEFAULT</code> <p>Example invocation: <pre><code>import math\nimport os\nfrom pynumaflow.mapper import Messages, Message, Datum, Mapper, MapMultiprocServer\n\ndef is_prime(n):\n    for i in range(2, int(math.ceil(math.sqrt(n)))):\n        if n % i == 0:\n            return False\n    else:\n        return True\n\nclass PrimeMap(Mapper):\n    def handler(self, keys: list[str], datum: Datum) -&gt; Messages:\n        val = datum.value\n        _ = datum.event_time\n        _ = datum.watermark\n        messages = Messages()\n        for i in range(2, 100000):\n            is_prime(i)\n        messages.append(Message(val, keys=keys))\n        return messages\n\nif __name__ == \"__main__\":\n    server_count = 2\n    prime_class = PrimeMap()\n    # Server count is the number of server processes to start\n    grpc_server = MapMultiprocServer(prime_class, server_count=server_count)\n    grpc_server.start()\n</code></pre></p> Source code in <code>pynumaflow/mapper/multiproc_server.py</code> <pre><code>def __init__(\n    self,\n    mapper_instance: MapSyncCallable,\n    server_count: int = _PROCESS_COUNT,\n    sock_path=MAP_SOCK_PATH,\n    max_message_size=MAX_MESSAGE_SIZE,\n    max_threads=NUM_THREADS_DEFAULT,\n    server_info_file=MAP_SERVER_INFO_FILE_PATH,\n):\n    \"\"\"\n    Create a new grpc Multiproc Map Server instance.\n    A new servicer instance is created and attached to the server.\n    The server instance is returned.\n\n    Args:\n        mapper_instance: The mapper instance to be used for Map UDF\n        server_count: The number of grpc server instances to be forked for multiproc\n        sock_path: The UNIX socket path to be used for the server\n        max_message_size: The max message size in bytes the server can receive and send\n        max_threads: The max number of threads to be spawned;\n                    defaults to 4 and max capped at 16\n\n    Example invocation:\n    ```py\n    import math\n    import os\n    from pynumaflow.mapper import Messages, Message, Datum, Mapper, MapMultiprocServer\n\n    def is_prime(n):\n        for i in range(2, int(math.ceil(math.sqrt(n)))):\n            if n % i == 0:\n                return False\n        else:\n            return True\n\n    class PrimeMap(Mapper):\n        def handler(self, keys: list[str], datum: Datum) -&gt; Messages:\n            val = datum.value\n            _ = datum.event_time\n            _ = datum.watermark\n            messages = Messages()\n            for i in range(2, 100000):\n                is_prime(i)\n            messages.append(Message(val, keys=keys))\n            return messages\n\n    if __name__ == \"__main__\":\n        server_count = 2\n        prime_class = PrimeMap()\n        # Server count is the number of server processes to start\n        grpc_server = MapMultiprocServer(prime_class, server_count=server_count)\n        grpc_server.start()\n    ```\n    \"\"\"\n    self.sock_path = f\"unix://{sock_path}\"\n    self.max_threads = min(max_threads, MAX_NUM_THREADS)\n    self.max_message_size = max_message_size\n    self.server_info_file = server_info_file\n\n    self.mapper_instance = mapper_instance\n\n    self._server_options = [\n        (\"grpc.max_send_message_length\", self.max_message_size),\n        (\"grpc.max_receive_message_length\", self.max_message_size),\n        (\"grpc.so_reuseport\", 1),\n        (\"grpc.so_reuseaddr\", 1),\n    ]\n    # Set the number of processes to be spawned to the number of CPUs or\n    # the value of the parameter server_count defined by the user\n    # Setting the max value to 2 * CPU count\n    # Used for multiproc server\n    self._process_count = min(server_count, 2 * _PROCESS_COUNT)\n    self.servicer = SyncMapServicer(handler=mapper_instance, multiproc=True)\n</code></pre>"},{"location":"api/mapper/#pynumaflow.mapper.MapMultiprocServer.start","title":"start","text":"<pre><code>start() -&gt; None\n</code></pre> <p>Starts the N grpc servers gRPC serves on the with given max threads. Here N = The number of CPUs or the value of the parameter server_count defined by the user. The max value is capped to 2 * CPU count.</p> Source code in <code>pynumaflow/mapper/multiproc_server.py</code> <pre><code>def start(self) -&gt; None:\n    \"\"\"\n    Starts the N grpc servers gRPC serves on the with given max threads.\n    Here N = The number of CPUs or the value of the parameter server_count\n    defined by the user. The max value is capped to 2 * CPU count.\n    \"\"\"\n\n    # Create the server info file\n    server_info = ServerInfo.get_default_server_info()\n    server_info.minimum_numaflow_version = MINIMUM_NUMAFLOW_VERSION[ContainerType.Mapper]\n    server_info.metadata = get_metadata_env(envs=METADATA_ENVS)\n    # Add the MULTIPROC metadata using the number of servers to use\n    server_info.metadata[MULTIPROC_KEY] = str(self._process_count)\n    # Add the MAP_MODE metadata to the server info for the correct map mode\n    server_info.metadata[MAP_MODE_KEY] = MapMode.UnaryMap\n\n    # Start the multiproc server\n    start_multiproc_server(\n        max_threads=self.max_threads,\n        servicer=self.servicer,\n        process_count=self._process_count,\n        server_info_file=self.server_info_file,\n        server_options=self._server_options,\n        udf_type=UDFType.Map,\n        server_info=server_info,\n    )\n</code></pre>"},{"location":"api/mapper/#pynumaflow.mapper.MapServer","title":"MapServer","text":"<pre><code>MapServer(\n    mapper_instance: MapSyncCallable,\n    sock_path=MAP_SOCK_PATH,\n    max_message_size=MAX_MESSAGE_SIZE,\n    max_threads=NUM_THREADS_DEFAULT,\n    server_info_file=MAP_SERVER_INFO_FILE_PATH,\n)\n</code></pre> <p>               Bases: <code>NumaflowServer</code></p> <p>Create a new grpc Map Server instance.</p> <p>Parameters:</p> Name Type Description Default <code>mapper_instance</code> <code>MapSyncCallable</code> <p>The mapper instance to be used for Map UDF</p> required <code>sock_path</code> <p>The UNIX socket path to be used for the server</p> <code>MAP_SOCK_PATH</code> <code>max_message_size</code> <p>The max message size in bytes the server can receive and send</p> <code>MAX_MESSAGE_SIZE</code> <code>max_threads</code> <p>The max number of threads to be spawned;             defaults to 4 and max capped at 16</p> <code>NUM_THREADS_DEFAULT</code> <p>Example Invocation: <pre><code>from pynumaflow.mapper import Messages, Message, Datum, MapServer, Mapper\n\nclass MessageForwarder(Mapper):\n    def handler(self, keys: list[str], datum: Datum) -&gt; Messages:\n        val = datum.value\n        _ = datum.event_time\n        _ = datum.watermark\n        return Messages(Message(value=val, keys=keys))\n\ndef my_handler(keys: list[str], datum: Datum) -&gt; Messages:\n    val = datum.value\n    _ = datum.event_time\n    _ = datum.watermark\n    return Messages(Message(value=val, keys=keys))\n\n\nif __name__ == \"__main__\":\n    # Use the class based approach or function based handler based on the env variable\n    # Both can be used and passed directly to the server class\n\n    invoke = os.getenv(\"INVOKE\", \"func_handler\")\n    if invoke == \"class\":\n        handler = MessageForwarder()\n    else:\n        handler = my_handler\n    grpc_server = MapServer(handler)\n    grpc_server.start()\n</code></pre></p> Source code in <code>pynumaflow/mapper/sync_server.py</code> <pre><code>def __init__(\n    self,\n    mapper_instance: MapSyncCallable,\n    sock_path=MAP_SOCK_PATH,\n    max_message_size=MAX_MESSAGE_SIZE,\n    max_threads=NUM_THREADS_DEFAULT,\n    server_info_file=MAP_SERVER_INFO_FILE_PATH,\n):\n    self.sock_path = f\"unix://{sock_path}\"\n    self.max_threads = min(max_threads, MAX_NUM_THREADS)\n    self.max_message_size = max_message_size\n    self.server_info_file = server_info_file\n\n    self.mapper_instance = mapper_instance\n\n    self._server_options = [\n        (\"grpc.max_send_message_length\", self.max_message_size),\n        (\"grpc.max_receive_message_length\", self.max_message_size),\n    ]\n    # Get the servicer instance for the sync server\n    self.servicer = SyncMapServicer(handler=mapper_instance)\n</code></pre>"},{"location":"api/mapper/#pynumaflow.mapper.MapServer.start","title":"start","text":"<pre><code>start() -&gt; None\n</code></pre> <p>Starts the Synchronous gRPC server on the given UNIX socket with given max threads.</p> Source code in <code>pynumaflow/mapper/sync_server.py</code> <pre><code>def start(self) -&gt; None:\n    \"\"\"\n    Starts the Synchronous gRPC server on the given UNIX socket with given max threads.\n    \"\"\"\n    _LOGGER.info(\n        \"Sync GRPC Server listening on: %s with max threads: %s\",\n        self.sock_path,\n        self.max_threads,\n    )\n\n    serv_info = ServerInfo.get_default_server_info()\n    serv_info.minimum_numaflow_version = MINIMUM_NUMAFLOW_VERSION[ContainerType.Mapper]\n    # Add the MAP_MODE metadata to the server info for the correct map mode\n    serv_info.metadata[MAP_MODE_KEY] = MapMode.UnaryMap\n    # Start the server\n    sync_server_start(\n        servicer=self.servicer,\n        bind_address=self.sock_path,\n        max_threads=self.max_threads,\n        server_info_file=self.server_info_file,\n        server_options=self._server_options,\n        udf_type=UDFType.Map,\n        server_info=serv_info,\n    )\n</code></pre>"},{"location":"api/mapper/#pynumaflow.mapper.Message","title":"Message  <code>dataclass</code>","text":"<pre><code>Message(\n    value: bytes,\n    keys: Optional[list[str]] = None,\n    tags: Optional[list[str]] = None,\n    user_metadata: Optional[UserMetadata] = None,\n)\n</code></pre> <p>Basic datatype for data passing to the next vertex/vertices.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>bytes</code> <p>data in bytes</p> required <code>keys</code> <code>Optional[list[str]]</code> <p>list of keys for the vertex (optional)</p> <code>None</code> <code>tags</code> <code>Optional[list[str]]</code> <p>list of tags for conditional forwarding (optional)</p> <code>None</code> <code>user_metadata</code> <code>Optional[UserMetadata]</code> <p>metadata for the message (optional)</p> <code>None</code> Source code in <code>pynumaflow/mapper/_dtypes.py</code> <pre><code>def __init__(\n    self,\n    value: bytes,\n    keys: Optional[list[str]] = None,\n    tags: Optional[list[str]] = None,\n    user_metadata: Optional[UserMetadata] = None,\n):\n    \"\"\"\n    Creates a Message object to send value to a vertex.\n    \"\"\"\n    self._keys = keys or []\n    self._tags = tags or []\n    self._value = value or b\"\"\n    self._user_metadata = user_metadata or UserMetadata()\n</code></pre>"},{"location":"api/mapper/#pynumaflow.mapper.Messages","title":"Messages","text":"<pre><code>Messages(*messages: M)\n</code></pre> <p>               Bases: <code>Sequence[M]</code></p> <p>Class to define a list of Message objects.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>M</code> <p>list of Message objects.</p> <code>()</code> Source code in <code>pynumaflow/mapper/_dtypes.py</code> <pre><code>def __init__(self, *messages: M):\n    self._messages = list(messages) or []\n</code></pre>"},{"location":"api/mapper/#pynumaflow.mapper.Datum","title":"Datum  <code>dataclass</code>","text":"<pre><code>Datum(\n    keys: list[str],\n    value: bytes,\n    event_time: datetime,\n    watermark: datetime,\n    headers: Optional[dict[str, str]] = None,\n    user_metadata: Optional[UserMetadata] = None,\n    system_metadata: Optional[SystemMetadata] = None,\n)\n</code></pre> <p>Class to define the important information for the event.</p> <p>Parameters:</p> Name Type Description Default <code>keys</code> <code>list[str]</code> <p>the keys of the event.</p> required <code>value</code> <code>bytes</code> <p>the payload of the event.</p> required <code>event_time</code> <code>datetime</code> <p>the event time of the event.</p> required <code>watermark</code> <code>datetime</code> <p>the watermark of the event.</p> required <code>headers</code> <code>Optional[dict[str, str]]</code> <p>the headers of the event.</p> <code>None</code> <p>Example usage <pre><code>from pynumaflow.mapper import Datum\nfrom datetime import datetime, timezone\n\nd = Datum(\n      keys=[\"test_key\"],\n      value=b'test_mock_message'\n      event_time=datetime.fromtimestamp(1662998400, timezone.utc),\n      watermark=datetime.fromtimestamp(1662998460, timezone.utc),\n      headers={\"key1\": \"value1\", \"key2\": \"value2\"},\n   )\n</code></pre></p> Source code in <code>pynumaflow/mapper/_dtypes.py</code> <pre><code>def __init__(\n    self,\n    keys: list[str],\n    value: bytes,\n    event_time: datetime,\n    watermark: datetime,\n    headers: Optional[dict[str, str]] = None,\n    user_metadata: Optional[UserMetadata] = None,\n    system_metadata: Optional[SystemMetadata] = None,\n):\n    self._keys = keys or list()\n    self._value = value or b\"\"\n    if not isinstance(event_time, datetime):\n        raise TypeError(f\"Wrong data type: {type(event_time)} for Datum.event_time\")\n    self._event_time = event_time\n    if not isinstance(watermark, datetime):\n        raise TypeError(f\"Wrong data type: {type(watermark)} for Datum.watermark\")\n    self._watermark = watermark\n    self._headers = headers or {}\n    self._user_metadata = user_metadata or UserMetadata()\n    self._system_metadata = system_metadata or SystemMetadata()\n</code></pre>"},{"location":"api/mapper/#pynumaflow.mapper.Datum.keys","title":"keys  <code>property</code>","text":"<pre><code>keys: list[str]\n</code></pre> <p>Returns the keys of the event</p>"},{"location":"api/mapper/#pynumaflow.mapper.Datum.value","title":"value  <code>property</code>","text":"<pre><code>value: bytes\n</code></pre> <p>Returns the value of the event.</p>"},{"location":"api/mapper/#pynumaflow.mapper.Datum.event_time","title":"event_time  <code>property</code>","text":"<pre><code>event_time: datetime\n</code></pre> <p>Returns the event time of the event.</p>"},{"location":"api/mapper/#pynumaflow.mapper.Datum.watermark","title":"watermark  <code>property</code>","text":"<pre><code>watermark: datetime\n</code></pre> <p>Returns the watermark of the event.</p>"},{"location":"api/mapper/#pynumaflow.mapper.Datum.headers","title":"headers  <code>property</code>","text":"<pre><code>headers: dict[str, str]\n</code></pre> <p>Returns the headers of the event.</p>"},{"location":"api/mapper/#pynumaflow.mapper.Datum.user_metadata","title":"user_metadata  <code>property</code>","text":"<pre><code>user_metadata: UserMetadata\n</code></pre> <p>Returns the user metadata of the event.</p>"},{"location":"api/mapper/#pynumaflow.mapper.Datum.system_metadata","title":"system_metadata  <code>property</code>","text":"<pre><code>system_metadata: SystemMetadata\n</code></pre> <p>Returns the system metadata of the event.</p>"},{"location":"api/mapper/#pynumaflow.mapper.Mapper","title":"Mapper","text":"<p>Provides an interface to write a SyncMapServicer which will be exposed over a Synchronous gRPC server.</p>"},{"location":"api/mapper/#pynumaflow.mapper.Mapper.handler","title":"handler  <code>abstractmethod</code>","text":"<pre><code>handler(keys: list[str], datum: Datum) -&gt; Messages\n</code></pre> <p>Implement this handler function which implements the MapSyncCallable interface.</p> Source code in <code>pynumaflow/mapper/_dtypes.py</code> <pre><code>@abstractmethod\ndef handler(self, keys: list[str], datum: Datum) -&gt; Messages:\n    \"\"\"\n    Implement this handler function which implements the MapSyncCallable interface.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/mapper/#pynumaflow.mapper.UserMetadata","title":"UserMetadata  <code>dataclass</code>","text":"<pre><code>UserMetadata(_data: dict[str, dict[str, bytes]] = dict())\n</code></pre> <p>UserMetadata wraps the user-generated metadata groups per message. It is read-write to UDFs.</p>"},{"location":"api/mapper/#pynumaflow.mapper.UserMetadata.groups","title":"groups","text":"<pre><code>groups() -&gt; list[str]\n</code></pre> <p>Returns the list of group names for the user metadata.</p> Source code in <code>pynumaflow/_metadata.py</code> <pre><code>def groups(self) -&gt; list[str]:\n    \"\"\"\n    Returns the list of group names for the user metadata.\n    \"\"\"\n    return list(self._data.keys())\n</code></pre>"},{"location":"api/mapper/#pynumaflow.mapper.UserMetadata.keys","title":"keys","text":"<pre><code>keys(group: str) -&gt; list[str]\n</code></pre> <p>Returns the list of keys for a given group.</p> Source code in <code>pynumaflow/_metadata.py</code> <pre><code>def keys(self, group: str) -&gt; list[str]:\n    \"\"\"\n    Returns the list of keys for a given group.\n    \"\"\"\n    keys = self._data.get(group) or {}\n    return list(keys.keys())\n</code></pre>"},{"location":"api/mapper/#pynumaflow.mapper.UserMetadata.value","title":"value","text":"<pre><code>value(group: str, key: str) -&gt; Optional[bytes]\n</code></pre> <p>Returns the value for a given group and key. If the group or key does not exist, returns None.</p> Source code in <code>pynumaflow/_metadata.py</code> <pre><code>def value(self, group: str, key: str) -&gt; Optional[bytes]:\n    \"\"\"\n    Returns the value for a given group and key.\n    If the group or key does not exist, returns None.\n    \"\"\"\n    value = self._data.get(group)\n    if value is None:\n        return None\n    return value.get(key)\n</code></pre>"},{"location":"api/mapper/#pynumaflow.mapper.UserMetadata.add_key","title":"add_key","text":"<pre><code>add_key(group: str, key: str, value: bytes)\n</code></pre> <p>Adds the value for a given group and key.</p> Source code in <code>pynumaflow/_metadata.py</code> <pre><code>def add_key(self, group: str, key: str, value: bytes):\n    \"\"\"\n    Adds the value for a given group and key.\n    \"\"\"\n    self._data.setdefault(group, {})[key] = value\n</code></pre>"},{"location":"api/mapper/#pynumaflow.mapper.UserMetadata.remove_key","title":"remove_key","text":"<pre><code>remove_key(group: str, key: str) -&gt; Optional[bytes]\n</code></pre> <p>Removes the key and its value for a given group and returns the value. If this key is the only key in the group, the group will be removed. Returns None if the group or key does not exist.</p> Source code in <code>pynumaflow/_metadata.py</code> <pre><code>def remove_key(self, group: str, key: str) -&gt; Optional[bytes]:\n    \"\"\"\n    Removes the key and its value for a given group and returns the value.\n    If this key is the only key in the group, the group will be removed.\n    Returns None if the group or key does not exist.\n    \"\"\"\n    group_data = self._data.pop(group, None)\n    if group_data is None:\n        return None\n    value = group_data.pop(key, None)\n    if group_data:\n        self._data[group] = group_data\n    return value\n</code></pre>"},{"location":"api/mapper/#pynumaflow.mapper.UserMetadata.remove_group","title":"remove_group","text":"<pre><code>remove_group(group: str) -&gt; Optional[dict[str, bytes]]\n</code></pre> <p>Removes the group and all its keys and values and returns the data. Returns None if the group does not exist.</p> Source code in <code>pynumaflow/_metadata.py</code> <pre><code>def remove_group(self, group: str) -&gt; Optional[dict[str, bytes]]:\n    \"\"\"\n    Removes the group and all its keys and values and returns the data.\n    Returns None if the group does not exist.\n    \"\"\"\n    return self._data.pop(group, None)\n</code></pre>"},{"location":"api/mapper/#pynumaflow.mapper.UserMetadata.clear","title":"clear","text":"<pre><code>clear()\n</code></pre> <p>Clears all the groups and all their keys and values.</p> Source code in <code>pynumaflow/_metadata.py</code> <pre><code>def clear(self):\n    \"\"\"\n    Clears all the groups and all their keys and values.\n    \"\"\"\n    self._data.clear()\n</code></pre>"},{"location":"api/mapper/#pynumaflow.mapper.SystemMetadata","title":"SystemMetadata  <code>dataclass</code>","text":"<pre><code>SystemMetadata(_data: dict[str, dict[str, bytes]] = dict())\n</code></pre> <p>System metadata is the mapping of group name to key-value pairs for a given group. System metadata wraps the system-generated metadata groups per message. It is read-only to UDFs.</p>"},{"location":"api/mapper/#pynumaflow.mapper.SystemMetadata.groups","title":"groups","text":"<pre><code>groups() -&gt; list[str]\n</code></pre> <p>Returns the list of group names for the system metadata.</p> Source code in <code>pynumaflow/_metadata.py</code> <pre><code>def groups(self) -&gt; list[str]:\n    \"\"\"\n    Returns the list of group names for the system metadata.\n    \"\"\"\n    return list(self._data.keys())\n</code></pre>"},{"location":"api/mapper/#pynumaflow.mapper.SystemMetadata.keys","title":"keys","text":"<pre><code>keys(group: str) -&gt; list[str]\n</code></pre> <p>Returns the list of keys for a given group.</p> Source code in <code>pynumaflow/_metadata.py</code> <pre><code>def keys(self, group: str) -&gt; list[str]:\n    \"\"\"\n    Returns the list of keys for a given group.\n    \"\"\"\n    return list(self._data.get(group, {}).keys())\n</code></pre>"},{"location":"api/mapper/#pynumaflow.mapper.SystemMetadata.value","title":"value","text":"<pre><code>value(group: str, key: str) -&gt; Optional[bytes]\n</code></pre> <p>Returns the value for a given group and key.</p> Source code in <code>pynumaflow/_metadata.py</code> <pre><code>def value(self, group: str, key: str) -&gt; Optional[bytes]:\n    \"\"\"\n    Returns the value for a given group and key.\n    \"\"\"\n    return self._data.get(group, {}).get(key)\n</code></pre>"},{"location":"api/mapstreamer/","title":"Map Streamer","text":"<p>The Map Streamer module provides classes and functions for implementing MapStream UDFs that stream results as they're produced. Unlike regular Map which returns all messages at once, Map Stream yields messages one at a time as they're ready, reducing latency for downstream consumers.</p>"},{"location":"api/mapstreamer/#classes","title":"Classes","text":""},{"location":"api/mapstreamer/#pynumaflow.mapstreamer.Message","title":"Message  <code>dataclass</code>","text":"<pre><code>Message(\n    value: bytes,\n    keys: Optional[list[str]] = None,\n    tags: Optional[list[str]] = None,\n)\n</code></pre> <p>Basic datatype for data passing to the next vertex/vertices.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>bytes</code> <p>data in bytes</p> required <code>keys</code> <code>Optional[list[str]]</code> <p>list of keys for vertex (optional)</p> <code>None</code> <code>tags</code> <code>Optional[list[str]]</code> <p>list of tags for conditional forwarding (optional)</p> <code>None</code> Source code in <code>pynumaflow/mapstreamer/_dtypes.py</code> <pre><code>def __init__(\n    self, value: bytes, keys: Optional[list[str]] = None, tags: Optional[list[str]] = None\n):\n    \"\"\"\n    Creates a Message object to send value to a vertex.\n    \"\"\"\n    self._keys = keys or []\n    self._tags = tags or []\n    self._value = value or b\"\"\n</code></pre>"},{"location":"api/mapstreamer/#pynumaflow.mapstreamer.Messages","title":"Messages","text":"<pre><code>Messages(*messages: M)\n</code></pre> <p>               Bases: <code>Sequence[M]</code></p> <p>Class to define a list of Message objects.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>M</code> <p>list of Message objects.</p> <code>()</code> Source code in <code>pynumaflow/mapstreamer/_dtypes.py</code> <pre><code>def __init__(self, *messages: M):\n    self._messages = list(messages) or []\n</code></pre>"},{"location":"api/mapstreamer/#pynumaflow.mapstreamer.Datum","title":"Datum  <code>dataclass</code>","text":"<pre><code>Datum(\n    keys: list[str],\n    value: bytes,\n    event_time: datetime,\n    watermark: datetime,\n    headers: Optional[dict[str, str]] = None,\n)\n</code></pre> <p>Class to define the important information for the event.</p> <p>Parameters:</p> Name Type Description Default <code>keys</code> <code>list[str]</code> <p>the keys of the event.</p> required <code>value</code> <code>bytes</code> <p>the payload of the event.</p> required <code>event_time</code> <code>datetime</code> <p>the event time of the event.</p> required <code>watermark</code> <code>datetime</code> <p>the watermark of the event.</p> required <code>headers</code> <code>Optional[dict[str, str]]</code> <p>the headers of the event.</p> <code>None</code> <p>Example: <pre><code>from pynumaflow.mapstreamer import Datum\nfrom datetime import datetime, timezone\n\nd = Datum(\n      keys=[\"test_key\"],\n      value=b\"test_mock_message\",\n      event_time=datetime.fromtimestamp(1662998400, timezone.utc),\n      watermark=datetime.fromtimestamp(1662998460, timezone.utc),\n      headers={\"key1\": \"value1\", \"key2\": \"value2\"},\n   )\n</code></pre></p> Source code in <code>pynumaflow/mapstreamer/_dtypes.py</code> <pre><code>def __init__(\n    self,\n    keys: list[str],\n    value: bytes,\n    event_time: datetime,\n    watermark: datetime,\n    headers: Optional[dict[str, str]] = None,\n):\n    self._keys = keys or list()\n    self._value = value or b\"\"\n    if not isinstance(event_time, datetime):\n        raise TypeError(f\"Wrong data type: {type(event_time)} for Datum.event_time\")\n    self._event_time = event_time\n    if not isinstance(watermark, datetime):\n        raise TypeError(f\"Wrong data type: {type(watermark)} for Datum.watermark\")\n    self._watermark = watermark\n    self._headers = headers or {}\n</code></pre>"},{"location":"api/mapstreamer/#pynumaflow.mapstreamer.Datum.keys","title":"keys  <code>property</code>","text":"<pre><code>keys: list[str]\n</code></pre> <p>Returns the keys of the event</p>"},{"location":"api/mapstreamer/#pynumaflow.mapstreamer.Datum.value","title":"value  <code>property</code>","text":"<pre><code>value: bytes\n</code></pre> <p>Returns the value of the event.</p>"},{"location":"api/mapstreamer/#pynumaflow.mapstreamer.Datum.event_time","title":"event_time  <code>property</code>","text":"<pre><code>event_time: datetime\n</code></pre> <p>Returns the event time of the event.</p>"},{"location":"api/mapstreamer/#pynumaflow.mapstreamer.Datum.watermark","title":"watermark  <code>property</code>","text":"<pre><code>watermark: datetime\n</code></pre> <p>Returns the watermark of the event.</p>"},{"location":"api/mapstreamer/#pynumaflow.mapstreamer.Datum.headers","title":"headers  <code>property</code>","text":"<pre><code>headers: dict[str, str]\n</code></pre> <p>Returns the headers of the event.</p>"},{"location":"api/mapstreamer/#pynumaflow.mapstreamer.MapStreamer","title":"MapStreamer","text":"<p>Provides an interface to write a Map Streamer which will be exposed over a gRPC server.</p> <p>Args:</p>"},{"location":"api/mapstreamer/#pynumaflow.mapstreamer.MapStreamer.handler","title":"handler  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>handler(\n    keys: list[str], datum: Datum\n) -&gt; AsyncIterable[Message]\n</code></pre> <p>Implement this handler function which implements the MapSyncCallable interface.</p> Source code in <code>pynumaflow/mapstreamer/_dtypes.py</code> <pre><code>@abstractmethod\nasync def handler(self, keys: list[str], datum: Datum) -&gt; AsyncIterable[Message]:\n    \"\"\"\n    Implement this handler function which implements the MapSyncCallable interface.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/mapstreamer/#pynumaflow.mapstreamer.MapStreamAsyncServer","title":"MapStreamAsyncServer","text":"<pre><code>MapStreamAsyncServer(\n    map_stream_instance: MapStreamCallable,\n    sock_path=MAP_STREAM_SOCK_PATH,\n    max_message_size=MAX_MESSAGE_SIZE,\n    max_threads=NUM_THREADS_DEFAULT,\n    server_info_file=MAP_SERVER_INFO_FILE_PATH,\n)\n</code></pre> <p>               Bases: <code>NumaflowServer</code></p> <p>Class for a new Map Stream Server instance.</p> <p>The server instance is returned.</p> <p>Parameters:</p> Name Type Description Default <code>map_stream_instance</code> <code>MapStreamCallable</code> <p>The map stream instance to be used for Map Stream UDF</p> required <code>sock_path</code> <p>The UNIX socket path to be used for the server</p> <code>MAP_STREAM_SOCK_PATH</code> <code>max_message_size</code> <p>The max message size in bytes the server can receive and send</p> <code>MAX_MESSAGE_SIZE</code> <code>max_threads</code> <p>The max number of threads to be spawned;             defaults to 4 and max capped at 16</p> <code>NUM_THREADS_DEFAULT</code> <code>server_type</code> <p>The type of server to be used</p> required <p>Example invocation: <pre><code>import os\nfrom collections.abc import AsyncIterable\nfrom pynumaflow.mapstreamer import Message, Datum, MapStreamAsyncServer, MapStreamer\n\nclass FlatMapStream(MapStreamer):\n    async def handler(self, keys: list[str], datum: Datum) -&gt; AsyncIterable[Message]:\n        val = datum.value\n        _ = datum.event_time\n        _ = datum.watermark\n        strs = val.decode(\"utf-8\").split(\",\")\n\n        if len(strs) == 0:\n            yield Message.to_drop()\n            return\n        for s in strs:\n            yield Message(str.encode(s))\n\nasync def map_stream_handler(_: list[str], datum: Datum) -&gt; AsyncIterable[Message]:\n\n    val = datum.value\n    _ = datum.event_time\n    _ = datum.watermark\n    strs = val.decode(\"utf-8\").split(\",\")\n\n    if len(strs) == 0:\n        yield Message.to_drop()\n        return\n    for s in strs:\n        yield Message(str.encode(s))\n\nif __name__ == \"__main__\":\n    invoke = os.getenv(\"INVOKE\", \"func_handler\")\n    if invoke == \"class\":\n        handler = FlatMapStream()\n    else:\n        handler = map_stream_handler\n    grpc_server = MapStreamAsyncServer(handler)\n    grpc_server.start()\n</code></pre></p> Source code in <code>pynumaflow/mapstreamer/async_server.py</code> <pre><code>def __init__(\n    self,\n    map_stream_instance: MapStreamCallable,\n    sock_path=MAP_STREAM_SOCK_PATH,\n    max_message_size=MAX_MESSAGE_SIZE,\n    max_threads=NUM_THREADS_DEFAULT,\n    server_info_file=MAP_SERVER_INFO_FILE_PATH,\n):\n    \"\"\"\n    Create a new grpc Async Map Stream Server instance.\n    A new servicer instance is created and attached to the server.\n    The server instance is returned.\n\n    Args:\n        map_stream_instance: The map stream instance to be used for Map Stream UDF\n        sock_path: The UNIX socket path to be used for the server\n        max_message_size: The max message size in bytes the server can receive and send\n        max_threads: The max number of threads to be spawned;\n                        defaults to 4 and max capped at 16\n        server_type: The type of server to be used\n\n    Example invocation:\n    ```py\n    import os\n    from collections.abc import AsyncIterable\n    from pynumaflow.mapstreamer import Message, Datum, MapStreamAsyncServer, MapStreamer\n\n    class FlatMapStream(MapStreamer):\n        async def handler(self, keys: list[str], datum: Datum) -&gt; AsyncIterable[Message]:\n            val = datum.value\n            _ = datum.event_time\n            _ = datum.watermark\n            strs = val.decode(\"utf-8\").split(\",\")\n\n            if len(strs) == 0:\n                yield Message.to_drop()\n                return\n            for s in strs:\n                yield Message(str.encode(s))\n\n    async def map_stream_handler(_: list[str], datum: Datum) -&gt; AsyncIterable[Message]:\n\n        val = datum.value\n        _ = datum.event_time\n        _ = datum.watermark\n        strs = val.decode(\"utf-8\").split(\",\")\n\n        if len(strs) == 0:\n            yield Message.to_drop()\n            return\n        for s in strs:\n            yield Message(str.encode(s))\n\n    if __name__ == \"__main__\":\n        invoke = os.getenv(\"INVOKE\", \"func_handler\")\n        if invoke == \"class\":\n            handler = FlatMapStream()\n        else:\n            handler = map_stream_handler\n        grpc_server = MapStreamAsyncServer(handler)\n        grpc_server.start()\n    ```\n    \"\"\"\n    self.map_stream_instance: MapStreamCallable = map_stream_instance\n    self.sock_path = f\"unix://{sock_path}\"\n    self.max_threads = min(max_threads, MAX_NUM_THREADS)\n    self.max_message_size = max_message_size\n    self.server_info_file = server_info_file\n\n    self._server_options = [\n        (\"grpc.max_send_message_length\", self.max_message_size),\n        (\"grpc.max_receive_message_length\", self.max_message_size),\n    ]\n\n    self.servicer = AsyncMapStreamServicer(handler=self.map_stream_instance)\n</code></pre>"},{"location":"api/mapstreamer/#pynumaflow.mapstreamer.MapStreamAsyncServer.start","title":"start","text":"<pre><code>start()\n</code></pre> <p>Starter function for the Async Map Stream server, we need a separate caller to the aexec so that all the async coroutines can be started from a single context</p> Source code in <code>pynumaflow/mapstreamer/async_server.py</code> <pre><code>def start(self):\n    \"\"\"\n    Starter function for the Async Map Stream server, we need a separate caller\n    to the aexec so that all the async coroutines can be started from a single context\n    \"\"\"\n    aiorun.run(self.aexec(), use_uvloop=True)\n</code></pre>"},{"location":"api/mapstreamer/#pynumaflow.mapstreamer.MapStreamAsyncServer.aexec","title":"aexec  <code>async</code>","text":"<pre><code>aexec()\n</code></pre> <p>Starts the Async gRPC server on the given UNIX socket with given max threads.</p> Source code in <code>pynumaflow/mapstreamer/async_server.py</code> <pre><code>async def aexec(self):\n    \"\"\"\n    Starts the Async gRPC server on the given UNIX socket with\n    given max threads.\n    \"\"\"\n    # As the server is async, we need to create a new server instance in the\n    # same thread as the event loop so that all the async calls are made in the\n    # same context\n    # Create a new async server instance and add the servicer to it\n    server = grpc.aio.server(options=self._server_options)\n    server.add_insecure_port(self.sock_path)\n    map_pb2_grpc.add_MapServicer_to_server(\n        self.servicer,\n        server,\n    )\n    _LOGGER.info(\"Starting Map Stream Server\")\n    serv_info = ServerInfo.get_default_server_info()\n    serv_info.minimum_numaflow_version = MINIMUM_NUMAFLOW_VERSION[ContainerType.Mapper]\n    # Add the MAP_MODE metadata to the server info for the correct map mode\n    serv_info.metadata[MAP_MODE_KEY] = MapMode.StreamMap\n\n    # Start the async server\n    await start_async_server(\n        server_async=server,\n        sock_path=self.sock_path,\n        max_threads=self.max_threads,\n        cleanup_coroutines=list(),\n        server_info_file=self.server_info_file,\n        server_info=serv_info,\n    )\n</code></pre>"},{"location":"api/reducer/","title":"Reducer","text":"<p>The Reducer module provides classes and functions for implementing Reduce UDFs that aggregate messages by key within time windows. It's used for operations like counting, summing, or computing statistics over groups of messages.</p>"},{"location":"api/reducer/#classes","title":"Classes","text":""},{"location":"api/reducer/#pynumaflow.reducer.Message","title":"Message  <code>dataclass</code>","text":"<pre><code>Message(\n    value: bytes,\n    keys: list[str] = None,\n    tags: list[str] = None,\n)\n</code></pre> <p>Basic datatype for data passing to the next vertex/vertices.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>bytes</code> <p>data in bytes</p> required <code>keys</code> <code>list[str]</code> <p>[]string keys for vertex (optional)</p> <code>None</code> <code>tags</code> <code>list[str]</code> <p>[]string tags for conditional forwarding (optional)</p> <code>None</code> Source code in <code>pynumaflow/reducer/_dtypes.py</code> <pre><code>def __init__(self, value: bytes, keys: list[str] = None, tags: list[str] = None):\n    \"\"\"\n    Creates a Message object to send value to a vertex.\n    \"\"\"\n    self._keys = keys or []\n    self._tags = tags or []\n    self._value = value or b\"\"\n</code></pre>"},{"location":"api/reducer/#pynumaflow.reducer.Messages","title":"Messages","text":"<pre><code>Messages(*messages: M)\n</code></pre> <p>               Bases: <code>Sequence[M]</code></p> <p>Class to define a list of Message objects.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>M</code> <p>list of Message objects.</p> <code>()</code> Source code in <code>pynumaflow/reducer/_dtypes.py</code> <pre><code>def __init__(self, *messages: M):\n    self._messages = list(messages) or []\n</code></pre>"},{"location":"api/reducer/#pynumaflow.reducer.Datum","title":"Datum  <code>dataclass</code>","text":"<pre><code>Datum(\n    keys: list[str],\n    value: bytes,\n    event_time: datetime,\n    watermark: datetime,\n    headers: Optional[dict[str, str]] = None,\n)\n</code></pre> <p>Class to define the important information for the event.</p> <p>Parameters:</p> Name Type Description Default <code>keys</code> <code>list[str]</code> <p>the keys of the event.</p> required <code>value</code> <code>bytes</code> <p>the payload of the event.</p> required <code>event_time</code> <code>datetime</code> <p>the event time of the event.</p> required <code>watermark</code> <code>datetime</code> <p>the watermark of the event.</p> required <code>headers</code> <code>Optional[dict[str, str]]</code> <p>the headers of the event.</p> <code>None</code> <p>Example usage <pre><code>from pynumaflow.reducer import Datum\nfrom datetime import datetime, timezone\n\nd = Datum(\n      keys=[\"test_key\"],\n      value=b\"test_mock_message\",\n      event_time=datetime.fromtimestamp(1662998400, timezone.utc),\n      watermark=datetime.fromtimestamp(1662998460, timezone.utc),\n      headers={\"key1\": \"value1\", \"key2\": \"value2\"},\n   )\n</code></pre></p> Source code in <code>pynumaflow/reducer/_dtypes.py</code> <pre><code>def __init__(\n    self,\n    keys: list[str],\n    value: bytes,\n    event_time: datetime,\n    watermark: datetime,\n    headers: Optional[dict[str, str]] = None,\n):\n    self._keys = keys or list()\n    self._value = value or b\"\"\n    if not isinstance(event_time, datetime):\n        raise TypeError(f\"Wrong data type: {type(event_time)} for Datum.event_time\")\n    self._event_time = event_time\n    if not isinstance(watermark, datetime):\n        raise TypeError(f\"Wrong data type: {type(watermark)} for Datum.watermark\")\n    self._watermark = watermark\n    self._headers = headers or {}\n</code></pre>"},{"location":"api/reducer/#pynumaflow.reducer.Datum.keys","title":"keys  <code>property</code>","text":"<pre><code>keys: list[str]\n</code></pre> <p>Returns the keys of the event</p>"},{"location":"api/reducer/#pynumaflow.reducer.Datum.value","title":"value  <code>property</code>","text":"<pre><code>value: bytes\n</code></pre> <p>Returns the value of the event.</p>"},{"location":"api/reducer/#pynumaflow.reducer.Datum.event_time","title":"event_time  <code>property</code>","text":"<pre><code>event_time: datetime\n</code></pre> <p>Returns the event time of the event.</p>"},{"location":"api/reducer/#pynumaflow.reducer.Datum.watermark","title":"watermark  <code>property</code>","text":"<pre><code>watermark: datetime\n</code></pre> <p>Returns the watermark of the event.</p>"},{"location":"api/reducer/#pynumaflow.reducer.Datum.headers","title":"headers  <code>property</code>","text":"<pre><code>headers: dict[str, str]\n</code></pre> <p>Returns the headers of the event.</p>"},{"location":"api/reducer/#pynumaflow.reducer.IntervalWindow","title":"IntervalWindow  <code>dataclass</code>","text":"<pre><code>IntervalWindow(start: datetime, end: datetime)\n</code></pre> <p>Defines the start and end of the interval window for the event.</p> Source code in <code>pynumaflow/reducer/_dtypes.py</code> <pre><code>def __init__(self, start: datetime, end: datetime):\n    self._start = start\n    self._end = end\n</code></pre>"},{"location":"api/reducer/#pynumaflow.reducer.IntervalWindow.start","title":"start  <code>property</code>","text":"<pre><code>start\n</code></pre> <p>Returns the start point of the interval window.</p>"},{"location":"api/reducer/#pynumaflow.reducer.IntervalWindow.end","title":"end  <code>property</code>","text":"<pre><code>end\n</code></pre> <p>Returns the end point of the interval window.</p>"},{"location":"api/reducer/#pynumaflow.reducer.Metadata","title":"Metadata  <code>dataclass</code>","text":"<pre><code>Metadata(interval_window: IntervalWindow)\n</code></pre> <p>Defines the metadata for the event.</p> Source code in <code>pynumaflow/reducer/_dtypes.py</code> <pre><code>def __init__(self, interval_window: IntervalWindow):\n    self._interval_window = interval_window\n</code></pre>"},{"location":"api/reducer/#pynumaflow.reducer.Metadata.interval_window","title":"interval_window  <code>property</code>","text":"<pre><code>interval_window\n</code></pre> <p>Returns the interval window for the event.</p>"},{"location":"api/reducer/#pynumaflow.reducer.Reducer","title":"Reducer","text":"<p>Provides an interface to write a Reducer which will be exposed over a gRPC server.</p>"},{"location":"api/reducer/#pynumaflow.reducer.Reducer.handler","title":"handler  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>handler(\n    keys: list[str],\n    datums: AsyncIterable[Datum],\n    md: Metadata,\n) -&gt; Messages\n</code></pre> <p>Implement this handler function which implements the ReduceCallable interface.</p> Source code in <code>pynumaflow/reducer/_dtypes.py</code> <pre><code>@abstractmethod\nasync def handler(\n    self, keys: list[str], datums: AsyncIterable[Datum], md: Metadata\n) -&gt; Messages:\n    \"\"\"\n    Implement this handler function which implements the ReduceCallable interface.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/reducer/#pynumaflow.reducer.ReduceAsyncServer","title":"ReduceAsyncServer","text":"<pre><code>ReduceAsyncServer(\n    reducer_instance: ReduceCallable,\n    init_args: tuple = (),\n    init_kwargs: Optional[dict] = None,\n    sock_path=REDUCE_SOCK_PATH,\n    max_message_size=MAX_MESSAGE_SIZE,\n    max_threads=NUM_THREADS_DEFAULT,\n    server_info_file=REDUCE_SERVER_INFO_FILE_PATH,\n)\n</code></pre> <p>               Bases: <code>NumaflowServer</code></p> <p>Class for a new Reduce Server instance. A new servicer instance is created and attached to the server. The server instance is returned.</p> <p>Parameters:</p> Name Type Description Default <code>reducer_instance</code> <code>ReduceCallable</code> <p>The reducer instance to be used for Reduce UDF</p> required <code>sock_path</code> <p>The UNIX socket path to be used for the server</p> <code>REDUCE_SOCK_PATH</code> <code>max_message_size</code> <p>The max message size in bytes the server can receive and send</p> <code>MAX_MESSAGE_SIZE</code> <code>max_threads</code> <p>The max number of threads to be spawned;             defaults to 4 and max capped at 16</p> <code>NUM_THREADS_DEFAULT</code> <p>Example invocation: <pre><code>import os\nfrom collections.abc import AsyncIterable\nfrom pynumaflow.reducer import Messages, Message, Datum, Metadata,\nReduceAsyncServer, Reducer\n\nclass ReduceCounter(Reducer):\n    def __init__(self, counter):\n        self.counter = counter\n\n    async def handler(\n        self, keys: list[str], datums: AsyncIterable[Datum], md: Metadata\n    ) -&gt; Messages:\n        interval_window = md.interval_window\n        self.counter = 0\n        async for _ in datums:\n            self.counter += 1\n        msg = (\n            f\"counter:{self.counter} interval_window_start:{interval_window.start} \"\n            f\"interval_window_end:{interval_window.end}\"\n        )\n        return Messages(Message(str.encode(msg), keys=keys))\n\nasync def reduce_handler(\n    keys: list[str], datums: AsyncIterable[Datum], md: Metadata\n) -&gt; Messages:\n    interval_window = md.interval_window\n    counter = 0\n    async for _ in datums:\n        counter += 1\n    msg = (\n        f\"counter:{counter} interval_window_start:{interval_window.start} \"\n        f\"interval_window_end:{interval_window.end}\"\n    )\n    return Messages(Message(str.encode(msg), keys=keys))\n\nif __name__ == \"__main__\":\n    invoke = os.getenv(\"INVOKE\", \"func_handler\")\n    if invoke == \"class\":\n        # Here we are using the class instance as the reducer_instance\n        # which will be used to invoke the handler function.\n        # We are passing the init_args for the class instance.\n        grpc_server = ReduceAsyncServer(ReduceCounter, init_args=(0,))\n    else:\n        # Here we are using the handler function directly as the reducer_instance.\n        grpc_server = ReduceAsyncServer(reduce_handler)\n    grpc_server.start()\n</code></pre></p> Source code in <code>pynumaflow/reducer/async_server.py</code> <pre><code>def __init__(\n    self,\n    reducer_instance: ReduceCallable,\n    init_args: tuple = (),\n    init_kwargs: Optional[dict] = None,\n    sock_path=REDUCE_SOCK_PATH,\n    max_message_size=MAX_MESSAGE_SIZE,\n    max_threads=NUM_THREADS_DEFAULT,\n    server_info_file=REDUCE_SERVER_INFO_FILE_PATH,\n):\n    init_kwargs = init_kwargs or {}\n    self.reducer_handler = get_handler(reducer_instance, init_args, init_kwargs)\n    self.sock_path = f\"unix://{sock_path}\"\n    self.max_message_size = max_message_size\n    self.max_threads = min(max_threads, MAX_NUM_THREADS)\n    self.server_info_file = server_info_file\n\n    self._server_options = [\n        (\"grpc.max_send_message_length\", self.max_message_size),\n        (\"grpc.max_receive_message_length\", self.max_message_size),\n    ]\n    # Get the servicer instance for the async server\n    self.servicer = AsyncReduceServicer(self.reducer_handler)\n</code></pre>"},{"location":"api/reducer/#pynumaflow.reducer.ReduceAsyncServer.start","title":"start","text":"<pre><code>start()\n</code></pre> <p>Starter function for the Async server class, need a separate caller so that all the async coroutines can be started from a single context</p> Source code in <code>pynumaflow/reducer/async_server.py</code> <pre><code>def start(self):\n    \"\"\"\n    Starter function for the Async server class, need a separate caller\n    so that all the async coroutines can be started from a single context\n    \"\"\"\n    _LOGGER.info(\n        \"Starting Async Reduce Server\",\n    )\n    aiorun.run(self.aexec(), use_uvloop=True)\n</code></pre>"},{"location":"api/reducer/#pynumaflow.reducer.ReduceAsyncServer.aexec","title":"aexec  <code>async</code>","text":"<pre><code>aexec()\n</code></pre> <p>Starts the Async gRPC server on the given UNIX socket with given max threads.</p> Source code in <code>pynumaflow/reducer/async_server.py</code> <pre><code>async def aexec(self):\n    \"\"\"\n    Starts the Async gRPC server on the given UNIX socket with\n    given max threads.\n    \"\"\"\n    # As the server is async, we need to create a new server instance in the\n    # same thread as the event loop so that all the async calls are made in the\n    # same context\n    # Create a new async server instance and add the servicer to it\n    server = grpc.aio.server(options=self._server_options)\n    server.add_insecure_port(self.sock_path)\n    reduce_servicer = self.servicer\n    reduce_pb2_grpc.add_ReduceServicer_to_server(reduce_servicer, server)\n\n    serv_info = ServerInfo.get_default_server_info()\n    serv_info.minimum_numaflow_version = MINIMUM_NUMAFLOW_VERSION[ContainerType.Reducer]\n    # Start the async server\n    await start_async_server(\n        server_async=server,\n        sock_path=self.sock_path,\n        max_threads=self.max_threads,\n        cleanup_coroutines=list(),\n        server_info_file=self.server_info_file,\n        server_info=serv_info,\n    )\n</code></pre>"},{"location":"api/reducestreamer/","title":"Reduce Streamer","text":"<p>The Reduce Streamer module provides classes and functions for implementing ReduceStream UDFs that emit results incrementally during reduction. Unlike regular Reduce which outputs only when the window closes, Reduce Stream emits results as they're computed. This is useful for early alerts or real-time dashboards.</p>"},{"location":"api/reducestreamer/#classes","title":"Classes","text":""},{"location":"api/reducestreamer/#pynumaflow.reducestreamer.Message","title":"Message  <code>dataclass</code>","text":"<pre><code>Message(\n    value: bytes,\n    keys: list[str] = None,\n    tags: list[str] = None,\n)\n</code></pre> <p>Basic datatype for data passing to the next vertex/vertices.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>bytes</code> <p>data in bytes</p> required <code>keys</code> <code>list[str]</code> <p>[]string keys for vertex (optional)</p> <code>None</code> <code>tags</code> <code>list[str]</code> <p>[]string tags for conditional forwarding (optional)</p> <code>None</code> Source code in <code>pynumaflow/reducestreamer/_dtypes.py</code> <pre><code>def __init__(\n    self,\n    value: bytes,\n    keys: list[str] = None,\n    tags: list[str] = None,\n):\n    \"\"\"\n    Creates a Message object to send value to a vertex.\n    \"\"\"\n    self._keys = keys or []\n    self._tags = tags or []\n    self._value = value or b\"\"\n</code></pre>"},{"location":"api/reducestreamer/#pynumaflow.reducestreamer.Datum","title":"Datum  <code>dataclass</code>","text":"<pre><code>Datum(\n    keys: list[str],\n    value: bytes,\n    event_time: datetime,\n    watermark: datetime,\n    headers: Optional[dict[str, str]] = None,\n)\n</code></pre> <p>Class to define the important information for the event.</p> <p>Parameters:</p> Name Type Description Default <code>keys</code> <code>list[str]</code> <p>the keys of the event.</p> required <code>value</code> <code>bytes</code> <p>the payload of the event.</p> required <code>event_time</code> <code>datetime</code> <p>the event time of the event.</p> required <code>watermark</code> <code>datetime</code> <p>the watermark of the event.</p> required <p>Example usage</p> <pre><code>from pynumaflow.reducer import Datum\nfrom datetime import datetime, timezone\n\nd = Datum(\n      keys=[\"test_key\"],\n      value=\"test_mock_message\".encode(),\n      event_time=datetime.fromtimestamp(1662998400, timezone.utc),\n      watermark=datetime.fromtimestamp(1662998460, timezone.utc),\n      headers={\"key1\": \"value1\", \"key2\": \"value2\"}\n   )\n</code></pre> Source code in <code>pynumaflow/reducestreamer/_dtypes.py</code> <pre><code>def __init__(\n    self,\n    keys: list[str],\n    value: bytes,\n    event_time: datetime,\n    watermark: datetime,\n    headers: Optional[dict[str, str]] = None,\n):\n    self._keys = keys or list()\n    self._value = value or b\"\"\n    if not isinstance(event_time, datetime):\n        raise TypeError(f\"Wrong data type: {type(event_time)} for Datum.event_time\")\n    self._event_time = event_time\n    if not isinstance(watermark, datetime):\n        raise TypeError(f\"Wrong data type: {type(watermark)} for Datum.watermark\")\n    self._watermark = watermark\n    self._headers = headers or {}\n</code></pre>"},{"location":"api/reducestreamer/#pynumaflow.reducestreamer.Datum.keys","title":"keys  <code>property</code>","text":"<pre><code>keys: list[str]\n</code></pre> <p>Returns the keys of the event</p>"},{"location":"api/reducestreamer/#pynumaflow.reducestreamer.Datum.value","title":"value  <code>property</code>","text":"<pre><code>value: bytes\n</code></pre> <p>Returns the value of the event.</p>"},{"location":"api/reducestreamer/#pynumaflow.reducestreamer.Datum.event_time","title":"event_time  <code>property</code>","text":"<pre><code>event_time: datetime\n</code></pre> <p>Returns the event time of the event.</p>"},{"location":"api/reducestreamer/#pynumaflow.reducestreamer.Datum.watermark","title":"watermark  <code>property</code>","text":"<pre><code>watermark: datetime\n</code></pre> <p>Returns the watermark of the event.</p>"},{"location":"api/reducestreamer/#pynumaflow.reducestreamer.Datum.headers","title":"headers  <code>property</code>","text":"<pre><code>headers: dict[str, str]\n</code></pre> <p>Returns the headers of the event.</p>"},{"location":"api/reducestreamer/#pynumaflow.reducestreamer.IntervalWindow","title":"IntervalWindow  <code>dataclass</code>","text":"<pre><code>IntervalWindow(start: datetime, end: datetime)\n</code></pre> <p>Defines the start and end of the interval window for the event.</p> Source code in <code>pynumaflow/reducestreamer/_dtypes.py</code> <pre><code>def __init__(self, start: datetime, end: datetime):\n    self._start = start\n    self._end = end\n</code></pre>"},{"location":"api/reducestreamer/#pynumaflow.reducestreamer.IntervalWindow.start","title":"start  <code>property</code>","text":"<pre><code>start\n</code></pre> <p>Returns the start point of the interval window.</p>"},{"location":"api/reducestreamer/#pynumaflow.reducestreamer.IntervalWindow.end","title":"end  <code>property</code>","text":"<pre><code>end\n</code></pre> <p>Returns the end point of the interval window.</p>"},{"location":"api/reducestreamer/#pynumaflow.reducestreamer.Metadata","title":"Metadata  <code>dataclass</code>","text":"<pre><code>Metadata(interval_window: IntervalWindow)\n</code></pre> <p>Defines the metadata for the event.</p> Source code in <code>pynumaflow/reducestreamer/_dtypes.py</code> <pre><code>def __init__(self, interval_window: IntervalWindow):\n    self._interval_window = interval_window\n</code></pre>"},{"location":"api/reducestreamer/#pynumaflow.reducestreamer.Metadata.interval_window","title":"interval_window  <code>property</code>","text":"<pre><code>interval_window\n</code></pre> <p>Returns the interval window for the event.</p>"},{"location":"api/reducestreamer/#pynumaflow.reducestreamer.ReduceStreamer","title":"ReduceStreamer","text":"<p>Provides an interface to write a ReduceStreamer which will be exposed over a gRPC server.</p>"},{"location":"api/reducestreamer/#pynumaflow.reducestreamer.ReduceStreamer.handler","title":"handler  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>handler(\n    keys: list[str],\n    datums: AsyncIterable[Datum],\n    output: NonBlockingIterator,\n    md: Metadata,\n)\n</code></pre> <p>Implement this handler function which implements the ReduceStreamCallable interface.</p> Source code in <code>pynumaflow/reducestreamer/_dtypes.py</code> <pre><code>@abstractmethod\nasync def handler(\n    self,\n    keys: list[str],\n    datums: AsyncIterable[Datum],\n    output: NonBlockingIterator,\n    md: Metadata,\n):\n    \"\"\"\n    Implement this handler function which implements the ReduceStreamCallable interface.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/reducestreamer/#pynumaflow.reducestreamer.ReduceWindow","title":"ReduceWindow  <code>dataclass</code>","text":"<pre><code>ReduceWindow(\n    start: datetime, end: datetime, slot: str = \"\"\n)\n</code></pre> <p>Defines the window for a reduce operation which includes the interval window along with the slot.</p> Source code in <code>pynumaflow/reducestreamer/_dtypes.py</code> <pre><code>def __init__(self, start: datetime, end: datetime, slot: str = \"\"):\n    self._window = IntervalWindow(start=start, end=end)\n    self._slot = slot\n</code></pre>"},{"location":"api/reducestreamer/#pynumaflow.reducestreamer.ReduceWindow.start","title":"start  <code>property</code>","text":"<pre><code>start\n</code></pre> <p>Returns the start point of the interval window.</p>"},{"location":"api/reducestreamer/#pynumaflow.reducestreamer.ReduceWindow.end","title":"end  <code>property</code>","text":"<pre><code>end\n</code></pre> <p>Returns the end point of the interval window.</p>"},{"location":"api/reducestreamer/#pynumaflow.reducestreamer.ReduceWindow.slot","title":"slot  <code>property</code>","text":"<pre><code>slot\n</code></pre> <p>Returns the slot from the window</p>"},{"location":"api/reducestreamer/#pynumaflow.reducestreamer.ReduceWindow.window","title":"window  <code>property</code>","text":"<pre><code>window\n</code></pre> <p>Return the interval window</p>"},{"location":"api/reducestreamer/#pynumaflow.reducestreamer.ReduceStreamAsyncServer","title":"ReduceStreamAsyncServer","text":"<pre><code>ReduceStreamAsyncServer(\n    reduce_stream_instance: ReduceStreamCallable,\n    init_args: tuple = (),\n    init_kwargs: Optional[dict] = None,\n    sock_path=REDUCE_STREAM_SOCK_PATH,\n    max_message_size=MAX_MESSAGE_SIZE,\n    max_threads=NUM_THREADS_DEFAULT,\n    server_info_file=REDUCE_STREAM_SERVER_INFO_FILE_PATH,\n)\n</code></pre> <p>               Bases: <code>NumaflowServer</code></p> <p>Class for a new Reduce Stream Server instance. A new servicer instance is created and attached to the server. The server instance is returned.</p> <p>Parameters:</p> Name Type Description Default <code>reduce_stream_instance</code> <code>ReduceStreamCallable</code> <p>The reducer instance to be used for     Reduce Streaming UDF</p> required <code>init_args</code> <code>tuple</code> <p>The arguments to be passed to the reduce_stream_handler</p> <code>()</code> <code>init_kwargs</code> <code>Optional[dict]</code> <p>The keyword arguments to be passed to the reduce_stream_handler</p> <code>None</code> <code>sock_path</code> <p>The UNIX socket path to be used for the server</p> <code>REDUCE_STREAM_SOCK_PATH</code> <code>max_message_size</code> <p>The max message size in bytes the server can receive and send</p> <code>MAX_MESSAGE_SIZE</code> <code>max_threads</code> <p>The max number of threads to be spawned;</p> <code>NUM_THREADS_DEFAULT</code> <code>server_info_file</code> <p>The path to the server info file</p> <code>REDUCE_STREAM_SERVER_INFO_FILE_PATH</code> <p>Example invocation: <pre><code>import os\nfrom collections.abc import AsyncIterable\nfrom pynumaflow.reducestreamer import Messages, Message, Datum, Metadata,\nReduceStreamAsyncServer, ReduceStreamer\n\nclass ReduceCounter(ReduceStreamer):\n    def __init__(self, counter):\n        self.counter = counter\n\n    async def handler(\n        self,\n        keys: list[str],\n        datums: AsyncIterable[Datum],\n        output: NonBlockingIterator,\n        md: Metadata,\n    ):\n        async for _ in datums:\n            self.counter += 1\n            if self.counter &gt; 20:\n                msg = f\"counter:{self.counter}\"\n                await output.put(Message(str.encode(msg), keys=keys))\n                self.counter = 0\n        msg = f\"counter:{self.counter}\"\n        await output.put(Message(str.encode(msg), keys=keys))\n\nasync def reduce_handler(\n        keys: list[str],\n        datums: AsyncIterable[Datum],\n        output: NonBlockingIterator,\n        md: Metadata,\n    ):\n    counter = 0\n    async for _ in datums:\n        counter += 1\n        if counter &gt; 20:\n            msg = f\"counter:{counter}\"\n            await output.put(Message(str.encode(msg), keys=keys))\n            counter = 0\n    msg = f\"counter:{counter}\"\n    await output.put(Message(str.encode(msg), keys=keys))\n\nif __name__ == \"__main__\":\n    invoke = os.getenv(\"INVOKE\", \"func_handler\")\n    if invoke == \"class\":\n        # Here we are using the class instance as the reducer_instance\n        # which will be used to invoke the handler function.\n        # We are passing the init_args for the class instance.\n        grpc_server = ReduceStreamAsyncServer(ReduceCounter, init_args=(0,))\n    else:\n        # Here we are using the handler function directly as the reducer_instance.\n        grpc_server = ReduceStreamAsyncServer(reduce_handler)\n    grpc_server.start()\n</code></pre></p> Source code in <code>pynumaflow/reducestreamer/async_server.py</code> <pre><code>def __init__(\n    self,\n    reduce_stream_instance: ReduceStreamCallable,\n    init_args: tuple = (),\n    init_kwargs: Optional[dict] = None,\n    sock_path=REDUCE_STREAM_SOCK_PATH,\n    max_message_size=MAX_MESSAGE_SIZE,\n    max_threads=NUM_THREADS_DEFAULT,\n    server_info_file=REDUCE_STREAM_SERVER_INFO_FILE_PATH,\n):\n    init_kwargs = init_kwargs or {}\n    self.reduce_stream_handler = get_handler(reduce_stream_instance, init_args, init_kwargs)\n    self.sock_path = f\"unix://{sock_path}\"\n    self.max_message_size = max_message_size\n    self.max_threads = min(max_threads, MAX_NUM_THREADS)\n    self.server_info_file = server_info_file\n\n    self._server_options = [\n        (\"grpc.max_send_message_length\", self.max_message_size),\n        (\"grpc.max_receive_message_length\", self.max_message_size),\n    ]\n    # Get the servicer instance for the async server\n    self.servicer = AsyncReduceStreamServicer(self.reduce_stream_handler)\n</code></pre>"},{"location":"api/reducestreamer/#pynumaflow.reducestreamer.ReduceStreamAsyncServer.start","title":"start","text":"<pre><code>start()\n</code></pre> <p>Starter function for the Async server class, need a separate caller so that all the async coroutines can be started from a single context</p> Source code in <code>pynumaflow/reducestreamer/async_server.py</code> <pre><code>def start(self):\n    \"\"\"\n    Starter function for the Async server class, need a separate caller\n    so that all the async coroutines can be started from a single context\n    \"\"\"\n    _LOGGER.info(\n        \"Starting Async Reduce Stream Server\",\n    )\n    aiorun.run(self.aexec(), use_uvloop=True)\n</code></pre>"},{"location":"api/reducestreamer/#pynumaflow.reducestreamer.ReduceStreamAsyncServer.aexec","title":"aexec  <code>async</code>","text":"<pre><code>aexec()\n</code></pre> <p>Starts the Async gRPC server on the given UNIX socket with given max threads.</p> Source code in <code>pynumaflow/reducestreamer/async_server.py</code> <pre><code>async def aexec(self):\n    \"\"\"\n    Starts the Async gRPC server on the given UNIX socket with\n    given max threads.\n    \"\"\"\n    # As the server is async, we need to create a new server instance in the\n    # same thread as the event loop so that all the async calls are made in the\n    # same context\n    # Create a new async server instance and add the servicer to it\n    server = grpc.aio.server(options=self._server_options)\n    server.add_insecure_port(self.sock_path)\n    reduce_pb2_grpc.add_ReduceServicer_to_server(self.servicer, server)\n\n    serv_info = ServerInfo.get_default_server_info()\n    serv_info.minimum_numaflow_version = MINIMUM_NUMAFLOW_VERSION[ContainerType.Reducestreamer]\n    await start_async_server(\n        server_async=server,\n        sock_path=self.sock_path,\n        max_threads=self.max_threads,\n        cleanup_coroutines=list(),\n        server_info_file=self.server_info_file,\n        server_info=serv_info,\n    )\n</code></pre>"},{"location":"api/sideinput/","title":"Side Input","text":"<p>Side Input allows you to inject external data into your UDFs. This is useful for configuration, lookup tables, or any data that UDFs need but isn't part of the main data stream.</p>"},{"location":"api/sideinput/#classes","title":"Classes","text":""},{"location":"api/sideinput/#pynumaflow.sideinput.Response","title":"Response  <code>dataclass</code>","text":"<pre><code>Response(value: bytes, no_broadcast: bool)\n</code></pre> <p>Class to define the important information for the event.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>bytes</code> <p>the payload of the event.</p> required <code>no_broadcast</code> <code>bool</code> <p>the flag to indicate whether the event should be broadcasted.</p> required <p>Example usage <pre><code>from pynumaflow.sideinput import Response\n\nResponse.broadcast_message(b\"hello\")\nResponse.no_broadcast_message()\n</code></pre></p>"},{"location":"api/sideinput/#pynumaflow.sideinput.Response.broadcast_message","title":"broadcast_message  <code>classmethod</code>","text":"<pre><code>broadcast_message(value: bytes) -&gt; R\n</code></pre> <p>Returns a SideInputResponse object with the given value, and the No broadcast flag set to False. This event will be broadcasted.</p> Source code in <code>pynumaflow/sideinput/_dtypes.py</code> <pre><code>@classmethod\ndef broadcast_message(cls: type[R], value: bytes) -&gt; R:\n    \"\"\"\n    Returns a SideInputResponse object with the given value,\n    and the No broadcast flag set to False. This event will be broadcasted.\n    \"\"\"\n    return Response(value=value, no_broadcast=False)\n</code></pre>"},{"location":"api/sideinput/#pynumaflow.sideinput.Response.no_broadcast_message","title":"no_broadcast_message  <code>classmethod</code>","text":"<pre><code>no_broadcast_message() -&gt; R\n</code></pre> <p>Returns a SideInputResponse object with the No broadcast flag set to True. This event will not be broadcasted.</p> Source code in <code>pynumaflow/sideinput/_dtypes.py</code> <pre><code>@classmethod\ndef no_broadcast_message(cls: type[R]) -&gt; R:\n    \"\"\"\n    Returns a SideInputResponse object with the No broadcast flag set to True.\n    This event will not be broadcasted.\n    \"\"\"\n    return Response(value=b\"\", no_broadcast=True)\n</code></pre>"},{"location":"api/sideinput/#pynumaflow.sideinput.SideInput","title":"SideInput","text":"<p>Provides an interface to write a SideInput Class which will be exposed over gRPC.</p>"},{"location":"api/sideinput/#pynumaflow.sideinput.SideInput.retrieve_handler","title":"retrieve_handler  <code>abstractmethod</code>","text":"<pre><code>retrieve_handler() -&gt; Response\n</code></pre> <p>This function is called when a Side Input request is received.</p> Source code in <code>pynumaflow/sideinput/_dtypes.py</code> <pre><code>@abstractmethod\ndef retrieve_handler(self) -&gt; Response:\n    \"\"\"\n    This function is called when a Side Input request is received.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/sideinput/#pynumaflow.sideinput.SideInputServer","title":"SideInputServer","text":"<pre><code>SideInputServer(\n    side_input_instance: RetrieverCallable,\n    sock_path=SIDE_INPUT_SOCK_PATH,\n    max_message_size=MAX_MESSAGE_SIZE,\n    max_threads=NUM_THREADS_DEFAULT,\n    side_input_dir_path=SIDE_INPUT_DIR_PATH,\n    server_info_file=SIDE_INPUT_SERVER_INFO_FILE_PATH,\n)\n</code></pre> <p>               Bases: <code>NumaflowServer</code></p> <p>Class for a new Side Input Server instance.</p> <p>Parameters:</p> Name Type Description Default <code>side_input_instance</code> <code>RetrieverCallable</code> <p>The side input instance to be used for Side Input UDF</p> required <code>sock_path</code> <p>The UNIX socket path to be used for the server</p> <code>SIDE_INPUT_SOCK_PATH</code> <code>max_message_size</code> <p>The max message size in bytes the server can receive and send</p> <code>MAX_MESSAGE_SIZE</code> <code>max_threads</code> <p>The max number of threads to be spawned;</p> <code>NUM_THREADS_DEFAULT</code> <p>Example invocation: <pre><code>import datetime\nfrom pynumaflow.sideinput import Response, SideInputServer, SideInput\n\nclass ExampleSideInput(SideInput):\n    def __init__(self):\n        self.counter = 0\n\n    def retrieve_handler(self) -&gt; Response:\n        time_now = datetime.datetime.now()\n        # val is the value to be broadcasted\n        val = f\"an example: {str(time_now)}\"\n        self.counter += 1\n        # broadcast every other time\n        if self.counter % 2 == 0:\n            # no_broadcast_message() is used to indicate that there is no broadcast\n            return Response.no_broadcast_message()\n        # broadcast_message() is used to indicate that there is a broadcast\n        return Response.broadcast_message(val.encode(\"utf-8\"))\n\nif __name__ == \"__main__\":\n    grpc_server = SideInputServer(ExampleSideInput())\n    grpc_server.start()\n</code></pre></p> Source code in <code>pynumaflow/sideinput/server.py</code> <pre><code>def __init__(\n    self,\n    side_input_instance: RetrieverCallable,\n    sock_path=SIDE_INPUT_SOCK_PATH,\n    max_message_size=MAX_MESSAGE_SIZE,\n    max_threads=NUM_THREADS_DEFAULT,\n    side_input_dir_path=SIDE_INPUT_DIR_PATH,\n    server_info_file=SIDE_INPUT_SERVER_INFO_FILE_PATH,\n):\n    self.sock_path = f\"unix://{sock_path}\"\n    self.max_threads = min(max_threads, MAX_NUM_THREADS)\n    self.max_message_size = max_message_size\n    self.server_info_file = server_info_file\n\n    self._server_options = [\n        (\"grpc.max_send_message_length\", self.max_message_size),\n        (\"grpc.max_receive_message_length\", self.max_message_size),\n    ]\n\n    self.side_input_instance = side_input_instance\n    self.side_input_dir_path = side_input_dir_path\n    self.servicer = SideInputServicer(side_input_instance)\n</code></pre>"},{"location":"api/sideinput/#pynumaflow.sideinput.SideInputServer.start","title":"start","text":"<pre><code>start()\n</code></pre> <p>Starts the Synchronous gRPC server on the given UNIX socket with given max threads.</p> Source code in <code>pynumaflow/sideinput/server.py</code> <pre><code>def start(self):\n    \"\"\"\n    Starts the Synchronous gRPC server on the given UNIX socket with given max threads.\n    \"\"\"\n    # Get the servicer instance based on the server type\n    side_input_servicer = self.servicer\n\n    _LOGGER.info(\n        \"Side Input GRPC Server listening on: %s with max threads: %s\",\n        self.sock_path,\n        self.max_threads,\n    )\n\n    serv_info = ServerInfo.get_default_server_info()\n    serv_info.minimum_numaflow_version = MINIMUM_NUMAFLOW_VERSION[ContainerType.Sideinput]\n    # Start the server\n    sync_server_start(\n        servicer=side_input_servicer,\n        bind_address=self.sock_path,\n        max_threads=self.max_threads,\n        server_info_file=self.server_info_file,\n        server_options=self._server_options,\n        udf_type=UDFType.SideInput,\n        server_info=serv_info,\n    )\n</code></pre>"},{"location":"api/sinker/","title":"Sinker","text":"<p>The Sinker module provides classes and functions for implementing User Defined Sinks that write processed data to external systems ((database, kafka topic, etc.)).</p>"},{"location":"api/sinker/#classes","title":"Classes","text":""},{"location":"api/sinker/#pynumaflow.sinker.SinkAsyncServer","title":"SinkAsyncServer","text":"<pre><code>SinkAsyncServer(\n    sinker_instance: SinkAsyncCallable,\n    sock_path=SINK_SOCK_PATH,\n    max_message_size=MAX_MESSAGE_SIZE,\n    max_threads=NUM_THREADS_DEFAULT,\n    server_info_file=SINK_SERVER_INFO_FILE_PATH,\n)\n</code></pre> <p>               Bases: <code>NumaflowServer</code></p> <p>SinkAsyncServer is the main class to start a gRPC server for a sinker. Create a new grpc Async Sink Server instance. A new servicer instance is created and attached to the server. The server instance is returned.</p> <p>Parameters:</p> Name Type Description Default <code>sinker_instance</code> <code>SinkAsyncCallable</code> <p>The sinker instance to be used for Sink UDF</p> required <code>sock_path</code> <p>The UNIX socket path to be used for the server</p> <code>SINK_SOCK_PATH</code> <code>max_message_size</code> <p>The max message size in bytes the server can receive and send</p> <code>MAX_MESSAGE_SIZE</code> <code>max_threads</code> <p>The max number of threads to be spawned;             defaults to 4 and max capped at 16</p> <code>NUM_THREADS_DEFAULT</code> <p>Example invocation: <pre><code>import os\nimport logging\nfrom collections.abc import AsyncIterable\nfrom pynumaflow.sinker import Datum, Responses, Response, Sinker\nfrom pynumaflow.sinker import SinkAsyncServer\nfrom pynumaflow._constants import _LOGGER\n\nlogging.basicConfig(level=logging.INFO)\n\nclass UserDefinedSink(Sinker):\n    async def handler(self, datums: AsyncIterable[Datum]) -&gt; Responses:\n        responses = Responses()\n        async for msg in datums:\n            logging.info(\"User Defined Sink %s\", msg.value.decode(\"utf-8\"))\n            responses.append(Response.as_success(msg.id))\n        return responses\n\n\nasync def udsink_handler(datums: AsyncIterable[Datum]) -&gt; Responses:\n    responses = Responses()\n    async for msg in datums:\n        logging.info(\"User Defined Sink %s\", msg.value.decode(\"utf-8\"))\n        responses.append(Response.as_success(msg.id))\n    return responses\n\n\nif __name__ == \"__main__\":\n    invoke = os.getenv(\"INVOKE\", \"func_handler\")\n    if invoke == \"class\":\n        sink_handler = UserDefinedSink()\n    else:\n        sink_handler = udsink_handler\n    grpc_server = SinkAsyncServer(sink_handler)\n    grpc_server.start()\n</code></pre></p> Source code in <code>pynumaflow/sinker/async_server.py</code> <pre><code>def __init__(\n    self,\n    sinker_instance: SinkAsyncCallable,\n    sock_path=SINK_SOCK_PATH,\n    max_message_size=MAX_MESSAGE_SIZE,\n    max_threads=NUM_THREADS_DEFAULT,\n    server_info_file=SINK_SERVER_INFO_FILE_PATH,\n):\n    # If the container type is fallback sink, then use the fallback sink address and path.\n    if os.getenv(ENV_UD_CONTAINER_TYPE, \"\") == UD_CONTAINER_FALLBACK_SINK:\n        _LOGGER.info(\"Using Fallback Sink\")\n        sock_path = FALLBACK_SINK_SOCK_PATH\n        server_info_file = FALLBACK_SINK_SERVER_INFO_FILE_PATH\n    elif os.getenv(ENV_UD_CONTAINER_TYPE, \"\") == UD_CONTAINER_ON_SUCCESS_SINK:\n        _LOGGER.info(\"Using On Success Sink\")\n        sock_path = ON_SUCCESS_SINK_SOCK_PATH\n        server_info_file = ON_SUCCESS_SINK_SERVER_INFO_FILE_PATH\n\n    self.sock_path = f\"unix://{sock_path}\"\n    self.max_threads = min(max_threads, MAX_NUM_THREADS)\n    self.max_message_size = max_message_size\n    self.server_info_file = server_info_file\n\n    self.sinker_instance = sinker_instance\n\n    self._server_options = [\n        (\"grpc.max_send_message_length\", self.max_message_size),\n        (\"grpc.max_receive_message_length\", self.max_message_size),\n    ]\n\n    self.servicer = AsyncSinkServicer(sinker_instance)\n</code></pre>"},{"location":"api/sinker/#pynumaflow.sinker.SinkAsyncServer.start","title":"start","text":"<pre><code>start()\n</code></pre> <p>Starter function for the Async server class, need a separate caller so that all the async coroutines can be started from a single context</p> Source code in <code>pynumaflow/sinker/async_server.py</code> <pre><code>def start(self):\n    \"\"\"\n    Starter function for the Async server class, need a separate caller\n    so that all the async coroutines can be started from a single context\n    \"\"\"\n    aiorun.run(self.aexec(), use_uvloop=True)\n</code></pre>"},{"location":"api/sinker/#pynumaflow.sinker.SinkAsyncServer.aexec","title":"aexec  <code>async</code>","text":"<pre><code>aexec()\n</code></pre> <p>Starts the Asynchronous gRPC server on the given UNIX socket with given max threads.</p> Source code in <code>pynumaflow/sinker/async_server.py</code> <pre><code>async def aexec(self):\n    \"\"\"\n    Starts the Asynchronous gRPC server on the given UNIX socket with given max threads.\n    \"\"\"\n    # As the server is async, we need to create a new server instance in the\n    # same thread as the event loop so that all the async calls are made in the\n    # same context\n    # Create a new server instance, add the servicer to it and start the server\n    server = grpc.aio.server(options=self._server_options)\n    server.add_insecure_port(self.sock_path)\n    sink_pb2_grpc.add_SinkServicer_to_server(self.servicer, server)\n    serv_info = ServerInfo.get_default_server_info()\n    serv_info.minimum_numaflow_version = MINIMUM_NUMAFLOW_VERSION[ContainerType.Sinker]\n    await start_async_server(\n        server_async=server,\n        sock_path=self.sock_path,\n        max_threads=self.max_threads,\n        cleanup_coroutines=list(),\n        server_info_file=self.server_info_file,\n        server_info=serv_info,\n    )\n</code></pre>"},{"location":"api/sinker/#pynumaflow.sinker.SinkServer","title":"SinkServer","text":"<pre><code>SinkServer(\n    sinker_instance: SinkSyncCallable,\n    sock_path=SINK_SOCK_PATH,\n    max_message_size=MAX_MESSAGE_SIZE,\n    max_threads=NUM_THREADS_DEFAULT,\n    server_info_file=SINK_SERVER_INFO_FILE_PATH,\n)\n</code></pre> <p>               Bases: <code>NumaflowServer</code></p> <p>SinkServer is the main class to start a gRPC server for a sinker.</p> <p>The server instance is returned.</p> <p>Parameters:</p> Name Type Description Default <code>sinker_instance</code> <code>SinkSyncCallable</code> <p>The sinker instance to be used for Sink UDF</p> required <code>sock_path</code> <p>The UNIX socket path to be used for the server</p> <code>SINK_SOCK_PATH</code> <code>max_message_size</code> <p>The max message size in bytes the server can receive and send</p> <code>MAX_MESSAGE_SIZE</code> <code>max_threads</code> <p>The max number of threads to be spawned;             defaults to 4 and max capped at 16</p> <code>NUM_THREADS_DEFAULT</code> <p>Example invocation: <pre><code>import os\nimport logging\nfrom collections.abc import Iterator\n\nfrom pynumaflow.sinker import Datum, Responses, Response, SinkServer\nfrom pynumaflow.sinker import Sinker\nfrom pynumaflow._constants import _LOGGER\n\nlogging.basicConfig(level=logging.INFO)\n\nclass UserDefinedSink(Sinker):\n    def handler(self, datums: Iterator[Datum]) -&gt; Responses:\n        responses = Responses()\n        for msg in datums:\n            logging.info(\"User Defined Sink %s\", msg.value.decode(\"utf-8\"))\n            responses.append(Response.as_success(msg.id))\n        return responses\n\ndef udsink_handler(datums: Iterator[Datum]) -&gt; Responses:\n    responses = Responses()\n    for msg in datums:\n        logging.info(\"User Defined Sink %s\", msg.value.decode(\"utf-8\"))\n        responses.append(Response.as_success(msg.id))\n    return responses\n\nif __name__ == \"__main__\":\n    invoke = os.getenv(\"INVOKE\", \"func_handler\")\n    if invoke == \"class\":\n        sink_handler = UserDefinedSink()\n    else:\n        sink_handler = udsink_handler\n    grpc_server = SinkServer(sink_handler)\n    grpc_server.start()\n</code></pre></p> Source code in <code>pynumaflow/sinker/server.py</code> <pre><code>def __init__(\n    self,\n    sinker_instance: SinkSyncCallable,\n    sock_path=SINK_SOCK_PATH,\n    max_message_size=MAX_MESSAGE_SIZE,\n    max_threads=NUM_THREADS_DEFAULT,\n    server_info_file=SINK_SERVER_INFO_FILE_PATH,\n):\n    \"\"\"\n    Create a new grpc Sink Server instance.\n    A new servicer instance is created and attached to the server.\n    The server instance is returned.\n\n    Args:\n        sinker_instance: The sinker instance to be used for Sink UDF\n        sock_path: The UNIX socket path to be used for the server\n        max_message_size: The max message size in bytes the server can receive and send\n        max_threads: The max number of threads to be spawned;\n                        defaults to 4 and max capped at 16\n    Example invocation:\n    ```py\n    import os\n    import logging\n    from collections.abc import Iterator\n\n    from pynumaflow.sinker import Datum, Responses, Response, SinkServer\n    from pynumaflow.sinker import Sinker\n    from pynumaflow._constants import _LOGGER\n\n    logging.basicConfig(level=logging.INFO)\n\n    class UserDefinedSink(Sinker):\n        def handler(self, datums: Iterator[Datum]) -&gt; Responses:\n            responses = Responses()\n            for msg in datums:\n                logging.info(\"User Defined Sink %s\", msg.value.decode(\"utf-8\"))\n                responses.append(Response.as_success(msg.id))\n            return responses\n\n    def udsink_handler(datums: Iterator[Datum]) -&gt; Responses:\n        responses = Responses()\n        for msg in datums:\n            logging.info(\"User Defined Sink %s\", msg.value.decode(\"utf-8\"))\n            responses.append(Response.as_success(msg.id))\n        return responses\n\n    if __name__ == \"__main__\":\n        invoke = os.getenv(\"INVOKE\", \"func_handler\")\n        if invoke == \"class\":\n            sink_handler = UserDefinedSink()\n        else:\n            sink_handler = udsink_handler\n        grpc_server = SinkServer(sink_handler)\n        grpc_server.start()\n    ```\n    \"\"\"\n    # If the container type is fallback sink, then use the fallback sink address and path.\n    if os.getenv(ENV_UD_CONTAINER_TYPE, \"\") == UD_CONTAINER_FALLBACK_SINK:\n        _LOGGER.info(\"Using Fallback Sink\")\n        sock_path = FALLBACK_SINK_SOCK_PATH\n        server_info_file = FALLBACK_SINK_SERVER_INFO_FILE_PATH\n    elif os.getenv(ENV_UD_CONTAINER_TYPE, \"\") == UD_CONTAINER_ON_SUCCESS_SINK:\n        _LOGGER.info(\"Using On Success Sink\")\n        sock_path = ON_SUCCESS_SINK_SOCK_PATH\n        server_info_file = ON_SUCCESS_SINK_SERVER_INFO_FILE_PATH\n\n    self.sock_path = f\"unix://{sock_path}\"\n    self.max_threads = min(max_threads, MAX_NUM_THREADS)\n    self.max_message_size = max_message_size\n    self.server_info_file = server_info_file\n\n    self.sinker_instance = sinker_instance\n\n    self._server_options = [\n        (\"grpc.max_send_message_length\", self.max_message_size),\n        (\"grpc.max_receive_message_length\", self.max_message_size),\n    ]\n    self.servicer = SyncSinkServicer(sinker_instance)\n</code></pre>"},{"location":"api/sinker/#pynumaflow.sinker.SinkServer.start","title":"start","text":"<pre><code>start()\n</code></pre> <p>Starts the Synchronous gRPC server on the given UNIX socket with given max threads.</p> Source code in <code>pynumaflow/sinker/server.py</code> <pre><code>def start(self):\n    \"\"\"\n    Starts the Synchronous gRPC server on the\n    given UNIX socket with given max threads.\n    \"\"\"\n    _LOGGER.info(\n        \"Sync GRPC Sink listening on: %s with max threads: %s\",\n        self.sock_path,\n        self.max_threads,\n    )\n    serv_info = ServerInfo.get_default_server_info()\n    serv_info.minimum_numaflow_version = MINIMUM_NUMAFLOW_VERSION[ContainerType.Sinker]\n    # Start the server\n    sync_server_start(\n        servicer=self.servicer,\n        bind_address=self.sock_path,\n        max_threads=self.max_threads,\n        server_info_file=self.server_info_file,\n        server_options=self._server_options,\n        udf_type=UDFType.Sink,\n        server_info=serv_info,\n    )\n</code></pre>"},{"location":"api/sinker/#pynumaflow.sinker.UserMetadata","title":"UserMetadata  <code>dataclass</code>","text":"<pre><code>UserMetadata(_data: dict[str, dict[str, bytes]] = dict())\n</code></pre> <p>UserMetadata wraps the user-generated metadata groups per message. It is read-write to UDFs.</p>"},{"location":"api/sinker/#pynumaflow.sinker.UserMetadata.groups","title":"groups","text":"<pre><code>groups() -&gt; list[str]\n</code></pre> <p>Returns the list of group names for the user metadata.</p> Source code in <code>pynumaflow/_metadata.py</code> <pre><code>def groups(self) -&gt; list[str]:\n    \"\"\"\n    Returns the list of group names for the user metadata.\n    \"\"\"\n    return list(self._data.keys())\n</code></pre>"},{"location":"api/sinker/#pynumaflow.sinker.UserMetadata.keys","title":"keys","text":"<pre><code>keys(group: str) -&gt; list[str]\n</code></pre> <p>Returns the list of keys for a given group.</p> Source code in <code>pynumaflow/_metadata.py</code> <pre><code>def keys(self, group: str) -&gt; list[str]:\n    \"\"\"\n    Returns the list of keys for a given group.\n    \"\"\"\n    keys = self._data.get(group) or {}\n    return list(keys.keys())\n</code></pre>"},{"location":"api/sinker/#pynumaflow.sinker.UserMetadata.value","title":"value","text":"<pre><code>value(group: str, key: str) -&gt; Optional[bytes]\n</code></pre> <p>Returns the value for a given group and key. If the group or key does not exist, returns None.</p> Source code in <code>pynumaflow/_metadata.py</code> <pre><code>def value(self, group: str, key: str) -&gt; Optional[bytes]:\n    \"\"\"\n    Returns the value for a given group and key.\n    If the group or key does not exist, returns None.\n    \"\"\"\n    value = self._data.get(group)\n    if value is None:\n        return None\n    return value.get(key)\n</code></pre>"},{"location":"api/sinker/#pynumaflow.sinker.UserMetadata.add_key","title":"add_key","text":"<pre><code>add_key(group: str, key: str, value: bytes)\n</code></pre> <p>Adds the value for a given group and key.</p> Source code in <code>pynumaflow/_metadata.py</code> <pre><code>def add_key(self, group: str, key: str, value: bytes):\n    \"\"\"\n    Adds the value for a given group and key.\n    \"\"\"\n    self._data.setdefault(group, {})[key] = value\n</code></pre>"},{"location":"api/sinker/#pynumaflow.sinker.UserMetadata.remove_key","title":"remove_key","text":"<pre><code>remove_key(group: str, key: str) -&gt; Optional[bytes]\n</code></pre> <p>Removes the key and its value for a given group and returns the value. If this key is the only key in the group, the group will be removed. Returns None if the group or key does not exist.</p> Source code in <code>pynumaflow/_metadata.py</code> <pre><code>def remove_key(self, group: str, key: str) -&gt; Optional[bytes]:\n    \"\"\"\n    Removes the key and its value for a given group and returns the value.\n    If this key is the only key in the group, the group will be removed.\n    Returns None if the group or key does not exist.\n    \"\"\"\n    group_data = self._data.pop(group, None)\n    if group_data is None:\n        return None\n    value = group_data.pop(key, None)\n    if group_data:\n        self._data[group] = group_data\n    return value\n</code></pre>"},{"location":"api/sinker/#pynumaflow.sinker.UserMetadata.remove_group","title":"remove_group","text":"<pre><code>remove_group(group: str) -&gt; Optional[dict[str, bytes]]\n</code></pre> <p>Removes the group and all its keys and values and returns the data. Returns None if the group does not exist.</p> Source code in <code>pynumaflow/_metadata.py</code> <pre><code>def remove_group(self, group: str) -&gt; Optional[dict[str, bytes]]:\n    \"\"\"\n    Removes the group and all its keys and values and returns the data.\n    Returns None if the group does not exist.\n    \"\"\"\n    return self._data.pop(group, None)\n</code></pre>"},{"location":"api/sinker/#pynumaflow.sinker.UserMetadata.clear","title":"clear","text":"<pre><code>clear()\n</code></pre> <p>Clears all the groups and all their keys and values.</p> Source code in <code>pynumaflow/_metadata.py</code> <pre><code>def clear(self):\n    \"\"\"\n    Clears all the groups and all their keys and values.\n    \"\"\"\n    self._data.clear()\n</code></pre>"},{"location":"api/sinker/#pynumaflow.sinker.SystemMetadata","title":"SystemMetadata  <code>dataclass</code>","text":"<pre><code>SystemMetadata(_data: dict[str, dict[str, bytes]] = dict())\n</code></pre> <p>System metadata is the mapping of group name to key-value pairs for a given group. System metadata wraps the system-generated metadata groups per message. It is read-only to UDFs.</p>"},{"location":"api/sinker/#pynumaflow.sinker.SystemMetadata.groups","title":"groups","text":"<pre><code>groups() -&gt; list[str]\n</code></pre> <p>Returns the list of group names for the system metadata.</p> Source code in <code>pynumaflow/_metadata.py</code> <pre><code>def groups(self) -&gt; list[str]:\n    \"\"\"\n    Returns the list of group names for the system metadata.\n    \"\"\"\n    return list(self._data.keys())\n</code></pre>"},{"location":"api/sinker/#pynumaflow.sinker.SystemMetadata.keys","title":"keys","text":"<pre><code>keys(group: str) -&gt; list[str]\n</code></pre> <p>Returns the list of keys for a given group.</p> Source code in <code>pynumaflow/_metadata.py</code> <pre><code>def keys(self, group: str) -&gt; list[str]:\n    \"\"\"\n    Returns the list of keys for a given group.\n    \"\"\"\n    return list(self._data.get(group, {}).keys())\n</code></pre>"},{"location":"api/sinker/#pynumaflow.sinker.SystemMetadata.value","title":"value","text":"<pre><code>value(group: str, key: str) -&gt; Optional[bytes]\n</code></pre> <p>Returns the value for a given group and key.</p> Source code in <code>pynumaflow/_metadata.py</code> <pre><code>def value(self, group: str, key: str) -&gt; Optional[bytes]:\n    \"\"\"\n    Returns the value for a given group and key.\n    \"\"\"\n    return self._data.get(group, {}).get(key)\n</code></pre>"},{"location":"api/sinker/#pynumaflow.sinker.Response","title":"Response  <code>dataclass</code>","text":"<pre><code>Response(\n    id: str,\n    success: bool,\n    err: Optional[str],\n    fallback: bool,\n    on_success: bool,\n    on_success_msg: Optional[Message],\n)\n</code></pre> <p>Basic datatype for UDSink response.</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>str</code> <p>the id of the event.</p> required <code>success</code> <code>bool</code> <p>boolean indicating whether the event was successfully processed.</p> required <code>err</code> <code>Optional[str]</code> <p>error message if the event was not successfully processed.</p> required <code>fallback</code> <code>bool</code> <p>fallback is true if the message to be sent to the fallback sink.</p> required"},{"location":"api/sinker/#pynumaflow.sinker.Responses","title":"Responses","text":"<pre><code>Responses(*responses: R)\n</code></pre> <p>               Bases: <code>Sequence[R]</code></p> <p>Container to hold a list of Response instances.</p> <p>Parameters:</p> Name Type Description Default <code>responses</code> <code>R</code> <p>list of Response instances.</p> <code>()</code> Source code in <code>pynumaflow/sinker/_dtypes.py</code> <pre><code>def __init__(self, *responses: R):\n    self._responses = list(responses) or []\n</code></pre>"},{"location":"api/sinker/#pynumaflow.sinker.Datum","title":"Datum  <code>dataclass</code>","text":"<pre><code>Datum(\n    keys: list[str],\n    sink_msg_id: str,\n    value: bytes,\n    event_time: datetime,\n    watermark: datetime,\n    headers: Optional[dict[str, str]] = None,\n    user_metadata: Optional[UserMetadata] = None,\n    system_metadata: Optional[SystemMetadata] = None,\n)\n</code></pre> <p>Class to define the important information for the event.</p> <p>Parameters:</p> Name Type Description Default <code>keys</code> <code>list[str]</code> <p>the keys of the event.</p> required <code>value</code> <code>bytes</code> <p>the payload of the event.</p> required <code>event_time</code> <code>datetime</code> <p>the event time of the event.</p> required <code>watermark</code> <code>datetime</code> <p>the watermark of the event.</p> required <code>headers</code> <code>Optional[dict[str, str]]</code> <p>the headers of the event.</p> <code>None</code> <p>Example usage <pre><code>from pynumaflow.sinker import Datum\nfrom datetime import datetime, timezone\n\nd = Datum(\n      keys=[\"test_key\"],\n      sink_msg_id=\"test_id\",\n      value=b\"test_mock_message\",\n      event_time=datetime.fromtimestamp(1662998400, timezone.utc),\n      watermark=datetime.fromtimestamp(1662998460, timezone.utc),\n      headers={\"key1\": \"value1\", \"key2\": \"value2\"},\n   )\n</code></pre></p> Source code in <code>pynumaflow/sinker/_dtypes.py</code> <pre><code>def __init__(\n    self,\n    keys: list[str],\n    sink_msg_id: str,\n    value: bytes,\n    event_time: datetime,\n    watermark: datetime,\n    headers: Optional[dict[str, str]] = None,\n    user_metadata: Optional[UserMetadata] = None,\n    system_metadata: Optional[SystemMetadata] = None,\n):\n    self._keys = keys\n    self._id = sink_msg_id or \"\"\n    self._value = value or b\"\"\n    if not isinstance(event_time, datetime):\n        raise TypeError(f\"Wrong data type: {type(event_time)} for Datum.event_time\")\n    self._event_time = event_time\n    if not isinstance(watermark, datetime):\n        raise TypeError(f\"Wrong data type: {type(watermark)} for Datum.watermark\")\n    self._watermark = watermark\n    self._headers = headers or {}\n    self._user_metadata = user_metadata or UserMetadata()\n    self._system_metadata = system_metadata or SystemMetadata()\n</code></pre>"},{"location":"api/sinker/#pynumaflow.sinker.Datum.id","title":"id  <code>property</code>","text":"<pre><code>id: str\n</code></pre> <p>Returns the id of the event.</p>"},{"location":"api/sinker/#pynumaflow.sinker.Datum.keys","title":"keys  <code>property</code>","text":"<pre><code>keys: list[str]\n</code></pre> <p>Returns the keys of the event.</p>"},{"location":"api/sinker/#pynumaflow.sinker.Datum.value","title":"value  <code>property</code>","text":"<pre><code>value: bytes\n</code></pre> <p>Returns the value of the event.</p>"},{"location":"api/sinker/#pynumaflow.sinker.Datum.event_time","title":"event_time  <code>property</code>","text":"<pre><code>event_time: datetime\n</code></pre> <p>Returns the event time of the event.</p>"},{"location":"api/sinker/#pynumaflow.sinker.Datum.watermark","title":"watermark  <code>property</code>","text":"<pre><code>watermark: datetime\n</code></pre> <p>Returns the watermark of the event.</p>"},{"location":"api/sinker/#pynumaflow.sinker.Datum.headers","title":"headers  <code>property</code>","text":"<pre><code>headers: dict[str, str]\n</code></pre> <p>Returns the headers of the event.</p>"},{"location":"api/sinker/#pynumaflow.sinker.Datum.user_metadata","title":"user_metadata  <code>property</code>","text":"<pre><code>user_metadata: UserMetadata\n</code></pre> <p>Returns the user metadata of the event.</p>"},{"location":"api/sinker/#pynumaflow.sinker.Datum.system_metadata","title":"system_metadata  <code>property</code>","text":"<pre><code>system_metadata: SystemMetadata\n</code></pre> <p>Returns the system metadata of the event.</p>"},{"location":"api/sinker/#pynumaflow.sinker.Sinker","title":"Sinker","text":"<p>Provides an interface to write a Sinker which will be exposed over a gRPC server.</p>"},{"location":"api/sinker/#pynumaflow.sinker.Sinker.handler","title":"handler  <code>abstractmethod</code>","text":"<pre><code>handler(datums: Iterator[Datum]) -&gt; Responses\n</code></pre> <p>Implement this handler function which implements the SinkCallable interface.</p> Source code in <code>pynumaflow/sinker/_dtypes.py</code> <pre><code>@abstractmethod\ndef handler(self, datums: Iterator[Datum]) -&gt; Responses:\n    \"\"\"\n    Implement this handler function which implements the SinkCallable interface.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/sinker/#pynumaflow.sinker.Message","title":"Message  <code>dataclass</code>","text":"<pre><code>Message(\n    value: bytes,\n    keys: Optional[list[str]] = None,\n    user_metadata: Optional[UserMetadata] = None,\n)\n</code></pre> <p>Basic datatype for OnSuccess UDSink message.</p> <p>Parameters:</p> Name Type Description Default <code>keys</code> <code>Optional[list[str]]</code> <p>list of keys for the on_success message.</p> <code>None</code> <code>value</code> <code>bytes</code> <p>payload of the on_success message.</p> required <code>user_metadata</code> <code>Optional[UserMetadata]</code> <p>user metadata of the on_success message.</p> <code>None</code> Source code in <code>pynumaflow/sinker/_dtypes.py</code> <pre><code>def __init__(\n    self,\n    value: bytes,\n    keys: Optional[list[str]] = None,\n    user_metadata: Optional[UserMetadata] = None,\n):\n    self._value = value\n    self._keys = keys\n    self._user_metadata = user_metadata\n</code></pre>"},{"location":"api/sinker/#pynumaflow.sinker.Message.keys","title":"keys  <code>property</code>","text":"<pre><code>keys: Optional[list[str]]\n</code></pre> <p>Returns the id of the event.</p>"},{"location":"api/sinker/#pynumaflow.sinker.Message.value","title":"value  <code>property</code>","text":"<pre><code>value: bytes\n</code></pre> <p>Returns the id of the event.</p>"},{"location":"api/sinker/#pynumaflow.sinker.Message.user_metadata","title":"user_metadata  <code>property</code>","text":"<pre><code>user_metadata: Optional[UserMetadata]\n</code></pre> <p>Returns the id of the event.</p>"},{"location":"api/sourcer/","title":"Sourcer","text":"<p>The Sourcer module provides classes and functions for implementing User Defined Sources that produce messages for Numaflow pipelines.</p>"},{"location":"api/sourcer/#classes","title":"Classes","text":""},{"location":"api/sourcer/#pynumaflow.sourcer.Message","title":"Message  <code>dataclass</code>","text":"<pre><code>Message(\n    payload: bytes,\n    offset: Offset,\n    event_time: datetime,\n    keys: Optional[list[str]] = None,\n    headers: Optional[dict[str, str]] = None,\n    user_metadata: Optional[UserMetadata] = None,\n)\n</code></pre> <p>Basic datatype for data passing to the next vertex/vertices.</p> <p>Parameters:</p> Name Type Description Default <code>payload</code> <code>bytes</code> <p>data in bytes</p> required <code>offset</code> <code>Offset</code> <p>the offset of the datum.</p> required <code>event_time</code> <code>datetime</code> <p>event time of the message, usually extracted from the payload.</p> required <code>keys</code> <code>Optional[list[str]]</code> <p>list of string keys for the vertex (optional)</p> <code>None</code> <code>headers</code> <code>Optional[dict[str, str]]</code> <p>dict of headers for the message (optional)</p> <code>None</code> <code>user_metadata</code> <code>Optional[UserMetadata]</code> <p>metadata for the message (optional)</p> <code>None</code> Source code in <code>pynumaflow/sourcer/_dtypes.py</code> <pre><code>def __init__(\n    self,\n    payload: bytes,\n    offset: Offset,\n    event_time: datetime,\n    keys: Optional[list[str]] = None,\n    headers: Optional[dict[str, str]] = None,\n    user_metadata: Optional[UserMetadata] = None,\n):\n    \"\"\"\n    Creates a Message object to send value to a vertex.\n    \"\"\"\n    self._payload = payload\n    self._offset = offset\n    self._event_time = event_time\n    self._keys = keys or []\n    self._headers = headers or {}\n    self._user_metadata = user_metadata or UserMetadata()\n</code></pre>"},{"location":"api/sourcer/#pynumaflow.sourcer.Message.user_metadata","title":"user_metadata  <code>property</code>","text":"<pre><code>user_metadata: UserMetadata\n</code></pre> <p>Returns the user metadata of the message.</p>"},{"location":"api/sourcer/#pynumaflow.sourcer.ReadRequest","title":"ReadRequest  <code>dataclass</code>","text":"<pre><code>ReadRequest(num_records: int, timeout_in_ms: int)\n</code></pre> <p>Class to define the request for reading datum stream from user defined source.</p> <p>Parameters:</p> Name Type Description Default <code>num_records</code> <code>int</code> <p>the number of records to read.</p> required <code>timeout_in_ms</code> <code>int</code> <p>the request timeout in milliseconds.</p> required <p>Example: <pre><code>from pynumaflow.sourcer import ReadRequest\nread_request = ReadRequest(num_records=10, timeout_in_ms=1000)\n</code></pre></p> Source code in <code>pynumaflow/sourcer/_dtypes.py</code> <pre><code>def __init__(\n    self,\n    num_records: int,\n    timeout_in_ms: int,\n):\n    if not isinstance(num_records, int):\n        raise TypeError(f\"Wrong data type: {type(num_records)} for ReadRequest.num_records\")\n    self._num_records = num_records\n    if not isinstance(timeout_in_ms, int):\n        raise TypeError(f\"Wrong data type: {type(timeout_in_ms)} for ReadRequest.timeout_in_ms\")\n    self._timeout_in_ms = timeout_in_ms\n</code></pre>"},{"location":"api/sourcer/#pynumaflow.sourcer.ReadRequest.num_records","title":"num_records  <code>property</code>","text":"<pre><code>num_records: int\n</code></pre> <p>Returns the num_records of the request</p>"},{"location":"api/sourcer/#pynumaflow.sourcer.ReadRequest.timeout_in_ms","title":"timeout_in_ms  <code>property</code>","text":"<pre><code>timeout_in_ms: int\n</code></pre> <p>Returns the timeout_in_ms of the request.</p>"},{"location":"api/sourcer/#pynumaflow.sourcer.PendingResponse","title":"PendingResponse  <code>dataclass</code>","text":"<pre><code>PendingResponse(count: int)\n</code></pre> <p>PendingResponse is the response for the pending request. It indicates the number of pending records at the user defined source. A negative count indicates that the pending information is not available.</p> <p>Parameters:</p> Name Type Description Default <code>count</code> <code>int</code> <p>the number of pending records.</p> required Source code in <code>pynumaflow/sourcer/_dtypes.py</code> <pre><code>def __init__(self, count: int):\n    if not isinstance(count, int):\n        raise TypeError(f\"Wrong data type: {type(count)} for Pending.count\")\n    self._count = count\n</code></pre>"},{"location":"api/sourcer/#pynumaflow.sourcer.PendingResponse.count","title":"count  <code>property</code>","text":"<pre><code>count: int\n</code></pre> <p>Returns the count of pending records</p>"},{"location":"api/sourcer/#pynumaflow.sourcer.AckRequest","title":"AckRequest  <code>dataclass</code>","text":"<pre><code>AckRequest(offsets: list[Offset])\n</code></pre> <p>Class for defining the request for acknowledging datum. It takes a list of offsets that need to be acknowledged.</p> <p>Parameters:</p> Name Type Description Default <code>offsets</code> <code>list[Offset]</code> <p>the offsets to be acknowledged.</p> required <p>Example: <pre><code>from pynumaflow.sourcer import AckRequest, Offset\n\noffset_val = Offset(offset=b\"123\", partition_id=0)\nack_request = AckRequest(offsets=[offset_val, offset_val])\n</code></pre></p> Source code in <code>pynumaflow/sourcer/_dtypes.py</code> <pre><code>def __init__(self, offsets: list[Offset]):\n    self._offsets = offsets\n</code></pre>"},{"location":"api/sourcer/#pynumaflow.sourcer.AckRequest.offsets","title":"offsets  <code>property</code>","text":"<pre><code>offsets: list[Offset]\n</code></pre> <p>Returns the offsets to be acknowledged.</p>"},{"location":"api/sourcer/#pynumaflow.sourcer.NackRequest","title":"NackRequest  <code>dataclass</code>","text":"<pre><code>NackRequest(offsets: list[Offset])\n</code></pre> <p>Class for defining the request for negatively acknowledging an offset. It takes a list of offsets that need to be negatively acknowledged on the source.</p> <p>Parameters:</p> Name Type Description Default <code>offsets</code> <code>list[Offset]</code> <p>the offsets to be negatively acknowledged.</p> required <p>Example: <pre><code>from pynumaflow.sourcer import NackRequest, Offset\noffset_val = Offset(offset=b\"123\", partition_id=0)\nnack_request = NackRequest(offsets=[offset_val, offset_val])\n</code></pre></p> Source code in <code>pynumaflow/sourcer/_dtypes.py</code> <pre><code>def __init__(self, offsets: list[Offset]):\n    self._offsets = offsets\n</code></pre>"},{"location":"api/sourcer/#pynumaflow.sourcer.NackRequest.offsets","title":"offsets  <code>property</code>","text":"<pre><code>offsets: list[Offset]\n</code></pre> <p>Returns the offsets to be negatively acknowledged.</p>"},{"location":"api/sourcer/#pynumaflow.sourcer.Offset","title":"Offset  <code>dataclass</code>","text":"<pre><code>Offset(offset: bytes, partition_id: int)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>offset</code> <code>bytes</code> <p>the offset of the datum.</p> required <code>partition_id</code> <code>int</code> <p>partition_id indicates which partition of the source the datum belongs to.</p> required Source code in <code>pynumaflow/sourcer/_dtypes.py</code> <pre><code>def __init__(self, offset: bytes, partition_id: int):\n    self._offset = offset\n    self._partition_id = partition_id\n</code></pre>"},{"location":"api/sourcer/#pynumaflow.sourcer.Offset.offset_with_default_partition_id","title":"offset_with_default_partition_id  <code>classmethod</code>","text":"<pre><code>offset_with_default_partition_id(offset: bytes)\n</code></pre> <p>Returns an Offset object with the given offset and default partition id.</p> Source code in <code>pynumaflow/sourcer/_dtypes.py</code> <pre><code>@classmethod\ndef offset_with_default_partition_id(cls, offset: bytes):\n    \"\"\"\n    Returns an Offset object with the given offset and default partition id.\n    \"\"\"\n    return Offset(offset=offset, partition_id=get_default_partitions()[0])\n</code></pre>"},{"location":"api/sourcer/#pynumaflow.sourcer.PartitionsResponse","title":"PartitionsResponse  <code>dataclass</code>","text":"<pre><code>PartitionsResponse(partitions: list[int])\n</code></pre> <p>PartitionsResponse is the response for the partition request. It indicates the number of partitions at the user defined source. A negative count indicates that the partition information is not available.</p> <p>Parameters:</p> Name Type Description Default <code>count</code> <p>the number of partitions.</p> required Source code in <code>pynumaflow/sourcer/_dtypes.py</code> <pre><code>def __init__(self, partitions: list[int]):\n    if not isinstance(partitions, list):\n        raise TypeError(f\"Wrong data type: {type(partitions)} for Partition.partitions\")\n    self._partitions = partitions\n</code></pre>"},{"location":"api/sourcer/#pynumaflow.sourcer.PartitionsResponse.partitions","title":"partitions  <code>property</code>","text":"<pre><code>partitions: list[int]\n</code></pre> <p>Returns the list of partitions</p>"},{"location":"api/sourcer/#pynumaflow.sourcer.Sourcer","title":"Sourcer","text":"<p>Provides an interface to write a Sourcer which will be exposed over an gRPC server.</p>"},{"location":"api/sourcer/#pynumaflow.sourcer.Sourcer.read_handler","title":"read_handler  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>read_handler(\n    datum: ReadRequest, output: NonBlockingIterator\n)\n</code></pre> <p>Implement this handler function which implements the SourceReadCallable interface. read_handler is used to read the data from the source and send the data forward for each read request we process num_records and increment the read_idx to indicate that the message has been read and the same is added to the ack set</p> Source code in <code>pynumaflow/sourcer/_dtypes.py</code> <pre><code>@abstractmethod\nasync def read_handler(self, datum: ReadRequest, output: NonBlockingIterator):\n    \"\"\"\n    Implement this handler function which implements the SourceReadCallable interface.\n    read_handler is used to read the data from the source and send the data forward\n    for each read request we process num_records and increment the read_idx to indicate that\n    the message has been read and the same is added to the ack set\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/sourcer/#pynumaflow.sourcer.Sourcer.ack_handler","title":"ack_handler  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>ack_handler(ack_request: AckRequest)\n</code></pre> <p>The ack handler is used to acknowledge the offsets that have been read</p> Source code in <code>pynumaflow/sourcer/_dtypes.py</code> <pre><code>@abstractmethod\nasync def ack_handler(self, ack_request: AckRequest):\n    \"\"\"\n    The ack handler is used to acknowledge the offsets that have been read\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/sourcer/#pynumaflow.sourcer.Sourcer.nack_handler","title":"nack_handler  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>nack_handler(nack_request: NackRequest)\n</code></pre> <p>The nack handler is used to negatively acknowledge the offsets on the source</p> Source code in <code>pynumaflow/sourcer/_dtypes.py</code> <pre><code>@abstractmethod\nasync def nack_handler(self, nack_request: NackRequest):\n    \"\"\"\n    The nack handler is used to negatively acknowledge the offsets on the source\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/sourcer/#pynumaflow.sourcer.Sourcer.pending_handler","title":"pending_handler  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>pending_handler() -&gt; PendingResponse\n</code></pre> <p>The simple source always returns zero to indicate there is no pending record.</p> Source code in <code>pynumaflow/sourcer/_dtypes.py</code> <pre><code>@abstractmethod\nasync def pending_handler(self) -&gt; PendingResponse:\n    \"\"\"\n    The simple source always returns zero to indicate there is no pending record.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/sourcer/#pynumaflow.sourcer.Sourcer.partitions_handler","title":"partitions_handler  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>partitions_handler() -&gt; PartitionsResponse\n</code></pre> <p>The simple source always returns zero to indicate there is no pending record.</p> Source code in <code>pynumaflow/sourcer/_dtypes.py</code> <pre><code>@abstractmethod\nasync def partitions_handler(self) -&gt; PartitionsResponse:\n    \"\"\"\n    The simple source always returns zero to indicate there is no pending record.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/sourcer/#pynumaflow.sourcer.UserMetadata","title":"UserMetadata  <code>dataclass</code>","text":"<pre><code>UserMetadata(_data: dict[str, dict[str, bytes]] = dict())\n</code></pre> <p>UserMetadata wraps the user-generated metadata groups per message. It is read-write to UDFs.</p>"},{"location":"api/sourcer/#pynumaflow.sourcer.UserMetadata.groups","title":"groups","text":"<pre><code>groups() -&gt; list[str]\n</code></pre> <p>Returns the list of group names for the user metadata.</p> Source code in <code>pynumaflow/_metadata.py</code> <pre><code>def groups(self) -&gt; list[str]:\n    \"\"\"\n    Returns the list of group names for the user metadata.\n    \"\"\"\n    return list(self._data.keys())\n</code></pre>"},{"location":"api/sourcer/#pynumaflow.sourcer.UserMetadata.keys","title":"keys","text":"<pre><code>keys(group: str) -&gt; list[str]\n</code></pre> <p>Returns the list of keys for a given group.</p> Source code in <code>pynumaflow/_metadata.py</code> <pre><code>def keys(self, group: str) -&gt; list[str]:\n    \"\"\"\n    Returns the list of keys for a given group.\n    \"\"\"\n    keys = self._data.get(group) or {}\n    return list(keys.keys())\n</code></pre>"},{"location":"api/sourcer/#pynumaflow.sourcer.UserMetadata.value","title":"value","text":"<pre><code>value(group: str, key: str) -&gt; Optional[bytes]\n</code></pre> <p>Returns the value for a given group and key. If the group or key does not exist, returns None.</p> Source code in <code>pynumaflow/_metadata.py</code> <pre><code>def value(self, group: str, key: str) -&gt; Optional[bytes]:\n    \"\"\"\n    Returns the value for a given group and key.\n    If the group or key does not exist, returns None.\n    \"\"\"\n    value = self._data.get(group)\n    if value is None:\n        return None\n    return value.get(key)\n</code></pre>"},{"location":"api/sourcer/#pynumaflow.sourcer.UserMetadata.add_key","title":"add_key","text":"<pre><code>add_key(group: str, key: str, value: bytes)\n</code></pre> <p>Adds the value for a given group and key.</p> Source code in <code>pynumaflow/_metadata.py</code> <pre><code>def add_key(self, group: str, key: str, value: bytes):\n    \"\"\"\n    Adds the value for a given group and key.\n    \"\"\"\n    self._data.setdefault(group, {})[key] = value\n</code></pre>"},{"location":"api/sourcer/#pynumaflow.sourcer.UserMetadata.remove_key","title":"remove_key","text":"<pre><code>remove_key(group: str, key: str) -&gt; Optional[bytes]\n</code></pre> <p>Removes the key and its value for a given group and returns the value. If this key is the only key in the group, the group will be removed. Returns None if the group or key does not exist.</p> Source code in <code>pynumaflow/_metadata.py</code> <pre><code>def remove_key(self, group: str, key: str) -&gt; Optional[bytes]:\n    \"\"\"\n    Removes the key and its value for a given group and returns the value.\n    If this key is the only key in the group, the group will be removed.\n    Returns None if the group or key does not exist.\n    \"\"\"\n    group_data = self._data.pop(group, None)\n    if group_data is None:\n        return None\n    value = group_data.pop(key, None)\n    if group_data:\n        self._data[group] = group_data\n    return value\n</code></pre>"},{"location":"api/sourcer/#pynumaflow.sourcer.UserMetadata.remove_group","title":"remove_group","text":"<pre><code>remove_group(group: str) -&gt; Optional[dict[str, bytes]]\n</code></pre> <p>Removes the group and all its keys and values and returns the data. Returns None if the group does not exist.</p> Source code in <code>pynumaflow/_metadata.py</code> <pre><code>def remove_group(self, group: str) -&gt; Optional[dict[str, bytes]]:\n    \"\"\"\n    Removes the group and all its keys and values and returns the data.\n    Returns None if the group does not exist.\n    \"\"\"\n    return self._data.pop(group, None)\n</code></pre>"},{"location":"api/sourcer/#pynumaflow.sourcer.UserMetadata.clear","title":"clear","text":"<pre><code>clear()\n</code></pre> <p>Clears all the groups and all their keys and values.</p> Source code in <code>pynumaflow/_metadata.py</code> <pre><code>def clear(self):\n    \"\"\"\n    Clears all the groups and all their keys and values.\n    \"\"\"\n    self._data.clear()\n</code></pre>"},{"location":"api/sourcer/#pynumaflow.sourcer.SourceAsyncServer","title":"SourceAsyncServer","text":"<pre><code>SourceAsyncServer(\n    sourcer_instance: SourceCallable,\n    sock_path=SOURCE_SOCK_PATH,\n    max_message_size=MAX_MESSAGE_SIZE,\n    max_threads=NUM_THREADS_DEFAULT,\n    server_info_file=SOURCE_SERVER_INFO_FILE_PATH,\n)\n</code></pre> <p>               Bases: <code>NumaflowServer</code></p> <p>Class for a new Async Source Server instance.</p> <p>The server instance is returned.</p> <p>Parameters:</p> Name Type Description Default <code>sourcer_instance</code> <code>SourceCallable</code> <p>The sourcer instance to be used for Source UDF</p> required <code>sock_path</code> <p>The UNIX socket path to be used for the server</p> <code>SOURCE_SOCK_PATH</code> <code>max_message_size</code> <p>The max message size in bytes the server can receive and send</p> <code>MAX_MESSAGE_SIZE</code> <code>max_threads</code> <p>The max number of threads to be spawned;             defaults to 4 and max capped at 16</p> <code>NUM_THREADS_DEFAULT</code> <p>Example invocation: <pre><code>from datetime import datetime\nfrom pynumaflow.shared.asynciter import NonBlockingIterator\nfrom pynumaflow.sourcer import (\n    ReadRequest,\n    Message,\n    AckRequest,\n    PendingResponse,\n    Offset,\n    PartitionsResponse,\n    get_default_partitions,\n    Sourcer,\n    SourceAsyncServer,\n    NackRequest,\n)\n\nclass AsyncSource(Sourcer):\n    # AsyncSource is a class for User Defined Source implementation.\n\n    def __init__(self):\n        # The offset idx till where the messages have been read\n        self.read_idx: int = 0\n        # Set to maintain a track of the offsets yet to be acknowledged\n        self.to_ack_set: set[int] = set()\n        # Set to maintain a track of the offsets that have been negatively acknowledged\n        self.nacked: set[int] = set()\n\n    async def read_handler(self, datum: ReadRequest, output: NonBlockingIterator):\n        '''\n        read_handler is used to read the data from the source and send the data forward\n        for each read request we process num_records and increment the read_idx to\n        indicate that the message has been read and the same is added to the ack set\n        '''\n        if self.to_ack_set:\n            return\n\n        for x in range(datum.num_records):\n            # If there are any nacked offsets, re-deliver them\n            if self.nacked:\n                idx = self.nacked.pop()\n            else:\n                idx = self.read_idx\n                self.read_idx += 1\n            headers = {\"x-txn-id\": str(uuid.uuid4())}\n            await output.put(\n                Message(\n                    payload=str(self.read_idx).encode(),\n                    offset=Offset.offset_with_default_partition_id(str(idx).encode()),\n                    event_time=datetime.now(),\n                    headers=headers,\n                )\n            )\n            self.to_ack_set.add(idx)\n\n    async def ack_handler(self, ack_request: AckRequest):\n        '''\n        The ack handler is used acknowledge the offsets that have been read, and remove\n        them from the to_ack_set\n        '''\n        for req in ack_request.offsets:\n            offset = int(req.offset)\n            self.to_ack_set.remove(offset)\n\n    async def nack_handler(self, ack_request: NackRequest):\n        '''\n        Add the offsets that have been negatively acknowledged to the nacked set\n        '''\n        for req in ack_request.offsets:\n            offset = int(req.offset)\n            self.to_ack_set.remove(offset)\n            self.nacked.add(offset)\n\n    async def pending_handler(self) -&gt; PendingResponse:\n        '''\n        The simple source always returns zero to indicate there is no pending record.\n        '''\n        return PendingResponse(count=0)\n\n    async def partitions_handler(self) -&gt; PartitionsResponse:\n        '''\n        The simple source always returns default partitions.\n        '''\n        return PartitionsResponse(partitions=get_default_partitions())\n\n\nif __name__ == \"__main__\":\n    ud_source = AsyncSource()\n    grpc_server = SourceAsyncServer(ud_source)\n    grpc_server.start()\n</code></pre></p> Source code in <code>pynumaflow/sourcer/async_server.py</code> <pre><code>def __init__(\n    self,\n    sourcer_instance: SourceCallable,\n    sock_path=SOURCE_SOCK_PATH,\n    max_message_size=MAX_MESSAGE_SIZE,\n    max_threads=NUM_THREADS_DEFAULT,\n    server_info_file=SOURCE_SERVER_INFO_FILE_PATH,\n):\n    \"\"\"\n    Create a new grpc Async Source Server instance.\n    A new servicer instance is created and attached to the server.\n    The server instance is returned.\n\n    Args:\n        sourcer_instance: The sourcer instance to be used for Source UDF\n        sock_path: The UNIX socket path to be used for the server\n        max_message_size: The max message size in bytes the server can receive and send\n        max_threads: The max number of threads to be spawned;\n                        defaults to 4 and max capped at 16\n\n    Example invocation:\n    ```py\n    from datetime import datetime\n    from pynumaflow.shared.asynciter import NonBlockingIterator\n    from pynumaflow.sourcer import (\n        ReadRequest,\n        Message,\n        AckRequest,\n        PendingResponse,\n        Offset,\n        PartitionsResponse,\n        get_default_partitions,\n        Sourcer,\n        SourceAsyncServer,\n        NackRequest,\n    )\n\n    class AsyncSource(Sourcer):\n        # AsyncSource is a class for User Defined Source implementation.\n\n        def __init__(self):\n            # The offset idx till where the messages have been read\n            self.read_idx: int = 0\n            # Set to maintain a track of the offsets yet to be acknowledged\n            self.to_ack_set: set[int] = set()\n            # Set to maintain a track of the offsets that have been negatively acknowledged\n            self.nacked: set[int] = set()\n\n        async def read_handler(self, datum: ReadRequest, output: NonBlockingIterator):\n            '''\n            read_handler is used to read the data from the source and send the data forward\n            for each read request we process num_records and increment the read_idx to\n            indicate that the message has been read and the same is added to the ack set\n            '''\n            if self.to_ack_set:\n                return\n\n            for x in range(datum.num_records):\n                # If there are any nacked offsets, re-deliver them\n                if self.nacked:\n                    idx = self.nacked.pop()\n                else:\n                    idx = self.read_idx\n                    self.read_idx += 1\n                headers = {\"x-txn-id\": str(uuid.uuid4())}\n                await output.put(\n                    Message(\n                        payload=str(self.read_idx).encode(),\n                        offset=Offset.offset_with_default_partition_id(str(idx).encode()),\n                        event_time=datetime.now(),\n                        headers=headers,\n                    )\n                )\n                self.to_ack_set.add(idx)\n\n        async def ack_handler(self, ack_request: AckRequest):\n            '''\n            The ack handler is used acknowledge the offsets that have been read, and remove\n            them from the to_ack_set\n            '''\n            for req in ack_request.offsets:\n                offset = int(req.offset)\n                self.to_ack_set.remove(offset)\n\n        async def nack_handler(self, ack_request: NackRequest):\n            '''\n            Add the offsets that have been negatively acknowledged to the nacked set\n            '''\n            for req in ack_request.offsets:\n                offset = int(req.offset)\n                self.to_ack_set.remove(offset)\n                self.nacked.add(offset)\n\n        async def pending_handler(self) -&gt; PendingResponse:\n            '''\n            The simple source always returns zero to indicate there is no pending record.\n            '''\n            return PendingResponse(count=0)\n\n        async def partitions_handler(self) -&gt; PartitionsResponse:\n            '''\n            The simple source always returns default partitions.\n            '''\n            return PartitionsResponse(partitions=get_default_partitions())\n\n\n    if __name__ == \"__main__\":\n        ud_source = AsyncSource()\n        grpc_server = SourceAsyncServer(ud_source)\n        grpc_server.start()\n    ```\n    \"\"\"\n    self.sock_path = f\"unix://{sock_path}\"\n    self.max_threads = min(max_threads, MAX_NUM_THREADS)\n    self.max_message_size = max_message_size\n    self.server_info_file = server_info_file\n\n    self.sourcer_instance = sourcer_instance\n\n    self._server_options = [\n        (\"grpc.max_send_message_length\", self.max_message_size),\n        (\"grpc.max_receive_message_length\", self.max_message_size),\n    ]\n\n    self.servicer = AsyncSourceServicer(source_handler=sourcer_instance)\n</code></pre>"},{"location":"api/sourcer/#pynumaflow.sourcer.SourceAsyncServer.start","title":"start","text":"<pre><code>start()\n</code></pre> <p>Starter function for the Async server class, need a separate caller so that all the async coroutines can be started from a single context</p> Source code in <code>pynumaflow/sourcer/async_server.py</code> <pre><code>def start(self):\n    \"\"\"\n    Starter function for the Async server class, need a separate caller\n    so that all the async coroutines can be started from a single context\n    \"\"\"\n    aiorun.run(self.aexec(), use_uvloop=True)\n</code></pre>"},{"location":"api/sourcer/#pynumaflow.sourcer.SourceAsyncServer.aexec","title":"aexec  <code>async</code>","text":"<pre><code>aexec()\n</code></pre> <p>Starts the Async gRPC server on the given UNIX socket with given max threads</p> Source code in <code>pynumaflow/sourcer/async_server.py</code> <pre><code>async def aexec(self):\n    \"\"\"\n    Starts the Async gRPC server on the given UNIX socket with given max threads\n    \"\"\"\n    # As the server is async, we need to create a new server instance in the\n    # same thread as the event loop so that all the async calls are made in the\n    # same context\n    # Create a new async server instance and add the servicer to it\n    server = grpc.aio.server(options=self._server_options)\n    server.add_insecure_port(self.sock_path)\n    source_servicer = self.servicer\n    source_pb2_grpc.add_SourceServicer_to_server(source_servicer, server)\n\n    serv_info = ServerInfo.get_default_server_info()\n    serv_info.minimum_numaflow_version = MINIMUM_NUMAFLOW_VERSION[ContainerType.Sourcer]\n    # Start the async server\n    await start_async_server(\n        server_async=server,\n        sock_path=self.sock_path,\n        max_threads=self.max_threads,\n        cleanup_coroutines=list(),\n        server_info_file=self.server_info_file,\n        server_info=serv_info,\n    )\n</code></pre>"},{"location":"api/sourcer/#pynumaflow.sourcer.get_default_partitions","title":"get_default_partitions","text":"<pre><code>get_default_partitions() -&gt; list[int]\n</code></pre> <p>Returns the default partition ids.</p> Source code in <code>pynumaflow/sourcer/_dtypes.py</code> <pre><code>def get_default_partitions() -&gt; list[int]:\n    \"\"\"\n    Returns the default partition ids.\n    \"\"\"\n    return [DefaultPartitionId]\n</code></pre>"},{"location":"api/sourcetransformer/","title":"Source Transformer","text":"<p>The Source Transformer module provides classes and functions for implementing Source Transform UDFs that transform data immediately after it's read from a source. Source Transform is useful for:</p> <ul> <li>Parsing/deserializing data at ingestion</li> <li>Filtering messages early</li> <li>Assigning event times</li> <li>Adding metadata</li> <li>Routing messages with tags</li> </ul>"},{"location":"api/sourcetransformer/#classes","title":"Classes","text":""},{"location":"api/sourcetransformer/#pynumaflow.sourcetransformer.Message","title":"Message  <code>dataclass</code>","text":"<pre><code>Message(\n    value: bytes,\n    event_time: datetime,\n    keys: list[str] = None,\n    tags: list[str] = None,\n    user_metadata: Optional[UserMetadata] = None,\n)\n</code></pre> <p>Basic datatype for data passing to the next vertex/vertices.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>bytes</code> <p>data in bytes</p> required <code>event_time</code> <code>datetime</code> <p>event time of the message, usually extracted from the payload.</p> required <code>keys</code> <code>list[str]</code> <p>[]string keys for vertex (optional)</p> <code>None</code> <code>tags</code> <code>list[str]</code> <p>[]string tags for conditional forwarding (optional)</p> <code>None</code> <code>user_metadata</code> <code>Optional[UserMetadata]</code> <p>metadata for the message (optional)</p> <code>None</code> Source code in <code>pynumaflow/sourcetransformer/_dtypes.py</code> <pre><code>def __init__(\n    self,\n    value: bytes,\n    event_time: datetime,\n    keys: list[str] = None,\n    tags: list[str] = None,\n    user_metadata: Optional[UserMetadata] = None,\n):\n    \"\"\"\n    Creates a Message object to send value to a vertex.\n    \"\"\"\n    self._tags = tags or []\n    self._keys = keys or []\n\n    # There is no year 0, so setting following as default event time.\n    self._event_time = event_time or datetime(1, 1, 1, 0, 0)\n    self._value = value or b\"\"\n    self._user_metadata = user_metadata or UserMetadata()\n</code></pre>"},{"location":"api/sourcetransformer/#pynumaflow.sourcetransformer.Messages","title":"Messages","text":"<pre><code>Messages(*messages: M)\n</code></pre> <p>               Bases: <code>Sequence[M]</code></p> <p>Class to define a list of Message objects.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>M</code> <p>list of Message objects.</p> <code>()</code> Source code in <code>pynumaflow/sourcetransformer/_dtypes.py</code> <pre><code>def __init__(self, *messages: M):\n    self._messages = list(messages) or []\n</code></pre>"},{"location":"api/sourcetransformer/#pynumaflow.sourcetransformer.Datum","title":"Datum  <code>dataclass</code>","text":"<pre><code>Datum(\n    keys: list[str],\n    value: bytes,\n    event_time: datetime,\n    watermark: datetime,\n    headers: Optional[dict[str, str]] = None,\n    user_metadata: Optional[UserMetadata] = None,\n    system_metadata: Optional[SystemMetadata] = None,\n)\n</code></pre> <p>Class to define the important information for the event.</p> <p>Parameters:</p> Name Type Description Default <code>keys</code> <code>list[str]</code> <p>the keys of the event.</p> required <code>value</code> <code>bytes</code> <p>the payload of the event.</p> required <code>event_time</code> <code>datetime</code> <p>the event time of the event.</p> required <code>watermark</code> <code>datetime</code> <p>the watermark of the event.</p> required <code>headers</code> <code>Optional[dict[str, str]]</code> <p>the headers of the event.</p> <code>None</code> <code>user_metadata</code> <code>Optional[UserMetadata]</code> <p>the user metadata of the event.</p> <code>None</code> <code>system_metadata</code> <code>Optional[SystemMetadata]</code> <p>the system metadata of the event.</p> <code>None</code> <p>Example: <pre><code>from pynumaflow.sourcetransformer import Datum\nfrom datetime import datetime, timezone\n\nd = Datum(\n        keys=[\"test_key\"],\n        value=b\"test_mock_message\",\n        event_time=datetime.fromtimestamp(1662998400, timezone.utc),\n        watermark=datetime.fromtimestamp(1662998460, timezone.utc),\n        headers={\"key1\": \"value1\", \"key2\": \"value2\"},\n   )\n</code></pre></p> Source code in <code>pynumaflow/sourcetransformer/_dtypes.py</code> <pre><code>def __init__(\n    self,\n    keys: list[str],\n    value: bytes,\n    event_time: datetime,\n    watermark: datetime,\n    headers: Optional[dict[str, str]] = None,\n    user_metadata: Optional[UserMetadata] = None,\n    system_metadata: Optional[SystemMetadata] = None,\n):\n    self._keys = keys or list()\n    self._value = value or b\"\"\n    if not isinstance(event_time, datetime):\n        raise TypeError(f\"Wrong data type: {type(event_time)} for Datum.event_time\")\n    self._event_time = event_time\n    if not isinstance(watermark, datetime):\n        raise TypeError(f\"Wrong data type: {type(watermark)} for Datum.watermark\")\n    self._watermark = watermark\n    self._headers = headers or {}\n    self._user_metadata = user_metadata or UserMetadata()\n    self._system_metadata = system_metadata or SystemMetadata()\n</code></pre>"},{"location":"api/sourcetransformer/#pynumaflow.sourcetransformer.Datum.keys","title":"keys  <code>property</code>","text":"<pre><code>keys: list[str]\n</code></pre> <p>Returns the keys of the event</p>"},{"location":"api/sourcetransformer/#pynumaflow.sourcetransformer.Datum.value","title":"value  <code>property</code>","text":"<pre><code>value: bytes\n</code></pre> <p>Returns the value of the event.</p>"},{"location":"api/sourcetransformer/#pynumaflow.sourcetransformer.Datum.event_time","title":"event_time  <code>property</code>","text":"<pre><code>event_time: datetime\n</code></pre> <p>Returns the event time of the event.</p>"},{"location":"api/sourcetransformer/#pynumaflow.sourcetransformer.Datum.watermark","title":"watermark  <code>property</code>","text":"<pre><code>watermark: datetime\n</code></pre> <p>Returns the watermark of the event.</p>"},{"location":"api/sourcetransformer/#pynumaflow.sourcetransformer.Datum.headers","title":"headers  <code>property</code>","text":"<pre><code>headers: dict[str, str]\n</code></pre> <p>Returns the headers of the event.</p>"},{"location":"api/sourcetransformer/#pynumaflow.sourcetransformer.Datum.user_metadata","title":"user_metadata  <code>property</code>","text":"<pre><code>user_metadata: UserMetadata\n</code></pre> <p>Returns the user metadata of the event.</p>"},{"location":"api/sourcetransformer/#pynumaflow.sourcetransformer.Datum.system_metadata","title":"system_metadata  <code>property</code>","text":"<pre><code>system_metadata: SystemMetadata\n</code></pre> <p>Returns the system metadata of the event.</p>"},{"location":"api/sourcetransformer/#pynumaflow.sourcetransformer.SourceTransformer","title":"SourceTransformer","text":"<p>Provides an interface to write a Source Transformer which will be exposed over a GRPC server.</p>"},{"location":"api/sourcetransformer/#pynumaflow.sourcetransformer.SourceTransformer.handler","title":"handler  <code>abstractmethod</code>","text":"<pre><code>handler(keys: list[str], datum: Datum) -&gt; Messages\n</code></pre> <p>Implement this handler function which implements the SourceTransformCallable interface.</p> Source code in <code>pynumaflow/sourcetransformer/_dtypes.py</code> <pre><code>@abstractmethod\ndef handler(self, keys: list[str], datum: Datum) -&gt; Messages:\n    \"\"\"\n    Implement this handler function which implements the\n    SourceTransformCallable interface.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/sourcetransformer/#pynumaflow.sourcetransformer.SourceTransformMultiProcServer","title":"SourceTransformMultiProcServer","text":"<pre><code>SourceTransformMultiProcServer(\n    source_transform_instance: SourceTransformCallable,\n    server_count: int = _PROCESS_COUNT,\n    sock_path=SOURCE_TRANSFORMER_SOCK_PATH,\n    max_message_size=MAX_MESSAGE_SIZE,\n    max_threads=NUM_THREADS_DEFAULT,\n    server_info_file=SOURCE_TRANSFORMER_SERVER_INFO_FILE_PATH,\n)\n</code></pre> <p>               Bases: <code>NumaflowServer</code></p> <p>Class for a new Source Transformer Server instance.</p> <p>The server instance is returned.</p> <p>Parameters:</p> Name Type Description Default <code>source_transform_instance</code> <code>SourceTransformCallable</code> <p>The source transformer instance to be used for</p> required <code>sock_path</code> <p>The UNIX socket path to be used for the server</p> <code>SOURCE_TRANSFORMER_SOCK_PATH</code> <code>server_count</code> <code>int</code> <p>The number of grpc server instances to be forked for multiproc</p> <code>_PROCESS_COUNT</code> <code>max_message_size</code> <p>The max message size in bytes the server can receive and send</p> <code>MAX_MESSAGE_SIZE</code> <code>max_threads</code> <p>The max number of threads to be spawned;             defaults to 4 and max capped at 16</p> <code>NUM_THREADS_DEFAULT</code> <p>Example invocation: <pre><code>import datetime\nimport logging\n\nfrom pynumaflow.sourcetransformer import Messages, Message, Datum, SourceTransformServer\n\n# This is a simple User Defined Function example which receives a message,\n# applies the following data transformation, and returns the message.\n# If the message event time is before year 2022, drop the message\n# with event time unchanged.\n# If it's within year 2022, update the tag to \"within_year_2022\" and\n# update the message event time to Jan 1st 2022.\n# Otherwise, (exclusively after year 2022), update the tag to\n# \"after_year_2022\" and update the\n\n\njanuary_first_2022 = datetime.datetime.fromtimestamp(1640995200)\njanuary_first_2023 = datetime.datetime.fromtimestamp(1672531200)\n\n\ndef my_handler(keys: list[str], datum: Datum) -&gt; Messages:\n    val = datum.value\n    event_time = datum.event_time\n    messages = Messages()\n\n    if event_time &lt; january_first_2022:\n        logging.info(\"Got event time:%s, it is before 2022, so dropping\", event_time)\n        messages.append(Message.to_drop(event_time))\n    elif event_time &lt; january_first_2023:\n        logging.info(\n            \"Got event time:%s, it is within year 2022, so\n            forwarding to within_year_2022\",\n            event_time,\n        )\n        message = Message(\n            value=val,\n            event_time=january_first_2022,\n            tags=[\"within_year_2022\"],\n        )\n        messages.append(message)\n    else:\n        logging.info(\n            \"Got event time:%s, it is after year 2022, so forwarding to after_year_2022\",\n            event_time,\n        )\n        message = Message(\n            value=val,\n            event_time=january_first_2023,\n            tags=[\"after_year_2022\"],\n        )\n        messages.append(message)\n\n    return messages\n\nif __name__ == \"__main__\":\n    grpc_server = SourceTransformMultiProcServer(\n        source_transform_instance=my_handler,\n        server_count=2,\n    )\n    grpc_server.start()\n</code></pre></p> Source code in <code>pynumaflow/sourcetransformer/multiproc_server.py</code> <pre><code>def __init__(\n    self,\n    source_transform_instance: SourceTransformCallable,\n    server_count: int = _PROCESS_COUNT,\n    sock_path=SOURCE_TRANSFORMER_SOCK_PATH,\n    max_message_size=MAX_MESSAGE_SIZE,\n    max_threads=NUM_THREADS_DEFAULT,\n    server_info_file=SOURCE_TRANSFORMER_SERVER_INFO_FILE_PATH,\n):\n    \"\"\"\n    Create a new grpc Source Transformer Multiproc Server instance.\n    A new servicer instance is created and attached to the server.\n    The server instance is returned.\n\n    Args:\n        source_transform_instance: The source transformer instance to be used for\n        Source Transformer UDF\n        sock_path: The UNIX socket path to be used for the server\n        server_count: The number of grpc server instances to be forked for multiproc\n        max_message_size: The max message size in bytes the server can receive and send\n        max_threads: The max number of threads to be spawned;\n                        defaults to 4 and max capped at 16\n\n    Example invocation:\n    ```py\n    import datetime\n    import logging\n\n    from pynumaflow.sourcetransformer import Messages, Message, Datum, SourceTransformServer\n\n    # This is a simple User Defined Function example which receives a message,\n    # applies the following data transformation, and returns the message.\n    # If the message event time is before year 2022, drop the message\n    # with event time unchanged.\n    # If it's within year 2022, update the tag to \"within_year_2022\" and\n    # update the message event time to Jan 1st 2022.\n    # Otherwise, (exclusively after year 2022), update the tag to\n    # \"after_year_2022\" and update the\n\n\n    january_first_2022 = datetime.datetime.fromtimestamp(1640995200)\n    january_first_2023 = datetime.datetime.fromtimestamp(1672531200)\n\n\n    def my_handler(keys: list[str], datum: Datum) -&gt; Messages:\n        val = datum.value\n        event_time = datum.event_time\n        messages = Messages()\n\n        if event_time &lt; january_first_2022:\n            logging.info(\"Got event time:%s, it is before 2022, so dropping\", event_time)\n            messages.append(Message.to_drop(event_time))\n        elif event_time &lt; january_first_2023:\n            logging.info(\n                \"Got event time:%s, it is within year 2022, so\n                forwarding to within_year_2022\",\n                event_time,\n            )\n            message = Message(\n                value=val,\n                event_time=january_first_2022,\n                tags=[\"within_year_2022\"],\n            )\n            messages.append(message)\n        else:\n            logging.info(\n                \"Got event time:%s, it is after year 2022, so forwarding to after_year_2022\",\n                event_time,\n            )\n            message = Message(\n                value=val,\n                event_time=january_first_2023,\n                tags=[\"after_year_2022\"],\n            )\n            messages.append(message)\n\n        return messages\n\n    if __name__ == \"__main__\":\n        grpc_server = SourceTransformMultiProcServer(\n            source_transform_instance=my_handler,\n            server_count=2,\n        )\n        grpc_server.start()\n    ```\n    \"\"\"\n    self.sock_path = f\"unix://{sock_path}\"\n    self.max_threads = min(max_threads, MAX_NUM_THREADS)\n    self.max_message_size = max_message_size\n    self.server_info_file = server_info_file\n\n    self.source_transform_instance = source_transform_instance\n\n    self._server_options = [\n        (\"grpc.max_send_message_length\", self.max_message_size),\n        (\"grpc.max_receive_message_length\", self.max_message_size),\n        (\"grpc.so_reuseport\", 1),\n        (\"grpc.so_reuseaddr\", 1),\n    ]\n    # Set the number of processes to be spawned to the number of CPUs or\n    # the value of the parameter server_count defined by the user\n    # Setting the max value to 2 * CPU count\n    # Used for multiproc server\n    self._process_count = min(server_count, 2 * _PROCESS_COUNT)\n    self.servicer = SourceTransformServicer(handler=source_transform_instance, multiproc=True)\n</code></pre>"},{"location":"api/sourcetransformer/#pynumaflow.sourcetransformer.SourceTransformMultiProcServer.start","title":"start","text":"<pre><code>start()\n</code></pre> <p>Starts the N gRPC servers on the given socket path with given max threads. Here N = The number of CPUs or the value of the parameter <code>server_count</code> defined by the user. The max value is capped to 2 * CPU count.</p> Source code in <code>pynumaflow/sourcetransformer/multiproc_server.py</code> <pre><code>def start(self):\n    \"\"\"\n    Starts the N gRPC servers on the given socket path with given max threads.\n    Here N = The number of CPUs or the value of the parameter `server_count`\n    defined by the user. The max value is capped to 2 * CPU count.\n    \"\"\"\n\n    serv_info = ServerInfo.get_default_server_info()\n    serv_info.minimum_numaflow_version = MINIMUM_NUMAFLOW_VERSION[\n        ContainerType.Sourcetransformer\n    ]\n    start_multiproc_server(\n        max_threads=self.max_threads,\n        servicer=self.servicer,\n        process_count=self._process_count,\n        server_info_file=self.server_info_file,\n        server_options=self._server_options,\n        udf_type=UDFType.SourceTransformer,\n        server_info=serv_info,\n    )\n</code></pre>"},{"location":"api/sourcetransformer/#pynumaflow.sourcetransformer.SourceTransformServer","title":"SourceTransformServer","text":"<pre><code>SourceTransformServer(\n    source_transform_instance: SourceTransformCallable,\n    sock_path=SOURCE_TRANSFORMER_SOCK_PATH,\n    max_message_size=MAX_MESSAGE_SIZE,\n    max_threads=NUM_THREADS_DEFAULT,\n    server_info_file=SOURCE_TRANSFORMER_SERVER_INFO_FILE_PATH,\n)\n</code></pre> <p>               Bases: <code>NumaflowServer</code></p> <p>Class for a new Source Transformer Server instance.</p> <p>The server instance is returned.</p> <p>Parameters:</p> Name Type Description Default <code>source_transform_instance</code> <code>SourceTransformCallable</code> <p>The source transformer instance to be used for</p> required <code>sock_path</code> <p>The UNIX socket path to be used for the server</p> <code>SOURCE_TRANSFORMER_SOCK_PATH</code> <code>max_message_size</code> <p>The max message size in bytes the server can receive and send</p> <code>MAX_MESSAGE_SIZE</code> <code>max_threads</code> <p>The max number of threads to be spawned;             defaults to 4 and max capped at 16</p> <code>NUM_THREADS_DEFAULT</code> <p>Below is a simple User Defined Function example which receives a message, applies the following data transformation, and returns the message.</p> <ul> <li>If the message event time is before year 2022, drop the message with event time unchanged.</li> <li>If it's within year 2022, update the tag to <code>within_year_2022</code> and update the message   event time to Jan 1st 2022.</li> <li>Otherwise, (exclusively after year 2022), update the tag to <code>after_year_2022</code> and update   the message event time to Jan 1st 2023.</li> </ul> <pre><code>import datetime\nimport logging\n\nfrom pynumaflow.sourcetransformer import Messages, Message, Datum, SourceTransformServer\n\n\njanuary_first_2022 = datetime.datetime.fromtimestamp(1640995200)\njanuary_first_2023 = datetime.datetime.fromtimestamp(1672531200)\n\n\ndef my_handler(keys: list[str], datum: Datum) -&gt; Messages:\n    val = datum.value\n    event_time = datum.event_time\n    messages = Messages()\n\n    if event_time &lt; january_first_2022:\n        logging.info(\"Got event time:%s, it is before 2022, so dropping\", event_time)\n        messages.append(Message.to_drop(event_time))\n    elif event_time &lt; january_first_2023:\n        logging.info(\n            \"Got event time:%s, it is within year 2022, so forwarding to within_year_2022\",\n            event_time,\n        )\n        messages.append(\n            Message(value=val, event_time=january_first_2022,\n                    tags=[\"within_year_2022\"])\n        )\n    else:\n        logging.info(\n            \"Got event time:%s, it is after year 2022, so forwarding to\n            after_year_2022\", event_time\n        )\n        messages.append(Message(value=val, event_time=january_first_2023,\n                        tags=[\"after_year_2022\"]))\n\n    return messages\n\n\nif __name__ == \"__main__\":\n    grpc_server = SourceTransformServer(my_handler)\n    grpc_server.start()\n</code></pre> Source code in <code>pynumaflow/sourcetransformer/server.py</code> <pre><code>def __init__(\n    self,\n    source_transform_instance: SourceTransformCallable,\n    sock_path=SOURCE_TRANSFORMER_SOCK_PATH,\n    max_message_size=MAX_MESSAGE_SIZE,\n    max_threads=NUM_THREADS_DEFAULT,\n    server_info_file=SOURCE_TRANSFORMER_SERVER_INFO_FILE_PATH,\n):\n    \"\"\"\n    Create a new grpc Source Transformer Server instance.\n    A new servicer instance is created and attached to the server.\n    The server instance is returned.\n\n    Args:\n        source_transform_instance: The source transformer instance to be used for\n        Source Transformer UDF\n        sock_path: The UNIX socket path to be used for the server\n        max_message_size: The max message size in bytes the server can receive and send\n        max_threads: The max number of threads to be spawned;\n                        defaults to 4 and max capped at 16\n\n    Below is a simple User Defined Function example which receives a message, applies the\n    following data transformation, and returns the message.\n\n    - If the message event time is before year 2022, drop the message with event time unchanged.\n    - If it's within year 2022, update the tag to `within_year_2022` and update the message\n      event time to Jan 1st 2022.\n    - Otherwise, (exclusively after year 2022), update the tag to `after_year_2022` and update\n      the message event time to Jan 1st 2023.\n\n    ```py\n    import datetime\n    import logging\n\n    from pynumaflow.sourcetransformer import Messages, Message, Datum, SourceTransformServer\n\n\n    january_first_2022 = datetime.datetime.fromtimestamp(1640995200)\n    january_first_2023 = datetime.datetime.fromtimestamp(1672531200)\n\n\n    def my_handler(keys: list[str], datum: Datum) -&gt; Messages:\n        val = datum.value\n        event_time = datum.event_time\n        messages = Messages()\n\n        if event_time &lt; january_first_2022:\n            logging.info(\"Got event time:%s, it is before 2022, so dropping\", event_time)\n            messages.append(Message.to_drop(event_time))\n        elif event_time &lt; january_first_2023:\n            logging.info(\n                \"Got event time:%s, it is within year 2022, so forwarding to within_year_2022\",\n                event_time,\n            )\n            messages.append(\n                Message(value=val, event_time=january_first_2022,\n                        tags=[\"within_year_2022\"])\n            )\n        else:\n            logging.info(\n                \"Got event time:%s, it is after year 2022, so forwarding to\n                after_year_2022\", event_time\n            )\n            messages.append(Message(value=val, event_time=january_first_2023,\n                            tags=[\"after_year_2022\"]))\n\n        return messages\n\n\n    if __name__ == \"__main__\":\n        grpc_server = SourceTransformServer(my_handler)\n        grpc_server.start()\n    ```\n    \"\"\"\n    self.sock_path = f\"unix://{sock_path}\"\n    self.max_threads = min(max_threads, MAX_NUM_THREADS)\n    self.max_message_size = max_message_size\n    self.server_info_file = server_info_file\n\n    self.source_transform_instance = source_transform_instance\n\n    self._server_options = [\n        (\"grpc.max_send_message_length\", self.max_message_size),\n        (\"grpc.max_receive_message_length\", self.max_message_size),\n    ]\n    self.servicer = SourceTransformServicer(handler=source_transform_instance)\n</code></pre>"},{"location":"api/sourcetransformer/#pynumaflow.sourcetransformer.SourceTransformServer.start","title":"start","text":"<pre><code>start()\n</code></pre> <p>Starts the Synchronous gRPC server on the given UNIX socket with given max threads.</p> Source code in <code>pynumaflow/sourcetransformer/server.py</code> <pre><code>def start(self):\n    \"\"\"\n    Starts the Synchronous gRPC server on the given UNIX socket with given max threads.\n    \"\"\"\n    _LOGGER.info(\n        \"Sync GRPC Server listening on: %s with max threads: %s\",\n        self.sock_path,\n        self.max_threads,\n    )\n    serv_info = ServerInfo.get_default_server_info()\n    serv_info.minimum_numaflow_version = MINIMUM_NUMAFLOW_VERSION[\n        ContainerType.Sourcetransformer\n    ]\n    # Start the sync server\n    sync_server_start(\n        servicer=self.servicer,\n        bind_address=self.sock_path,\n        max_threads=self.max_threads,\n        server_info_file=self.server_info_file,\n        server_options=self._server_options,\n        udf_type=UDFType.SourceTransformer,\n        server_info=serv_info,\n    )\n</code></pre>"},{"location":"api/sourcetransformer/#pynumaflow.sourcetransformer.SourceTransformAsyncServer","title":"SourceTransformAsyncServer","text":"<pre><code>SourceTransformAsyncServer(\n    source_transform_instance: SourceTransformAsyncCallable,\n    sock_path=SOURCE_TRANSFORMER_SOCK_PATH,\n    max_message_size=MAX_MESSAGE_SIZE,\n    max_threads=NUM_THREADS_DEFAULT,\n    server_info_file=SOURCE_TRANSFORMER_SERVER_INFO_FILE_PATH,\n)\n</code></pre> <p>               Bases: <code>NumaflowServer</code></p> <p>Create a new grpc Source Transformer Server instance. A new servicer instance is created and attached to the server. The server instance is returned.</p> <p>Parameters:</p> Name Type Description Default <code>source_transform_instance</code> <code>SourceTransformAsyncCallable</code> <p>The source transformer instance to be used for</p> required <code>sock_path</code> <p>The UNIX socket path to be used for the server</p> <code>SOURCE_TRANSFORMER_SOCK_PATH</code> <code>max_message_size</code> <p>The max message size in bytes the server can receive and send</p> <code>MAX_MESSAGE_SIZE</code> <code>max_threads</code> <p>The max number of threads to be spawned;             defaults to 4 and max capped at 16</p> <code>NUM_THREADS_DEFAULT</code> <p>Below is a simple User Defined Function example which receives a message, applies the following data transformation, and returns the message.</p> <ul> <li>If the message event time is before year 2022, drop the message with event time unchanged.</li> <li>If it's within year 2022, update the tag to <code>within_year_2022</code> and update the message   event time to Jan 1st 2022.</li> <li>Otherwise, (exclusively after year 2022), update the tag to <code>after_year_2022</code> and update   the message event time to Jan 1st 2023.</li> </ul> <pre><code>import datetime\nimport logging\nfrom pynumaflow.sourcetransformer import Messages, Message, Datum, SourceTransformServer\n\njanuary_first_2022 = datetime.datetime.fromtimestamp(1640995200)\njanuary_first_2023 = datetime.datetime.fromtimestamp(1672531200)\n\n\nasync def my_handler(keys: list[str], datum: Datum) -&gt; Messages:\n    val = datum.value\n    event_time = datum.event_time\n    messages = Messages()\n\n    if event_time &lt; january_first_2022:\n        logging.info(\"Got event time:%s, it is before 2022, so dropping\", event_time)\n        messages.append(Message.to_drop(event_time))\n    elif event_time &lt; january_first_2023:\n        logging.info(\n            \"Got event time:%s, it is within year 2022, so forwarding to within_year_2022\",\n            event_time,\n        )\n        messages.append(\n            Message(value=val, event_time=january_first_2022,\n                    tags=[\"within_year_2022\"])\n        )\n    else:\n        logging.info(\n            \"Got event time:%s, it is after year 2022, so forwarding to\n            after_year_2022\", event_time\n        )\n        messages.append(Message(value=val, event_time=january_first_2023,\n                        tags=[\"after_year_2022\"]))\n\n    return messages\n\n\nif __name__ == \"__main__\":\n    grpc_server = SourceTransformAsyncServer(my_handler)\n    grpc_server.start()\n</code></pre> Source code in <code>pynumaflow/sourcetransformer/async_server.py</code> <pre><code>def __init__(\n    self,\n    source_transform_instance: SourceTransformAsyncCallable,\n    sock_path=SOURCE_TRANSFORMER_SOCK_PATH,\n    max_message_size=MAX_MESSAGE_SIZE,\n    max_threads=NUM_THREADS_DEFAULT,\n    server_info_file=SOURCE_TRANSFORMER_SERVER_INFO_FILE_PATH,\n):\n    self.sock_path = f\"unix://{sock_path}\"\n    self.max_threads = min(max_threads, MAX_NUM_THREADS)\n    self.max_message_size = max_message_size\n    self.server_info_file = server_info_file\n\n    self.source_transform_instance = source_transform_instance\n\n    self._server_options = [\n        (\"grpc.max_send_message_length\", self.max_message_size),\n        (\"grpc.max_receive_message_length\", self.max_message_size),\n    ]\n    self.servicer = SourceTransformAsyncServicer(handler=source_transform_instance)\n</code></pre>"},{"location":"api/sourcetransformer/#pynumaflow.sourcetransformer.SourceTransformAsyncServer.start","title":"start","text":"<pre><code>start() -&gt; None\n</code></pre> <p>Starter function for the Async server class, need a separate caller so that all the async coroutines can be started from a single context</p> Source code in <code>pynumaflow/sourcetransformer/async_server.py</code> <pre><code>def start(self) -&gt; None:\n    \"\"\"\n    Starter function for the Async server class, need a separate caller\n    so that all the async coroutines can be started from a single context\n    \"\"\"\n    aiorun.run(self.aexec(), use_uvloop=True)\n</code></pre>"},{"location":"api/sourcetransformer/#pynumaflow.sourcetransformer.SourceTransformAsyncServer.aexec","title":"aexec  <code>async</code>","text":"<pre><code>aexec() -&gt; None\n</code></pre> <p>Starts the Async gRPC server on the given UNIX socket with given max threads.</p> Source code in <code>pynumaflow/sourcetransformer/async_server.py</code> <pre><code>async def aexec(self) -&gt; None:\n    \"\"\"\n    Starts the Async gRPC server on the given UNIX socket with\n    given max threads.\n    \"\"\"\n\n    # As the server is async, we need to create a new server instance in the\n    # same thread as the event loop so that all the async calls are made in the\n    # same context\n\n    server_new = grpc.aio.server(options=self._server_options)\n    server_new.add_insecure_port(self.sock_path)\n    transform_pb2_grpc.add_SourceTransformServicer_to_server(self.servicer, server_new)\n\n    serv_info = ServerInfo.get_default_server_info()\n    serv_info.minimum_numaflow_version = MINIMUM_NUMAFLOW_VERSION[\n        ContainerType.Sourcetransformer\n    ]\n\n    # Start the async server\n    await start_async_server(\n        server_async=server_new,\n        sock_path=self.sock_path,\n        max_threads=self.max_threads,\n        cleanup_coroutines=list(),\n        server_info_file=self.server_info_file,\n        server_info=serv_info,\n    )\n</code></pre>"},{"location":"api/sourcetransformer/#pynumaflow.sourcetransformer.UserMetadata","title":"UserMetadata  <code>dataclass</code>","text":"<pre><code>UserMetadata(_data: dict[str, dict[str, bytes]] = dict())\n</code></pre> <p>UserMetadata wraps the user-generated metadata groups per message. It is read-write to UDFs.</p>"},{"location":"api/sourcetransformer/#pynumaflow.sourcetransformer.UserMetadata.groups","title":"groups","text":"<pre><code>groups() -&gt; list[str]\n</code></pre> <p>Returns the list of group names for the user metadata.</p> Source code in <code>pynumaflow/_metadata.py</code> <pre><code>def groups(self) -&gt; list[str]:\n    \"\"\"\n    Returns the list of group names for the user metadata.\n    \"\"\"\n    return list(self._data.keys())\n</code></pre>"},{"location":"api/sourcetransformer/#pynumaflow.sourcetransformer.UserMetadata.keys","title":"keys","text":"<pre><code>keys(group: str) -&gt; list[str]\n</code></pre> <p>Returns the list of keys for a given group.</p> Source code in <code>pynumaflow/_metadata.py</code> <pre><code>def keys(self, group: str) -&gt; list[str]:\n    \"\"\"\n    Returns the list of keys for a given group.\n    \"\"\"\n    keys = self._data.get(group) or {}\n    return list(keys.keys())\n</code></pre>"},{"location":"api/sourcetransformer/#pynumaflow.sourcetransformer.UserMetadata.value","title":"value","text":"<pre><code>value(group: str, key: str) -&gt; Optional[bytes]\n</code></pre> <p>Returns the value for a given group and key. If the group or key does not exist, returns None.</p> Source code in <code>pynumaflow/_metadata.py</code> <pre><code>def value(self, group: str, key: str) -&gt; Optional[bytes]:\n    \"\"\"\n    Returns the value for a given group and key.\n    If the group or key does not exist, returns None.\n    \"\"\"\n    value = self._data.get(group)\n    if value is None:\n        return None\n    return value.get(key)\n</code></pre>"},{"location":"api/sourcetransformer/#pynumaflow.sourcetransformer.UserMetadata.add_key","title":"add_key","text":"<pre><code>add_key(group: str, key: str, value: bytes)\n</code></pre> <p>Adds the value for a given group and key.</p> Source code in <code>pynumaflow/_metadata.py</code> <pre><code>def add_key(self, group: str, key: str, value: bytes):\n    \"\"\"\n    Adds the value for a given group and key.\n    \"\"\"\n    self._data.setdefault(group, {})[key] = value\n</code></pre>"},{"location":"api/sourcetransformer/#pynumaflow.sourcetransformer.UserMetadata.remove_key","title":"remove_key","text":"<pre><code>remove_key(group: str, key: str) -&gt; Optional[bytes]\n</code></pre> <p>Removes the key and its value for a given group and returns the value. If this key is the only key in the group, the group will be removed. Returns None if the group or key does not exist.</p> Source code in <code>pynumaflow/_metadata.py</code> <pre><code>def remove_key(self, group: str, key: str) -&gt; Optional[bytes]:\n    \"\"\"\n    Removes the key and its value for a given group and returns the value.\n    If this key is the only key in the group, the group will be removed.\n    Returns None if the group or key does not exist.\n    \"\"\"\n    group_data = self._data.pop(group, None)\n    if group_data is None:\n        return None\n    value = group_data.pop(key, None)\n    if group_data:\n        self._data[group] = group_data\n    return value\n</code></pre>"},{"location":"api/sourcetransformer/#pynumaflow.sourcetransformer.UserMetadata.remove_group","title":"remove_group","text":"<pre><code>remove_group(group: str) -&gt; Optional[dict[str, bytes]]\n</code></pre> <p>Removes the group and all its keys and values and returns the data. Returns None if the group does not exist.</p> Source code in <code>pynumaflow/_metadata.py</code> <pre><code>def remove_group(self, group: str) -&gt; Optional[dict[str, bytes]]:\n    \"\"\"\n    Removes the group and all its keys and values and returns the data.\n    Returns None if the group does not exist.\n    \"\"\"\n    return self._data.pop(group, None)\n</code></pre>"},{"location":"api/sourcetransformer/#pynumaflow.sourcetransformer.UserMetadata.clear","title":"clear","text":"<pre><code>clear()\n</code></pre> <p>Clears all the groups and all their keys and values.</p> Source code in <code>pynumaflow/_metadata.py</code> <pre><code>def clear(self):\n    \"\"\"\n    Clears all the groups and all their keys and values.\n    \"\"\"\n    self._data.clear()\n</code></pre>"},{"location":"api/sourcetransformer/#pynumaflow.sourcetransformer.SystemMetadata","title":"SystemMetadata  <code>dataclass</code>","text":"<pre><code>SystemMetadata(_data: dict[str, dict[str, bytes]] = dict())\n</code></pre> <p>System metadata is the mapping of group name to key-value pairs for a given group. System metadata wraps the system-generated metadata groups per message. It is read-only to UDFs.</p>"},{"location":"api/sourcetransformer/#pynumaflow.sourcetransformer.SystemMetadata.groups","title":"groups","text":"<pre><code>groups() -&gt; list[str]\n</code></pre> <p>Returns the list of group names for the system metadata.</p> Source code in <code>pynumaflow/_metadata.py</code> <pre><code>def groups(self) -&gt; list[str]:\n    \"\"\"\n    Returns the list of group names for the system metadata.\n    \"\"\"\n    return list(self._data.keys())\n</code></pre>"},{"location":"api/sourcetransformer/#pynumaflow.sourcetransformer.SystemMetadata.keys","title":"keys","text":"<pre><code>keys(group: str) -&gt; list[str]\n</code></pre> <p>Returns the list of keys for a given group.</p> Source code in <code>pynumaflow/_metadata.py</code> <pre><code>def keys(self, group: str) -&gt; list[str]:\n    \"\"\"\n    Returns the list of keys for a given group.\n    \"\"\"\n    return list(self._data.get(group, {}).keys())\n</code></pre>"},{"location":"api/sourcetransformer/#pynumaflow.sourcetransformer.SystemMetadata.value","title":"value","text":"<pre><code>value(group: str, key: str) -&gt; Optional[bytes]\n</code></pre> <p>Returns the value for a given group and key.</p> Source code in <code>pynumaflow/_metadata.py</code> <pre><code>def value(self, group: str, key: str) -&gt; Optional[bytes]:\n    \"\"\"\n    Returns the value for a given group and key.\n    \"\"\"\n    return self._data.get(group, {}).get(key)\n</code></pre>"}]}